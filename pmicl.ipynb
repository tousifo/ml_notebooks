{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVBSwMBZlmHz4cVKw/ZkHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/pmicl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/oasis_dataset.py\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ZIP_PATH = '/content/oaisis.zip'\n",
        "EXTRACT_DIR = '/content/oasis_data/'\n",
        "\n",
        "def extract_zip():\n",
        "    try:\n",
        "        if not os.path.exists(ZIP_PATH):\n",
        "            raise FileNotFoundError(f\"{ZIP_PATH} not found. Please upload the file to Colab.\")\n",
        "        os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "        if not os.listdir(EXTRACT_DIR):\n",
        "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(EXTRACT_DIR)\n",
        "            print(f\"Extracted {ZIP_PATH} to {EXTRACT_DIR}\")\n",
        "        else:\n",
        "            print(f\"Directory {EXTRACT_DIR} already contains files, skipping extraction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting ZIP file: {e}\")\n",
        "        raise\n",
        "\n",
        "extract_zip()\n",
        "\n",
        "class PatchExtractor:\n",
        "    def __init__(self, patch_size=32, K=80, spatial_threshold=5):\n",
        "        self.patch_size = patch_size\n",
        "        self.K = K\n",
        "        self.spatial_threshold = spatial_threshold\n",
        "\n",
        "    def extract_patches(self, image, prob_map):\n",
        "        patches = []\n",
        "        coords = []\n",
        "        prob_map_copy = prob_map.copy()\n",
        "        for _ in range(self.K):\n",
        "            max_prob_idx = np.argmax(prob_map_copy)\n",
        "            y, x = np.unravel_index(max_prob_idx, prob_map_copy.shape)\n",
        "            patch = self.get_patch(image, (x, y))\n",
        "            patches.append(patch)\n",
        "            coords.append((x, y))\n",
        "            prob_map_copy = self.mask_neighbors(prob_map_copy, (x, y))\n",
        "        return patches, coords\n",
        "\n",
        "    def get_patch(self, image, center):\n",
        "        half_size = self.patch_size // 2\n",
        "        x, y = center\n",
        "        patch = image[\n",
        "            max(0, y - half_size):y + half_size,\n",
        "            max(0, x - half_size):x + half_size\n",
        "        ]\n",
        "        if patch.shape[0] < self.patch_size or patch.shape[1] < self.patch_size:\n",
        "            patch = np.pad(patch, [(0, max(0, self.patch_size - patch.shape[0])),\n",
        "                                   (0, max(0, self.patch_size - patch.shape[1]))],\n",
        "                           mode='constant')\n",
        "        return patch\n",
        "\n",
        "    def mask_neighbors(self, prob_map, center):\n",
        "        half_size = self.spatial_threshold\n",
        "        x, y = center\n",
        "        prob_map[\n",
        "            max(0, y - half_size):y + half_size + 1,\n",
        "            max(0, x - half_size):x + half_size + 1\n",
        "        ] = 0\n",
        "        return prob_map\n",
        "\n",
        "    def dynamic_sample(self, image, N=20):\n",
        "        candidate_patches = []\n",
        "        candidate_coords = []\n",
        "        stride = self.patch_size // 2\n",
        "        h, w = image.shape\n",
        "        for y in range(0, h - self.patch_size + 1, stride):\n",
        "            for x in range(0, w - self.patch_size + 1, stride):\n",
        "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
        "                candidate_patches.append(patch)\n",
        "                candidate_coords.append((x, y))\n",
        "        indices = np.random.choice(len(candidate_patches), min(N, len(candidate_patches)), replace=False)\n",
        "        return [candidate_patches[i] for i in indices], [candidate_coords[i] for i in indices]\n",
        "\n",
        "class OasisDataset(Dataset):\n",
        "    def __init__(self, data_dir, patch_size=32, n_sampled_patches=20, subset_size=1000):\n",
        "        self.data_dir = os.path.join(data_dir, 'Data')\n",
        "        self.patch_size = patch_size\n",
        "        self.n_sampled_patches = n_sampled_patches\n",
        "        self.patch_extractor = PatchExtractor(patch_size=patch_size)\n",
        "        self.class_map = {\n",
        "            'Non Demented': 0,\n",
        "            'Very mild Dementia': 1,\n",
        "            'Mild Dementia': 2,\n",
        "            'Moderate Dementia': 3\n",
        "        }\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.subset_size = subset_size\n",
        "\n",
        "        print(f\"Looking for images in {self.data_dir}\")\n",
        "        if not os.path.exists(self.data_dir):\n",
        "            raise FileNotFoundError(f\"Data directory {self.data_dir} not found\")\n",
        "\n",
        "        available_dirs = os.listdir(self.data_dir)\n",
        "        print(f\"Available directories: {available_dirs}\")\n",
        "\n",
        "        for class_name in self.class_map:\n",
        "            matching_dir = next((d for d in available_dirs if d.lower() == class_name.lower()), None)\n",
        "            if not matching_dir:\n",
        "                print(f\"Warning: No directory found for {class_name}\")\n",
        "                continue\n",
        "            class_dir = os.path.join(self.data_dir, matching_dir)\n",
        "            print(f\"Checking directory: {class_dir}\")\n",
        "            img_files = [f for f in os.listdir(class_dir) if f.lower().endswith('.jpg')]\n",
        "            img_paths = [os.path.join(class_dir, f) for f in img_files]\n",
        "            print(f\"Found {len(img_paths)} .jpg files in {class_dir}\")\n",
        "            for img_path in img_paths:\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.labels.append(self.class_map[class_name])\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Failed to load {img_path}: {e}\")\n",
        "\n",
        "        if not self.image_paths:\n",
        "            raise ValueError(f\"No valid images found in {self.data_dir}.\")\n",
        "\n",
        "        total_images = len(self.image_paths)\n",
        "        if self.subset_size > total_images:\n",
        "            self.subset_size = total_images\n",
        "            print(f\"Subset size adjusted to {self.subset_size} (total available images)\")\n",
        "\n",
        "        if self.subset_size < total_images:\n",
        "            X_subset, _, y_subset, _ = train_test_split(\n",
        "                self.image_paths, self.labels, train_size=self.subset_size, stratify=self.labels, random_state=42\n",
        "            )\n",
        "            self.image_paths = X_subset\n",
        "            self.labels = y_subset\n",
        "\n",
        "        print(f\"Loaded {len(self.image_paths)} images: \"\n",
        "              f\"{len([l for l in self.labels if l == 0])} CN, \"\n",
        "              f\"{len([l for l in self.labels if l == 1])} MCI, \"\n",
        "              f\"{len([l for l in self.labels if l == 2])} Mild, \"\n",
        "              f\"{len([l for l in self.labels if l == 3])} Moderate\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = np.array(Image.open(img_path).convert('L')) / 255.0\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            raise\n",
        "        prob_map = np.random.rand(image.shape[0], image.shape[1])\n",
        "        patches, coords = self.patch_extractor.extract_patches(image, prob_map)\n",
        "        sampled_patches, sampled_coords = self.patch_extractor.dynamic_sample(image, self.n_sampled_patches)\n",
        "        patches_tensor = torch.tensor(sampled_patches, dtype=torch.float).unsqueeze(1)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        return patches_tensor, label_tensor, coords\n",
        "\n",
        "def get_dataloader(data_dir, batch_size=2, subset_size=1000):\n",
        "    dataset = OasisDataset(data_dir, patch_size=32, n_sampled_patches=20, subset_size=subset_size)\n",
        "    class_counts = np.bincount(dataset.labels)\n",
        "    weights = 1.0 / class_counts[dataset.labels]\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=0)\n",
        "    return dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEboLVMEzFcF",
        "outputId": "7ab35f1d-6033-4d68-8eaf-4bc3d36b59e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/oasis_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from oasis_dataset import OasisDataset\n",
        "import numpy as np\n",
        "try:\n",
        "    dataset = OasisDataset('/content/oasis_data/', subset_size=1000)\n",
        "    print(f\"Class distribution: {np.bincount(dataset.labels)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Dataset error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Ks-LxI4xTK",
        "outputId": "cbd661f9-ca49-4762-efcf-7c34615ba7b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted /content/oaisis.zip to /content/oasis_data/\n",
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Very mild Dementia', 'Non Demented', 'Mild Dementia', 'Moderate Dementia']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1000 images: 778 CN, 159 MCI, 58 Mild, 5 Moderate\n",
            "Class distribution: [778 159  58   5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from oasis_dataset import get_dataloader\n",
        "try:\n",
        "    dataloader = get_dataloader('/content/oasis_data/', batch_size=2)\n",
        "    for patches, labels, coords in dataloader:\n",
        "        print(\"Patches shape:\", patches.shape)  # Expected: [2, 20, 1, 32, 32]\n",
        "        print(\"Labels shape:\", labels.shape)   # Expected: [2]\n",
        "        print(\"Coords length:\", len(coords))   # Expected: 2\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"DataLoader error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiFVnIT6oPYp",
        "outputId": "89ec1e0e-5790-416a-b8f9-0ba44fc69d12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Very mild Dementia', 'Non Demented', 'Mild Dementia', 'Moderate Dementia']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1000 images: 778 CN, 159 MCI, 58 Mild, 5 Moderate\n",
            "Patches shape: torch.Size([2, 20, 1, 32, 32])\n",
            "Labels shape: torch.Size([2])\n",
            "Coords length: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/oasis_dataset.py:160: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  patches_tensor = torch.tensor(sampled_patches, dtype=torch.float).unsqueeze(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pmicl_model"
      ],
      "metadata": {
        "id": "utWP-fX8TDvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pmicl_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# Check PyTorch version for compatibility\n",
        "TORCH_VERSION = torch.__version__.split('+')[0]\n",
        "if not torch.__version__.startswith(('2.3', '2.4', '2.5', '2.6')):\n",
        "    print(f\"Warning: PyTorch version {TORCH_VERSION} detected. This code is tested with PyTorch 2.3.x to 2.6.x.\")\n",
        "\n",
        "# Import torch-geometric with error handling\n",
        "try:\n",
        "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "except ImportError:\n",
        "    print(\"ERROR: torch-geometric is not installed. Please run the following in a Colab cell:\")\n",
        "    print(\"!pip install torch-geometric\")\n",
        "    print(\"!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html\")\n",
        "    sys.exit(1)\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Configuration\n",
        "EMBED_DIM = 128\n",
        "NUM_CLASSES = 4  # Updated for CN, MCI, Mild, Moderate\n",
        "TEMPERATURE = 0.1\n",
        "PROTOTYPE_LOSS_WEIGHT = 0.5\n",
        "GRAPH_LOSS_WEIGHT = 0.3\n",
        "\n",
        "# Graph Constructor for mi-Graph\n",
        "class GraphConstructor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def build_graph(self, patches):\n",
        "        try:\n",
        "            # Check if patches is already a NumPy array\n",
        "            if isinstance(patches, np.ndarray):\n",
        "                patch_features = patches\n",
        "            else:\n",
        "                # Assume patches is a PyTorch tensor\n",
        "                patch_features = patches.cpu().numpy()\n",
        "            dist_matrix = euclidean_distances(patch_features)\n",
        "            edge_index = []\n",
        "            edge_weight = []\n",
        "            for i in range(len(patch_features)):\n",
        "                for j in range(i + 1, len(patch_features)):\n",
        "                    edge_index.append([i, j])\n",
        "                    edge_index.append([j, i])\n",
        "                    edge_weight.append(dist_matrix[i, j])\n",
        "                    edge_weight.append(dist_matrix[i, j])\n",
        "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "            edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "            return edge_index, edge_weight\n",
        "        except Exception as e:\n",
        "            print(f\"Error building graph: {e}\")\n",
        "            raise\n",
        "\n",
        "# 2D CNN for Patch Encoding\n",
        "class PatchEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=1, embed_dim=EMBED_DIM):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Linear(64 * 4 * 4, embed_dim)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        B, N, C, H, W = patches.shape\n",
        "        patches_flat = patches.view(B * N, C, H, W)\n",
        "        features = self.conv_layers(patches_flat)\n",
        "        features_flat = features.view(B * N, -1)\n",
        "        embeddings = self.fc(features_flat)\n",
        "        embeddings_normalized = F.normalize(embeddings, p=2, dim=1)\n",
        "        return embeddings_normalized.view(B, N, self.embed_dim)\n",
        "\n",
        "# GNN for Graph Processing (mi-Graph)\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(EMBED_DIM, 64)\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "# Attention-Based MIL Aggregator (PMICL)\n",
        "class AttentionMIL(nn.Module):\n",
        "    def __init__(self, embed_dim=EMBED_DIM):\n",
        "        super(AttentionMIL, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(embed_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, patch_embeddings):\n",
        "        u = self.attention(patch_embeddings)\n",
        "        a = F.softmax(u, dim=1)\n",
        "        bag_embedding = torch.sum(a * patch_embeddings, dim=1)\n",
        "        return bag_embedding, a.squeeze(-1)\n",
        "\n",
        "# PMICL Model with mi-Graph Integration\n",
        "class PMICL(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(PMICL, self).__init__()\n",
        "        self.encoder = PatchEncoder()\n",
        "        self.gnn = GNN()\n",
        "        self.attention_mil = AttentionMIL()\n",
        "        self.fc = nn.Linear(EMBED_DIM + 32, num_classes)\n",
        "        self.prototypes = nn.Parameter(torch.randn(num_classes, EMBED_DIM))\n",
        "        nn.init.xavier_uniform_(self.prototypes)\n",
        "\n",
        "    def forward(self, patches, edge_index, batch, bag_labels=None):\n",
        "        patch_embeddings = self.encoder(patches)\n",
        "        B, N, D = patch_embeddings.shape\n",
        "        patch_embeddings_flat = patch_embeddings.view(B * N, D)\n",
        "        graph_features = self.gnn(patch_embeddings_flat, edge_index, batch)\n",
        "        bag_embedding, attention_weights = self.attention_mil(patch_embeddings)\n",
        "        combined_features = torch.cat([bag_embedding, graph_features], dim=1)\n",
        "        logits = self.fc(combined_features)\n",
        "        proto_loss = torch.tensor(0.0, device=patches.device)\n",
        "        if bag_labels is not None:\n",
        "            sim_matrix = torch.matmul(patch_embeddings_flat, self.prototypes.t())\n",
        "            logits_proto = sim_matrix / TEMPERATURE\n",
        "            patch_labels = bag_labels.unsqueeze(1).repeat(1, N).view(B * N)\n",
        "            proto_loss = F.cross_entropy(logits_proto, patch_labels)\n",
        "        return logits, proto_loss, attention_weights, patch_embeddings\n",
        "\n",
        "# Graph Loss for Feature Smoothness\n",
        "def graph_loss(patch_embeddings, edge_index, edge_weight):\n",
        "    diff = patch_embeddings[edge_index[0]] - patch_embeddings[edge_index[1]]\n",
        "    loss = (edge_weight * (diff ** 2).sum(dim=1)).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "sCSvS3SDhlYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94c3015-70e5-4630-c593-79e5a6f24e81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pmicl_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train_eval\n"
      ],
      "metadata": {
        "id": "C8IcjcGCukLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/train_eval.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pmicl_model import PMICL, GraphConstructor, graph_loss\n",
        "from oasis_dataset import get_dataloader\n",
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device(\"cpu\")  # Use CPU to avoid CUDA issues\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 10\n",
        "EMBED_DIM = 128\n",
        "PROTOTYPE_LOSS_WEIGHT = 0.5\n",
        "GRAPH_LOSS_WEIGHT = 0.3\n",
        "NUM_CLASSES = 4  # Updated for CN, MCI, Mild, Moderate\n",
        "\n",
        "# Training Loop\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_cls_loss = 0\n",
        "    total_proto_loss = 0\n",
        "    total_graph_loss = 0\n",
        "    graph_constructor = GraphConstructor()\n",
        "\n",
        "    for patches, labels, coords in dataloader:\n",
        "        patches, labels = patches.to(device), labels.to(device)\n",
        "        B, N, C, H, W = patches.shape\n",
        "\n",
        "        # Build graph\n",
        "        patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "        edge_index, edge_weight = graph_constructor.build_graph(patch_features)\n",
        "        edge_index, edge_weight = edge_index.to(device), edge_weight.to(device)\n",
        "        batch = torch.repeat_interleave(torch.arange(B, device=device), N)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits, proto_loss, attention_weights, patch_embeddings = model(patches, edge_index, batch, labels)\n",
        "\n",
        "        # Losses\n",
        "        cls_loss = F.cross_entropy(logits, labels)\n",
        "        graph = graph_loss(patch_embeddings.view(B * N, EMBED_DIM), edge_index, edge_weight)\n",
        "        loss = cls_loss + PROTOTYPE_LOSS_WEIGHT * proto_loss + GRAPH_LOSS_WEIGHT * graph\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_cls_loss += cls_loss.item()\n",
        "        total_proto_loss += proto_loss.item()\n",
        "        total_graph_loss += graph.item()\n",
        "\n",
        "    return (total_loss / len(dataloader), total_cls_loss / len(dataloader),\n",
        "            total_proto_loss / len(dataloader), total_graph_loss / len(dataloader))\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    graph_constructor = GraphConstructor()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for patches, labels, coords in dataloader:\n",
        "            patches, labels = patches.to(device), labels.to(device)\n",
        "            B, N, C, H, W = patches.shape\n",
        "            patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "            edge_index, edge_weight = graph_constructor.build_graph(patch_features)\n",
        "            edge_index, edge_weight = edge_index.to(device), edge_weight.to(device)\n",
        "            batch = torch.repeat_interleave(torch.arange(B, device=device), N)\n",
        "\n",
        "            logits, _, _, _ = model(patches, edge_index, batch)\n",
        "            preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    acc = accuracy_score(true_labels, preds)\n",
        "    f1 = f1_score(true_labels, preds, average='macro')  # Macro for multi-class\n",
        "    auc = roc_auc_score(true_labels, preds, multi_class='ovr')  # OVR for multi-class\n",
        "    return acc, f1, auc\n",
        "\n",
        "# Visualize Attention Weights\n",
        "def visualize_attention(image, coords, attention_weights, patch_size=32):\n",
        "    attention_map = np.zeros(image.shape)\n",
        "    for (x, y), weight in zip(coords, attention_weights):\n",
        "        attention_map[y:y+patch_size, x:x+patch_size] += weight\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(\"Original Slice\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(attention_map, cmap='hot')\n",
        "    plt.title(\"Attention Map\")\n",
        "    plt.savefig('/content/attention_map.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize DataLoader\n",
        "    dataloader = get_dataloader('/content/oasis_data/', BATCH_SIZE)\n",
        "\n",
        "    # Initialize Model\n",
        "    model = PMICL(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "        losses = train_epoch(model, dataloader, optimizer, DEVICE)\n",
        "        print(f\"Loss: {losses[0]:.4f}, Cls: {losses[1]:.4f}, Proto: {losses[2]:.4f}, Graph: {losses[3]:.4f}\")\n",
        "\n",
        "        # Evaluate every epoch\n",
        "        acc, f1, auc = evaluate(model, dataloader, DEVICE)\n",
        "        print(f\"Val: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
        "\n",
        "    # Visualize attention for a sample\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        patches, labels, coords = next(iter(dataloader))\n",
        "        from PIL import Image\n",
        "        image = np.array(Image.open(dataloader.dataset.image_paths[0]).convert('L')) / 255.0\n",
        "        patches = patches.to(DEVICE)\n",
        "        B, N, C, H, W = patches.shape\n",
        "        patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "        edge_index, edge_weight = GraphConstructor().build_graph(patch_features)\n",
        "        edge_index, edge_weight = edge_index.to(DEVICE), edge_weight.to(DEVICE)\n",
        "        batch = torch.repeat_interleave(torch.arange(B, device=DEVICE), N)\n",
        "\n",
        "        _, _, attention_weights, _ = model(patches, edge_index, batch)\n",
        "        visualize_attention(image, coords[0], attention_weights[0].cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daNd8mzsvi0m",
        "outputId": "da31a3f8-cc39-4da6-b54d-35c98d98e0ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/train_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/__pycache__"
      ],
      "metadata": {
        "id": "TAERW6QsvmVV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy pillow scikit-learn matplotlib torch-geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr7VgahUvqyM",
        "outputId": "7f6373db-a285-4832-df9d-99384699ca4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m849.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_scatter-2.1.2%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (545 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.2/545.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_sparse-0.6.18%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_cluster-1.6.3%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.1/781.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cpu torch-scatter-2.1.2+pt26cpu torch-sparse-0.6.18+pt26cpu torch-spline-conv-1.2.2+pt26cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -A 5 \"class PMICL\" /content/pmicl_model.py\n",
        "!grep -A 5 \"def build_graph\" /content/pmicl_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbKvuSaJvyPC",
        "outputId": "3d31c11c-f79b-49a8-adf0-a66563d66edd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class PMICL(nn.Module):\n",
            "    def __init__(self, num_classes=NUM_CLASSES):\n",
            "        super(PMICL, self).__init__()\n",
            "        self.encoder = PatchEncoder()\n",
            "        self.gnn = GNN()\n",
            "        self.attention_mil = AttentionMIL()\n",
            "    def build_graph(self, patches):\n",
            "        try:\n",
            "            # Check if patches is already a NumPy array\n",
            "            if isinstance(patches, np.ndarray):\n",
            "                patch_features = patches\n",
            "            else:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pmicl_model import PMICL, GraphConstructor, graph_loss\n",
        "print(\"Imported PMICL, GraphConstructor, and graph_loss successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2VsDOHzwswm",
        "outputId": "6d44cbc6-76e2-4303-c0ef-a157c23bd1ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported PMICL, GraphConstructor, and graph_loss successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/train_eval.py"
      ],
      "metadata": {
        "id": "dcxZXbaJwx9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "fff39550-b22c-464f-8748-597369a21dff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Very mild Dementia', 'Non Demented', 'Mild Dementia', 'Moderate Dementia']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1000 images: 778 CN, 159 MCI, 58 Mild, 5 Moderate\n",
            "Epoch 1/10\n",
            "Loss: 2.1391, Cls: 1.3881, Proto: 1.4724, Graph: 0.0494\n",
            "Val: Acc=0.2750, F1=0.1078, AUC=0.4919\n",
            "Epoch 2/10\n",
            "Loss: 2.1015, Cls: 1.3857, Proto: 1.4273, Graph: 0.0070\n",
            "Val: Acc=0.2500, F1=0.1000, AUC=0.5031\n",
            "Epoch 3/10\n",
            "Loss: 2.1049, Cls: 1.3876, Proto: 1.4323, Graph: 0.0039\n",
            "Val: Acc=0.2670, F1=0.1054, AUC=0.4935\n",
            "Epoch 4/10\n",
            "Loss: 2.0956, Cls: 1.3874, Proto: 1.4148, Graph: 0.0026\n",
            "Val: Acc=0.2620, F1=0.1038, AUC=0.5257\n",
            "Epoch 5/10\n",
            "Loss: 2.0934, Cls: 1.3859, Proto: 1.4137, Graph: 0.0022\n",
            "Val: Acc=0.2410, F1=0.0971, AUC=0.5173\n",
            "Epoch 6/10\n",
            "Loss: 2.0907, Cls: 1.3871, Proto: 1.4059, Graph: 0.0021\n",
            "Val: Acc=0.2590, F1=0.1029, AUC=0.4964\n",
            "Epoch 7/10\n",
            "Loss: 2.0937, Cls: 1.3877, Proto: 1.4109, Graph: 0.0021\n",
            "Val: Acc=0.2550, F1=0.1016, AUC=0.5302\n",
            "Epoch 8/10\n",
            "Loss: 2.0880, Cls: 1.3848, Proto: 1.4052, Graph: 0.0020\n",
            "Val: Acc=0.2640, F1=0.1044, AUC=0.5513\n",
            "Epoch 9/10\n",
            "Loss: 2.0916, Cls: 1.3871, Proto: 1.4079, Graph: 0.0021\n",
            "Val: Acc=0.2300, F1=0.0935, AUC=0.5446\n",
            "Epoch 10/10\n",
            "Loss: 2.0874, Cls: 1.3858, Proto: 1.4018, Graph: 0.0021\n",
            "Val: Acc=0.2480, F1=0.0994, AUC=0.5993\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}