{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK3wCrnw8xrTGj2eJJ9Zib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/pmicl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/oasis_dataset.py\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "ZIP_PATH = '/content/oaisis.zip'\n",
        "EXTRACT_DIR = '/content/oasis_data/'\n",
        "\n",
        "def extract_zip():\n",
        "    try:\n",
        "        if not os.path.exists(ZIP_PATH):\n",
        "            raise FileNotFoundError(f\"{ZIP_PATH} not found. Please upload the file to Colab.\")\n",
        "        os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "        if not os.listdir(EXTRACT_DIR):\n",
        "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(EXTRACT_DIR)\n",
        "            print(f\"Extracted {ZIP_PATH} to {EXTRACT_DIR}\")\n",
        "        else:\n",
        "            print(f\"Directory {EXTRACT_DIR} already contains files, skipping extraction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting ZIP file: {e}\")\n",
        "        raise\n",
        "\n",
        "extract_zip()\n",
        "\n",
        "class PatchExtractor:\n",
        "    def __init__(self, patch_size=32, K=80, spatial_threshold=5):\n",
        "        self.patch_size = patch_size\n",
        "        self.K = K\n",
        "        self.spatial_threshold = spatial_threshold\n",
        "\n",
        "    def extract_patches(self, image, prob_map):\n",
        "        patches = []\n",
        "        coords = []\n",
        "        prob_map_copy = prob_map.copy()\n",
        "        for _ in range(self.K):\n",
        "            max_prob_idx = np.argmax(prob_map_copy)\n",
        "            y, x = np.unravel_index(max_prob_idx, prob_map_copy.shape)\n",
        "            patch = self.get_patch(image, (x, y))\n",
        "            patches.append(patch)\n",
        "            coords.append((x, y))\n",
        "            prob_map_copy = self.mask_neighbors(prob_map_copy, (x, y))\n",
        "        return patches, coords\n",
        "\n",
        "    def get_patch(self, image, center):\n",
        "        half_size = self.patch_size // 2\n",
        "        x, y = center\n",
        "        patch = image[\n",
        "            max(0, y - half_size):y + half_size,\n",
        "            max(0, x - half_size):x + half_size\n",
        "        ]\n",
        "        if patch.shape[0] < self.patch_size or patch.shape[1] < self.patch_size:\n",
        "            patch = np.pad(patch, [(0, max(0, self.patch_size - patch.shape[0])),\n",
        "                                   (0, max(0, self.patch_size - patch.shape[1]))],\n",
        "                           mode='constant')\n",
        "        return patch\n",
        "\n",
        "    def mask_neighbors(self, prob_map, center):\n",
        "        half_size = self.spatial_threshold\n",
        "        x, y = center\n",
        "        prob_map[\n",
        "            max(0, y - half_size):y + half_size + 1,\n",
        "            max(0, x - half_size):x + half_size + 1\n",
        "        ] = 0\n",
        "        return prob_map\n",
        "\n",
        "    def dynamic_sample(self, image, N=20):\n",
        "        candidate_patches = []\n",
        "        candidate_coords = []\n",
        "        stride = self.patch_size // 2\n",
        "        h, w = image.shape\n",
        "        for y in range(0, h - self.patch_size + 1, stride):\n",
        "            for x in range(0, w - self.patch_size + 1, stride):\n",
        "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
        "                candidate_patches.append(patch)\n",
        "                candidate_coords.append((x, y))\n",
        "        indices = np.random.choice(len(candidate_patches), min(N, len(candidate_patches)), replace=False)\n",
        "        return [candidate_patches[i] for i in indices], [candidate_coords[i] for i in indices]\n",
        "\n",
        "class OasisDataset(Dataset):\n",
        "    def __init__(self, data_dir, patch_size=32, n_sampled_patches=20):\n",
        "        self.data_dir = os.path.join(data_dir, 'Data')\n",
        "        self.patch_size = patch_size\n",
        "        self.n_sampled_patches = n_sampled_patches\n",
        "        self.patch_extractor = PatchExtractor(patch_size=patch_size)\n",
        "        self.class_map = {\n",
        "            'Non Demented': 0,\n",
        "            'Very mild Dementia': 1,\n",
        "            'Mild Dementia': 2,\n",
        "            'Moderate Dementia': 3\n",
        "        }\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        print(f\"Looking for images in {self.data_dir}\")\n",
        "        if not os.path.exists(self.data_dir):\n",
        "            raise FileNotFoundError(f\"Data directory {self.data_dir} not found\")\n",
        "\n",
        "        available_dirs = os.listdir(self.data_dir)\n",
        "        print(f\"Available directories: {available_dirs}\")\n",
        "\n",
        "        class_valid_paths = {}\n",
        "        for class_name in self.class_map:\n",
        "            matching_dir = next((d for d in available_dirs if d.lower() == class_name.lower()), None)\n",
        "            if not matching_dir:\n",
        "                print(f\"Warning: No directory found for {class_name}\")\n",
        "                continue\n",
        "            class_dir = os.path.join(self.data_dir, matching_dir)\n",
        "            print(f\"Checking directory: {class_dir}\")\n",
        "            img_files = [f for f in os.listdir(class_dir) if f.lower().endswith('.jpg')]\n",
        "            img_paths = [os.path.join(class_dir, f) for f in img_files]\n",
        "            print(f\"Found {len(img_paths)} .jpg files in {class_dir}\")\n",
        "\n",
        "            valid_paths = []\n",
        "            for img_path in img_paths:\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()\n",
        "                    valid_paths.append(img_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Failed to load {img_path}: {e}\")\n",
        "            class_valid_paths[class_name] = valid_paths\n",
        "\n",
        "        if not class_valid_paths:\n",
        "            raise ValueError(f\"No valid images found in {self.data_dir}.\")\n",
        "\n",
        "        min_images = min(len(paths) for paths in class_valid_paths.values())\n",
        "        self.images_per_class = min_images\n",
        "\n",
        "        for class_name in self.class_map:\n",
        "            if class_name not in class_valid_paths:\n",
        "                continue\n",
        "            valid_paths = class_valid_paths[class_name]\n",
        "            sampled_paths = np.random.choice(valid_paths, self.images_per_class, replace=False)\n",
        "            self.image_paths.extend(sampled_paths)\n",
        "            self.labels.extend([self.class_map[class_name]] * self.images_per_class)\n",
        "\n",
        "        print(f\"Loaded {len(self.image_paths)} images: \"\n",
        "              f\"{len([l for l in self.labels if l == 0])} CN, \"\n",
        "              f\"{len([l for l in self.labels if l == 1])} MCI, \"\n",
        "              f\"{len([l for l in self.labels if l == 2])} Mild, \"\n",
        "              f\"{len([l for l in self.labels if l == 3])} Moderate\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = np.array(Image.open(img_path).convert('L')) / 255.0\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            raise\n",
        "        prob_map = np.random.rand(image.shape[0], image.shape[1])\n",
        "        patches, coords = self.patch_extractor.extract_patches(image, prob_map)\n",
        "        sampled_patches, sampled_coords = self.patch_extractor.dynamic_sample(image, self.n_sampled_patches)\n",
        "        patches_tensor = torch.tensor(sampled_patches, dtype=torch.float).unsqueeze(1)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        print(f\"__getitem__: sampled_coords length: {len(sampled_coords)}\")\n",
        "        return patches_tensor, label_tensor, sampled_coords\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    patches = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.stack([item[1] for item in batch])\n",
        "    coords = [item[2] for item in batch]  # List of length batch_size, each with n_sampled_patches coords\n",
        "    print(f\"custom_collate_fn: batch size: {len(batch)}, coords length: {len(coords)}, coords[0] length: {len(coords[0])}\")\n",
        "    return patches, labels, coords\n",
        "\n",
        "def get_dataloader(data_dir, batch_size=2):\n",
        "    dataset = OasisDataset(data_dir, patch_size=32, n_sampled_patches=20)\n",
        "    class_counts = np.bincount(dataset.labels)\n",
        "    weights = 1.0 / class_counts[dataset.labels]\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=0, collate_fn=custom_collate_fn)\n",
        "    return dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEboLVMEzFcF",
        "outputId": "9b36c4b5-c507-4cec-ee2b-ec0a7ee6cf50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/oasis_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from oasis_dataset import OasisDataset\n",
        "import numpy as np\n",
        "try:\n",
        "    dataset = OasisDataset('/content/oasis_data/', subset_size=1000)\n",
        "    print(f\"Class distribution: {np.bincount(dataset.labels)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Dataset error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Ks-LxI4xTK",
        "outputId": "31ed9df7-28bd-4a7b-8b1e-e727a16ecec9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted /content/oaisis.zip to /content/oasis_data/\n",
            "Dataset error: OasisDataset.__init__() got an unexpected keyword argument 'subset_size'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from oasis_dataset import get_dataloader\n",
        "try:\n",
        "    dataloader = get_dataloader('/content/oasis_data/', batch_size=2)\n",
        "    for patches, labels, coords in dataloader:\n",
        "        print(\"Patches shape:\", patches.shape)  # Expected: [2, 20, 1, 32, 32]\n",
        "        print(\"Labels shape:\", labels.shape)   # Expected: [2]\n",
        "        print(\"Coords length:\", len(coords))   # Expected: 2\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"DataLoader error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiFVnIT6oPYp",
        "outputId": "bbb8529b-5970-4ad0-ee95-5a122827c037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1952 images: 488 CN, 488 MCI, 488 Mild, 488 Moderate\n",
            "Patches shape: torch.Size([2, 20, 1, 32, 32])\n",
            "Labels shape: torch.Size([2])\n",
            "Coords length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/oasis_dataset.py:160: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  patches_tensor = torch.tensor(sampled_patches, dtype=torch.float).unsqueeze(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pmicl_model"
      ],
      "metadata": {
        "id": "utWP-fX8TDvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pmicl_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GraphConstructor:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def build_graph(self, features):\n",
        "        from sklearn.neighbors import kneighbors_graph\n",
        "        adj = kneighbors_graph(features, n_neighbors=self.k, mode='connectivity', include_self=False)\n",
        "        edge_index = torch.tensor(adj.nonzero(), dtype=torch.long)\n",
        "        edge_weight = torch.ones(edge_index.shape[1], dtype=torch.float)\n",
        "        return edge_index, edge_weight\n",
        "\n",
        "def graph_loss(embeddings, edge_index, edge_weight):\n",
        "    # embeddings: [num_nodes, embed_dim], e.g., [B*N, 128]\n",
        "    print(f\"graph_loss: embeddings shape: {embeddings.shape}, edge_index max: {edge_index.max()}\")\n",
        "    source = embeddings[edge_index[0]]\n",
        "    target = embeddings[edge_index[1]]\n",
        "    similarity = F.cosine_similarity(source, target, dim=-1)\n",
        "    loss = -torch.mean(edge_weight * similarity)  # Scalar loss\n",
        "    return loss\n",
        "\n",
        "class PMICL(nn.Module):\n",
        "    def __init__(self, num_classes=4, embed_dim=128, num_prototypes=10):\n",
        "        super(PMICL, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, embed_dim)\n",
        "        self.gcn = GCNConv(embed_dim, embed_dim)\n",
        "        self.fc2 = nn.Linear(embed_dim, num_classes)\n",
        "        self.prototypes = nn.Parameter(torch.randn(num_prototypes, embed_dim))\n",
        "\n",
        "    def forward(self, patches, edge_index, batch, labels=None):\n",
        "        B, N, C, H, W = patches.shape  # e.g., [2, 20, 1, 32, 32]\n",
        "        print(f\"forward: patches shape: {patches.shape}, edge_index shape: {edge_index.shape}, batch shape: {batch.shape}\")\n",
        "        # CNN feature extraction\n",
        "        x = patches.view(B * N, C, H, W)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(B * N, -1)\n",
        "        x = F.relu(self.fc1(x))  # Patch embeddings: [B*N, embed_dim], e.g., [40, 128]\n",
        "\n",
        "        # GCN\n",
        "        x_gcn = F.relu(self.gcn(x, edge_index))  # [B*N, embed_dim]\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = F.softmax(torch.matmul(x_gcn, x_gcn.t()) / (x_gcn.shape[-1] ** 0.5), dim=-1)  # [B*N, B*N], e.g., [40, 40]\n",
        "        x = torch.matmul(attention_weights, x_gcn)  # [B*N, embed_dim]\n",
        "\n",
        "        # Graph loss on patch-level embeddings\n",
        "        graph_loss_val = graph_loss(x, edge_index, torch.ones(edge_index.shape[1], device=x.device))\n",
        "\n",
        "        # Global pooling per image\n",
        "        x = x.view(B, N, -1)  # [B, N, embed_dim]\n",
        "        x = x.mean(dim=1)  # [B, embed_dim], e.g., [2, 128]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.fc2(x)  # [B, num_classes], e.g., [2, 4]\n",
        "\n",
        "        # Classification loss\n",
        "        cls_loss = F.cross_entropy(logits, labels) if labels is not None else torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "        # Prototype loss\n",
        "        proto_dist = torch.cdist(x, self.prototypes)  # [B, num_prototypes], e.g., [2, 10]\n",
        "        proto_loss = proto_dist.mean()  # Scalar\n",
        "\n",
        "        return logits, cls_loss, proto_loss, graph_loss_val, attention_weights"
      ],
      "metadata": {
        "id": "sCSvS3SDhlYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b29310f-3d62-41b0-910d-9ad68bf15d2b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/pmicl_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train_eval\n"
      ],
      "metadata": {
        "id": "C8IcjcGCukLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/train_eval.py\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from oasis_dataset import get_dataloader\n",
        "from pmicl_model import PMICL, GraphConstructor\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 2  # Match dataloader test\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 4  # CN, MCI, Mild, Moderate\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    cls_loss_total = 0\n",
        "    proto_loss_total = 0\n",
        "    graph_loss_total = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    graph_constructor = GraphConstructor()\n",
        "\n",
        "    for patches, labels, coords in dataloader:\n",
        "        patches, labels = patches.to(device), labels.to(device)\n",
        "        B, N, C, H, W = patches.shape\n",
        "        patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "        print(f\"train_epoch: patch_features shape: {patch_features.shape}, B: {B}, N: {N}\")\n",
        "        edge_index, edge_weight = graph_constructor.build_graph(patch_features)\n",
        "        print(f\"train_epoch: edge_index shape: {edge_index.shape}, edge_index max: {edge_index.max()}\")\n",
        "        edge_index, edge_weight = edge_index.to(device), edge_weight.to(device)\n",
        "        batch = torch.repeat_interleave(torch.arange(B, device=device), N)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, cls_loss, proto_loss, graph_loss, _ = model(patches, edge_index, batch, labels)\n",
        "        loss = cls_loss + proto_loss + graph_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        cls_loss_total += cls_loss.item()\n",
        "        proto_loss_total += proto_loss.item()\n",
        "        graph_loss_total += graph_loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    return (total_loss / len(dataloader), cls_loss_total / len(dataloader),\n",
        "            proto_loss_total / len(dataloader), graph_loss_total / len(dataloader), train_acc)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    graph_constructor = GraphConstructor()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for patches, labels, coords in dataloader:\n",
        "            patches, labels = patches.to(device), labels.to(device)\n",
        "            B, N, C, H, W = patches.shape\n",
        "            patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "            edge_index, edge_weight = graph_constructor.build_graph(patch_features)\n",
        "            edge_index, edge_weight = edge_index.to(device), edge_weight.to(device)\n",
        "            batch = torch.repeat_interleave(torch.arange(B, device=device), N)\n",
        "\n",
        "            logits, _, _, _, _ = model(patches, edge_index, batch)\n",
        "            preds.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "\n",
        "    acc = accuracy_score(true_labels, np.argmax(preds, axis=1))\n",
        "    f1 = f1_score(true_labels, np.argmax(preds, axis=1), average='macro')\n",
        "    auc = roc_auc_score(true_labels, preds, multi_class='ovr')\n",
        "    return acc, f1, auc, true_labels, np.argmax(preds, axis=1)\n",
        "\n",
        "def generate_attention_map(model, dataloader, device):\n",
        "    model.eval()\n",
        "    graph_constructor = GraphConstructor()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for patches, _, coords in dataloader:\n",
        "            patches = patches.to(device)\n",
        "            B, N, C, H, W = patches.shape\n",
        "            patch_features = patches.view(B * N, -1).cpu().numpy()\n",
        "            edge_index, edge_weight = graph_constructor.build_graph(patch_features)\n",
        "            edge_index, edge_weight = edge_index.to(device), edge_weight.to(device)\n",
        "            batch = torch.repeat_interleave(torch.arange(B, device=device), N)\n",
        "\n",
        "            _, _, _, _, attention_weights = model(patches, edge_index, batch)\n",
        "            attention_weights = attention_weights.cpu().numpy()  # [B*N, B*N], e.g., [40, 40]\n",
        "\n",
        "            # Aggregate attention weights per image\n",
        "            attention_per_image = attention_weights[:N, :N]  # First image’s attention, [N, N], e.g., [20, 20]\n",
        "\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(attention_per_image, cmap='hot')\n",
        "            plt.colorbar()\n",
        "            plt.title(\"Attention Map\")\n",
        "            plt.savefig('/content/attention_map.png')\n",
        "            plt.close()\n",
        "            break\n",
        "\n",
        "def plot_confusion_matrix(true_labels, pred_labels, class_names):\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig('/content/confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(train_accs, val_accs, val_f1s, val_aucs, losses):\n",
        "    epochs = range(1, len(train_accs) + 1)\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_accs, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('/content/accuracy_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Validation Metrics Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
        "    plt.plot(epochs, val_f1s, label='Validation F1 Score')\n",
        "    plt.plot(epochs, val_aucs, label='Validation AUC')\n",
        "    plt.title('Validation Metrics Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('/content/validation_metrics_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, losses, label='Training Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('/content/loss_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    dataloader = get_dataloader('/content/oasis_data/', batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize model\n",
        "    model = PMICL(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Track metrics\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    val_f1s = []\n",
        "    val_aucs = []\n",
        "    losses = []\n",
        "    class_names = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "        loss, cls_loss, proto_loss, graph_loss, train_acc = train_epoch(model, dataloader, optimizer, DEVICE)\n",
        "        print(f\"Loss: {loss:.4f}, Cls: {cls_loss:.4f}, Proto: {proto_loss:.4f}, Graph: {graph_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        acc, f1, auc, true_labels, pred_labels = evaluate(model, dataloader, DEVICE)\n",
        "        print(f\"Val: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
        "\n",
        "        # Store metrics\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(acc)\n",
        "        val_f1s.append(f1)\n",
        "        val_aucs.append(auc)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # Generate plots\n",
        "    plot_confusion_matrix(true_labels, pred_labels, class_names)\n",
        "    plot_metrics(train_accs, val_accs, val_f1s, val_aucs, losses)\n",
        "    generate_attention_map(model, dataloader, DEVICE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daNd8mzsvi0m",
        "outputId": "a2183d65-ecde-4938-a4f5-8f3fac18bc0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/train_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/__pycache__"
      ],
      "metadata": {
        "id": "TAERW6QsvmVV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from oasis_dataset import get_dataloader\n",
        "dataloader = get_dataloader('/content/oasis_data/', batch_size=2)\n",
        "patches, labels, coords = next(iter(dataloader))\n",
        "print(f\"Patches shape: {patches.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Coords length: {len(coords)}\")\n",
        "print(f\"Coords[0] length: {len(coords[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lYkV-bJxEvz",
        "outputId": "41587f9c-60ab-45a8-c7cf-12094b102f4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1952 images: 488 CN, 488 MCI, 488 Mild, 488 Moderate\n",
            "Patches shape: torch.Size([2, 20, 1, 32, 32])\n",
            "Labels shape: torch.Size([2])\n",
            "Coords length: 20\n",
            "Coords[0] length: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy pillow scikit-learn matplotlib torch-geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr7VgahUvqyM",
        "outputId": "ab6abcd0-5a27-4c5c-f9dc-fd289571ccb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_scatter-2.1.2%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (545 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.2/545.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_sparse-0.6.18%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_cluster-1.6.3%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.1/781.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt26cpu-cp311-cp311-linux_x86_64.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m859.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cpu torch-scatter-2.1.2+pt26cpu torch-sparse-0.6.18+pt26cpu torch-spline-conv-1.2.2+pt26cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/train_eval.py"
      ],
      "metadata": {
        "id": "dcxZXbaJwx9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "ae30d144-71ef-424d-b1a8-25bc33a4e388"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in /content/oasis_data/Data\n",
            "Available directories: ['Mild Dementia', 'Very mild Dementia', 'Moderate Dementia', 'Non Demented']\n",
            "Checking directory: /content/oasis_data/Data/Non Demented\n",
            "Found 67222 .jpg files in /content/oasis_data/Data/Non Demented\n",
            "Checking directory: /content/oasis_data/Data/Very mild Dementia\n",
            "Found 13725 .jpg files in /content/oasis_data/Data/Very mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Mild Dementia\n",
            "Found 5002 .jpg files in /content/oasis_data/Data/Mild Dementia\n",
            "Checking directory: /content/oasis_data/Data/Moderate Dementia\n",
            "Found 488 .jpg files in /content/oasis_data/Data/Moderate Dementia\n",
            "Loaded 1952 images: 488 CN, 488 MCI, 488 Mild, 488 Moderate\n",
            "Epoch 1/10\n",
            "train_epoch: patch_features shape: (40, 1024), B: 2, N: 20\n",
            "train_epoch: edge_index shape: torch.Size([2, 200]), edge_index max: 39\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for dimension 0 with size 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/train_eval.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/train_eval.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss:.4f}, Cls: {cls_loss:.4f}, Proto: {proto_loss:.4f}, Graph: {graph_loss:.4f}, Train Acc: {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train_eval.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproto_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgraph_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pmicl_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, patches, edge_index, batch, labels)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Prototype loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mproto_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, num_prototypes], e.g., [2, 10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mproto_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pmicl_model.py\u001b[0m in \u001b[0;36mgraph_loss\u001b[0;34m(embeddings, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgraph_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# embeddings: [num_nodes, embed_dim], e.g., [B*N, 128]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"graph_loss: embeddings shape: {embeddings.shape}, edge_index max: {edge_index.max()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
          ]
        }
      ]
    }
  ]
}