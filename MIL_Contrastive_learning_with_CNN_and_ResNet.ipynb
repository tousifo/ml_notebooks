{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IrigU7kyBXt8q9uekt9_vGs5ErNy63ZY",
      "authorship_tag": "ABX9TyNPU+nj9vKgYdMsuXZ/pDBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/MIL_Contrastive_learning_with_CNN_and_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess"
      ],
      "metadata": {
        "id": "aFX3KdLbB17g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5QSL0JEBIH5",
        "outputId": "190f449f-669f-4e80-c01c-cc3e9499db06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data root: /content/oasis_data/Data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import zipfile\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Path & Extraction for dataset\n",
        "ZIP_PATH = \"/content/drive/MyDrive/oaisis.zip\"  # Path to your dataset ZIP file\n",
        "EXTRACT_DIR = \"/content/oasis_data\"  # Path to unzip dataset\n",
        "\n",
        "# Extract the dataset if not already extracted\n",
        "if not os.path.exists(EXTRACT_DIR) or not os.listdir(EXTRACT_DIR):\n",
        "    print(f\"Extracting {ZIP_PATH} -> {EXTRACT_DIR}...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "# Label Mapping for different classes\n",
        "LABEL_MAP = {\n",
        "    \"Non Demented\": 0,\n",
        "    \"Very mild Dementia\": 1,\n",
        "    \"Mild Dementia\": 2,\n",
        "    \"Moderate Dementia\": 3\n",
        "}\n",
        "\n",
        "# Finding the data root directory (where class folders are)\n",
        "DATA_ROOT = None\n",
        "for cand in [EXTRACT_DIR] + [os.path.join(EXTRACT_DIR, d) for d in os.listdir(EXTRACT_DIR)]:\n",
        "    if all(os.path.isdir(os.path.join(cand, lbl)) for lbl in LABEL_MAP):\n",
        "        DATA_ROOT = cand\n",
        "        break\n",
        "if DATA_ROOT is None:\n",
        "    raise RuntimeError(f\"Could not find class folders under {EXTRACT_DIR}\")\n",
        "print(\"Using data root:\", DATA_ROOT)\n",
        "\n",
        "# Build samples and split data into train and validation sets\n",
        "all_samples = []\n",
        "for label_name, lbl in LABEL_MAP.items():\n",
        "    pattern = os.path.join(DATA_ROOT, label_name, \"*.jpg\")\n",
        "    files = glob.glob(pattern)\n",
        "    if not files:\n",
        "        raise RuntimeError(f\"No images for '{label_name}' in {pattern}\")\n",
        "    all_samples += [(fp, lbl) for fp in files]\n",
        "\n",
        "# Splitting data (80% train, 20% validation)\n",
        "paths, labels = zip(*all_samples)\n",
        "labels = np.array(labels)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(sss.split(paths, labels))\n",
        "train_list = [all_samples[i] for i in train_idx]\n",
        "val_list = [all_samples[i] for i in val_idx]\n",
        "\n",
        "# Compute class weights to handle imbalanced data\n",
        "class_counts = np.bincount(labels[train_idx], minlength=len(LABEL_MAP))\n",
        "class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float32)\n",
        "sample_weights = [class_weights[lbl].item() for _, lbl in train_list]\n",
        "train_sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# Transformations for data augmentation\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize(280),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Dataset class for loading the patches\n",
        "class OASISPatchDataset(Dataset):\n",
        "    def __init__(self, samples, transform):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        fp, lbl = self.samples[i]\n",
        "        img = Image.open(fp).convert('L')  # Convert image to grayscale\n",
        "        img_t = self.transform(img)\n",
        "        return img_t, lbl\n",
        "\n",
        "# Dataset objects\n",
        "train_ds = OASISPatchDataset(train_list, train_tf)\n",
        "val_ds = OASISPatchDataset(val_list, val_tf)\n",
        "\n",
        "# DataLoader objects for batch processing\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MIL Model"
      ],
      "metadata": {
        "id": "yc-dh5SBB-Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simpler CNN model for feature extraction\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 64 * 64, embed_dim)  # Now embedding dimension matches output\n",
        "        # No need for the final classification layer here, only embeddings are needed\n",
        "        # self.fc2 = nn.Linear(256, num_classes)  # Classification layer if needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten the tensor to feed into fully connected layers\n",
        "        x = torch.flatten(x, 1)  # Flatten from the second dimension (batch size is kept as-is)\n",
        "\n",
        "        # Fully connected layers to get the embeddings (output dimension matches the embedding size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x  # Embeddings for contrastive loss"
      ],
      "metadata": {
        "id": "RtgVJzGlCAMC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contrastive Learning"
      ],
      "metadata": {
        "id": "iEZIijmwCBMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contrastive loss function for learning prototypes\n",
        "class PrototypeContrastiveLoss(nn.Module):\n",
        "    def __init__(self, n_cls, dim, temp):\n",
        "        super().__init__()\n",
        "        self.protos = nn.Parameter(torch.randn(n_cls, dim))  # Create class prototypes (n_cls x dim)\n",
        "        self.temp = temp\n",
        "\n",
        "    def forward(self, emb, labels):\n",
        "        B, D = emb.shape  # B: batch size, D: embedding dimension\n",
        "        # Ensure the prototypes are of the same size as embeddings (B x D)\n",
        "        prototypes = self.protos.unsqueeze(0).expand(B, -1, -1)  # Expand prototypes to match batch size\n",
        "        # Compute cosine similarity between embeddings and prototypes\n",
        "        sims = F.cosine_similarity(emb.unsqueeze(1), prototypes, dim=-1) / self.temp\n",
        "        return F.cross_entropy(sims, labels)  # Return cross-entropy loss"
      ],
      "metadata": {
        "id": "o-On6IWhCEy_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train & Evaluate"
      ],
      "metadata": {
        "id": "ARSlB16ECH6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined MIL and Contrastive Learning Model\n",
        "class CombinedMILContrastiveCNN(nn.Module):\n",
        "    def __init__(self, embed_dim, num_classes, temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.cnn = SimpleCNN(embed_dim)  # Output embeddings from CNN\n",
        "        self.prot_loss = PrototypeContrastiveLoss(num_classes, embed_dim, temperature)\n",
        "\n",
        "    def forward(self, patches, labels=None):\n",
        "        B, C, H, W = patches.shape  # Get batch size, channels, height, and width\n",
        "\n",
        "        # Apply CNN to all patches\n",
        "        emb = self.cnn(patches)  # Now emb is of shape [B, embed_dim]\n",
        "\n",
        "        # Contrastive loss\n",
        "        p_loss = self.prot_loss(emb, labels) if labels is not None else 0\n",
        "\n",
        "        return emb, p_loss\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model, train_loader, val_loader, epochs=10, lr=1e-4, proto_weight=0.1):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        all_p, all_t = [], []\n",
        "        for patches, labels in tqdm(train_loader, desc=f\"Epoch {ep + 1}\"):\n",
        "            patches, labels = patches.to(DEVICE), labels.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            emb, p_loss = model(patches, labels)\n",
        "            c_loss = F.cross_entropy(emb.view(-1, emb.shape[-1]), labels.repeat(patches.shape[1]))\n",
        "            loss = (1 - proto_weight) * c_loss + proto_weight * p_loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            preds = emb.argmax(1)  # Changed to emb to reflect final prediction layer\n",
        "            all_p += preds.cpu().tolist()\n",
        "            all_t += labels.cpu().tolist()\n",
        "\n",
        "        print(f\"Epoch {ep + 1} F1 Score: {f1_score(all_t, all_p, average='weighted'):.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for patches, labels in tqdm(val_loader, desc=f\"Evaluating Epoch {ep + 1}\"):\n",
        "                patches = patches.to(DEVICE)\n",
        "                emb, _ = model(patches)\n",
        "                preds = emb.argmax(1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.numpy())\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "# Device setup\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CombinedMILContrastiveCNN(embed_dim=128, num_classes=4, temperature=0.1).to(DEVICE)\n",
        "\n",
        "# Train and evaluate\n",
        "train_and_evaluate(model, train_loader, val_loader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bLo2-ICKiU",
        "outputId": "77d0fc80-7195-455f-b7e0-8c3007e0fe65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 4322/4322 [04:19<00:00, 16.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 F1 Score: 0.4923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 1: 100%|██████████| 1081/1081 [00:43<00:00, 25.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 4322/4322 [04:25<00:00, 16.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 F1 Score: 0.5824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 2: 100%|██████████| 1081/1081 [00:43<00:00, 25.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 4322/4322 [04:16<00:00, 16.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 F1 Score: 0.6222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 3: 100%|██████████| 1081/1081 [00:43<00:00, 24.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 4322/4322 [04:16<00:00, 16.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 F1 Score: 0.6546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 4: 100%|██████████| 1081/1081 [00:42<00:00, 25.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 4322/4322 [04:21<00:00, 16.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 F1 Score: 0.6784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 5: 100%|██████████| 1081/1081 [00:43<00:00, 24.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 4322/4322 [04:26<00:00, 16.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 F1 Score: 0.6977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 6: 100%|██████████| 1081/1081 [00:44<00:00, 24.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 4322/4322 [04:18<00:00, 16.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 F1 Score: 0.7114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 7: 100%|██████████| 1081/1081 [00:42<00:00, 25.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 4322/4322 [04:20<00:00, 16.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 F1 Score: 0.7234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 8: 100%|██████████| 1081/1081 [00:42<00:00, 25.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 4322/4322 [04:18<00:00, 16.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 F1 Score: 0.7281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 9: 100%|██████████| 1081/1081 [00:44<00:00, 24.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 4322/4322 [04:19<00:00, 16.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 F1 Score: 0.7352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Epoch 10: 100%|██████████| 1081/1081 [00:42<00:00, 25.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.7685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}