{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/ALS_QNN_PRO_ACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJL4ko9VLqi"
      },
      "source": [
        "## Cell 1 — Imports & setup for the ALS QNN notebook\n",
        "\n",
        "**Purpose:** bring in the sklearn utilities, pin/install Qiskit, and import the quantum pieces used by a variational regressor.\n",
        "\n",
        "### What’s in this cell\n",
        "- **Data utilities:** `train_test_split`, `MinMaxScaler`\n",
        "- **Metrics:** `mean_squared_error`, `pearsonr`\n",
        "- **Install (pinned):** `%pip install qiskit~=1.0 qiskit-machine-learning~=0.8.1 qiskit_algorithms`\n",
        "- **Quantum stack:** `ZZFeatureMap`, `RealAmplitudes`, `COBYLA`, `VQR`, `Sampler`\n",
        "\n",
        "> **Note:** If the install line complains about `qiskit_algorithms`, the PyPI package name is often `qiskit-algorithms` (hyphen). We’re not changing your code here—this is just a heads-up.\n",
        "\n",
        "### Why scale features?\n",
        "Angle encoders work better when inputs live in a tight range; min–max scaling avoids angle wraparound and makes training steadier.\n",
        "\n",
        "<details>\n",
        "<summary>Quick I/O expectations</summary>\n",
        "After preprocessing later:\n",
        "- Feature vectors will be scaled to a small range (often [0, 1]).\n",
        "- The `ZZFeatureMap(feature_dimension=...)` should match your final number of features.\n",
        "</details>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "x2alDHQpyaLS",
        "outputId": "e1ba93a9-abcd-4ef8-ef6c-f006f7d02e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit~=1.0\n",
            "  Downloading qiskit-1.4.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit-machine-learning~=0.8.1\n",
            "  Downloading qiskit_machine_learning-0.8.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting qiskit_algorithms\n",
            "  Downloading qiskit_algorithms-0.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit~=1.0)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (1.16.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (2.9.0.post0)\n",
            "Collecting stevedore>=3.0.0 (from qiskit~=1.0)\n",
            "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (4.15.0)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit~=1.0)\n",
            "  Downloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting scipy>=1.5 (from qiskit~=1.0)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning~=0.8.1) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning~=0.8.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit~=1.0) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning~=0.8.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning~=0.8.1) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit~=1.0) (1.3.0)\n",
            "Downloading qiskit-1.4.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_machine_learning-0.8.4-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_algorithms-0.4.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, stevedore, scipy, rustworkx, qiskit, qiskit-machine-learning, qiskit_algorithms\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "Successfully installed qiskit-1.4.4 qiskit-machine-learning-0.8.4 qiskit_algorithms-0.4.0 rustworkx-0.17.1 scipy-1.15.3 stevedore-5.5.0 symengine-0.13.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "35002981835e4bf0ac2e991e544fd9b3",
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split  # quick train/validation split\n",
        "from sklearn.preprocessing import MinMaxScaler        # keep features in a compact range for angle encoding\n",
        "from sklearn.metrics import mean_squared_error        # regression loss (lower is better)\n",
        "from scipy.stats import pearsonr                      # correlation between predictions and targets (closer to 1 is better)\n",
        "%pip install qiskit~=1.0 qiskit-machine-learning~=0.8.1 qiskit_algorithms  # pinned install; if it fails, try 'qiskit-algorithms' manually\n",
        "\n",
        "# Qiskit Imports\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes  # feature map + ansatz for the variational circuit\n",
        "from qiskit_algorithms.optimizers import COBYLA                  # gradient-free optimizer suited to noisy objectives\n",
        "from qiskit_machine_learning.algorithms.regressors import VQR    # variational quantum regressor wrapper\n",
        "from qiskit.primitives import Sampler                            # primitive that evaluates circuits (shot-based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LFlmvTIW-NT"
      },
      "source": [
        "## Cell 2 — CV-safe PRO-ACT preprocessing (`ALSDataProcessor`)\n",
        "\n",
        "**Purpose:** Turn the raw PRO-ACT CSVs into a clean, cross-validation-safe feature matrix `X` and target `y` (ALSFRS Total slope), then save a single file you can feed into any model (classical or quantum).\n",
        "\n",
        "### What this cell does\n",
        "- Anchors timelines to each subject’s **first ALSFRS visit** (t = 0).\n",
        "- Builds features from the **first 0–90 days** after that anchor across longitudinal tables (ALSFRS, FVC, vitals, labs, grip, muscle).\n",
        "- Harmonizes **ALSFRS-R** subitems when present (Q10 from 10a; merges Q5a/Q5b).\n",
        "- Collapses **FVC trials** to the **max per test time** before summarizing.\n",
        "- Creates **7 summaries** per signal: `min, max, median, std, first, last, slope`.\n",
        "- Drops columns with **>30% missing**, but does **no scaling/encoding/imputation** here (to avoid leakage).\n",
        "- Keeps only **eligible subjects**: have an ALSFRS measure **>3 months** and **>12 months** after anchor.\n",
        "- Saves `final_processed_als_data.csv` and returns a dict with `X`, `y`, `subject_ids`, and the raw joined frame.\n",
        "\n",
        "### Expected inputs (filenames)\n",
        "- `PROACT_ALSFRS.csv` *(required)*, plus any of:  \n",
        "  `PROACT_FVC.csv`, `PROACT_VITALSIGNS.csv`, `PROACT_RILUZOLE.csv`, `PROACT_DEMOGRAPHICS.csv`,  \n",
        "  `PROACT_LABS.csv`, `PROACT_DEATHDATA.csv`, `PROACT_HANDGRIPSTRENGTH.csv`,  \n",
        "  `PROACT_MUSCLESTRENGTH.csv`, `PROACT_ALSHISTORY.csv`.\n",
        "\n",
        "> **Key columns the code looks for**\n",
        "> - `subject_id` (auto-detected if a similar name exists)\n",
        "> - Any time field containing **“delta”** or **“day”** (used as days since that table’s baseline)\n",
        "> - In ALSFRS: `ALSFRS_Total` (numeric)\n",
        "\n",
        "### Outputs\n",
        "- **File:** `final_processed_als_data.csv`  \n",
        "- **Return (dict):**  \n",
        "  - `X` → features (after 30% missing filter)  \n",
        "  - `y` → `alsfrs_slope` (per-subject target)  \n",
        "  - `subject_ids` → subject mapping  \n",
        "  - `raw_frame` → `[subject_id, alsfrs_slope, X...]` (what gets saved)\n",
        "\n",
        "> **Windowing:** all longitudinal features are computed over **0–90 days** *from the ALSFRS anchor*.  \n",
        "> **Slope target:** computed between the **first point after 3 months** and the **first point after 12 months**.\n",
        "\n",
        "### Gotchas / tips\n",
        "- If a table has **no time column**, it’s skipped with a warning.\n",
        "- **Pivoting** of wide/long lab/grip/muscle tables is “best effort” by name patterns; if that fails, it continues safely.\n",
        "- **No encodings** are done here. Do one-hot/ordinal and scaling **inside your CV folds**.\n",
        "- If your time unit isn’t in days, ensure the column names include `day`/`delta` or rename before loading.\n",
        "\n",
        "<details>\n",
        "<summary>How features are built (quick recipe)</summary>\n",
        "\n",
        "1) **Anchor** each subject’s rows to the first ALSFRS visit (t=0).  \n",
        "2) **Align** other tables to ALSFRS time by subtracting each subject’s ALSFRS anchor day.  \n",
        "3) **Restrict** to days **0–90**.  \n",
        "4) For every numeric signal per table, compute:  \n",
        "   `min, max, median, std (ddof=0), first, last, slope(first→last over time)`  \n",
        "   *(slope is NaN if only one observation or zero time span).*  \n",
        "5) **Merge** all per-table feature blocks on `subject_id`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Eligibility logic for y</summary>\n",
        "A subject is kept if they have **any** ALSFRS measure strictly **>3.0 months** *and* strictly **>12.0 months** after the anchor.  \n",
        "The slope target uses the very first record after 3 months and the very first after 12 months.\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3KCwYyArHq",
        "outputId": "06d30ec2-49e0-4bb6-d4e0-70c97241f487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Starting ALS Data Preprocessing Pipeline ======\n",
            "--- Loading and Inspecting Data ---\n",
            "✓ PROACT_ALSFRS.csv: (73845, 20)\n",
            "✓ PROACT_FVC.csv: (49110, 10)\n",
            "✓ PROACT_VITALSIGNS.csv: (84721, 36)\n",
            "✓ PROACT_RILUZOLE.csv: (10363, 3)\n",
            "✓ PROACT_DEMOGRAPHICS.csv: (12504, 14)\n",
            "✓ PROACT_LABS.csv: (2937162, 5)\n",
            "✓ PROACT_DEATHDATA.csv: (5043, 3)\n",
            "✓ PROACT_HANDGRIPSTRENGTH.csv: (19032, 11)\n",
            "✓ PROACT_MUSCLESTRENGTH.csv: (204875, 10)\n",
            "✓ PROACT_ALSHISTORY.csv: (13765, 16)\n",
            "\n",
            "Calculated ALSFRS slope for 1897 patients.\n",
            "\n",
            "--- Generating Longitudinal Features (anchored to first ALSFRS; window = 0–90 days) ---\n",
            "\n",
            "Eligible patients: 3317 / 8538\n",
            "\n",
            "--- Handling Missing Values (Dropping cols with >30% missing) ---\n",
            "Dropped 1413 columns for >30% missingness.\n",
            "\n",
            "✅ Saved CV-safe engineered data to 'final_processed_als_data.csv'\n",
            "Feature matrix shape: (1897, 346) | Target length: 1897\n",
            "\n",
            "Preview of columns: ['Demographics_Delta', 'Age', 'Race_Caucasian', 'Sex', 'Subject_used_Riluzole', 'Riluzole_use_Delta', 'Subject_ALS_History_Delta', 'Site_of_Onset', 'alsfrs_Q1_Speech_min', 'alsfrs_Q1_Speech_max']\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # keep notebook output tidy; data has mixed types/old columns\n",
        "\n",
        "\n",
        "class ALSDataProcessor:\n",
        "    \"\"\"\n",
        "    CV-safe preprocessing for PRO-ACT to reproduce the paper's EDA:\n",
        "      - Anchor to FIRST ALSFRS visit (t=0) per subject\n",
        "      - Inputs: first 3 months (0–90 days from anchor) for all longitudinal tables\n",
        "      - Outcome: ALSFRS Total slope between FIRST-after-3mo and FIRST-after-12mo\n",
        "      - ALSFRS-R harmonization hooks (Q10 from 10a; merge Q5a/Q5b)\n",
        "      - FVC reduced to max-of-trials per test before summarization\n",
        "      - Seven summaries: min, max, median, std, first, last, slope (slope=NaN if only 1 obs)\n",
        "      - Drop features with >30% missing (no other transforms here — avoid leakage)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # identifiers/time-like columns we should NOT summarize as numeric features\n",
        "        self.id_and_delta_cols = {\n",
        "            \"subject_id\",\n",
        "            \"alsfrs_delta\",\n",
        "            \"fvc_delta\",\n",
        "            \"vitals_delta\",\n",
        "            \"labs_delta\",\n",
        "            \"grip_delta\",\n",
        "            \"muscle_delta\",\n",
        "            \"onset_delta\",\n",
        "            \"death_delta\",\n",
        "            \"history_delta\",\n",
        "            \"anchor_days\",\n",
        "            \"days_from_alsfrs_anchor\",\n",
        "        }\n",
        "\n",
        "    # --------- Utilities ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_time_col(df: pd.DataFrame) -> Optional[str]:\n",
        "        \"\"\"Find a time column that represents days since baseline in a table.\"\"\"\n",
        "        # Prefer delta\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if \"delta\" in lc:\n",
        "                return c\n",
        "        # Fallback to 'days' if present\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if \"day\" in lc:\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    # --------- ALSFRS-R harmonization ---------\n",
        "\n",
        "    def _convert_alsfrs_r(self, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prepare ALSFRS table. If ALSFRS-R subitems exist, map per paper:\n",
        "          - Q10 <- 10a (dyspnea). Ignore 10b/10c.\n",
        "          - Merge Q5a/Q5b into Q5 if present.\n",
        "        If only totals exist, this is a no-op aside from coercions.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "\n",
        "        if \"ALSFRS_Total\" in df.columns:\n",
        "            df[\"ALSFRS_Total\"] = pd.to_numeric(df[\"ALSFRS_Total\"], errors=\"coerce\")\n",
        "\n",
        "        # Try to locate subitems by loose names\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "\n",
        "        # Q10 from 10a (dyspnea)\n",
        "        for candidate in [\"alsfrs_r_q10a\", \"q10a\", \"dyspnea\", \"alsfrs_q10a\"]:\n",
        "            if candidate in cols:\n",
        "                df[\"Q10\"] = pd.to_numeric(df[cols[candidate]], errors=\"coerce\")\n",
        "                break\n",
        "\n",
        "        # Merge Q5a/Q5b\n",
        "        q5a = next(\n",
        "            (cols[k] for k in [\"alsfrs_r_q5a\", \"q5a\", \"cutting_wout_gastrostomy\"] if k in cols),\n",
        "            None,\n",
        "        )\n",
        "        q5b = next(\n",
        "            (cols[k] for k in [\"alsfrs_r_q5b\", \"q5b\", \"cutting_with_gastrostomy\"] if k in cols),\n",
        "            None,\n",
        "        )\n",
        "        if q5a and q5b:\n",
        "            q5a_vals = pd.to_numeric(df[q5a], errors=\"coerce\").values\n",
        "            q5b_vals = pd.to_numeric(df[q5b], errors=\"coerce\").values\n",
        "            df[\"Q5\"] = np.nanmax(np.vstack([q5a_vals, q5b_vals]), axis=0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # --------- Anchoring ---------\n",
        "\n",
        "    def _alsfrs_anchor_days(self, alsfrs_df: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Compute per-subject anchor day = first ALSFRS visit (min delta/days).\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        return anchor_map\n",
        "\n",
        "    # --------- Data I/O ---------\n",
        "\n",
        "    def load_and_inspect_data(self, file_path: str = \"\") -> Dict[str, pd.DataFrame]:\n",
        "        datasets: Dict[str, pd.DataFrame] = {}\n",
        "        file_list = [\n",
        "            \"PROACT_ALSFRS.csv\",\n",
        "            \"PROACT_FVC.csv\",\n",
        "            \"PROACT_VITALSIGNS.csv\",\n",
        "            \"PROACT_RILUZOLE.csv\",\n",
        "            \"PROACT_DEMOGRAPHICS.csv\",\n",
        "            \"PROACT_LABS.csv\",\n",
        "            \"PROACT_DEATHDATA.csv\",\n",
        "            \"PROACT_HANDGRIPSTRENGTH.csv\",\n",
        "            \"PROACT_MUSCLESTRENGTH.csv\",\n",
        "            \"PROACT_ALSHISTORY.csv\",\n",
        "        ]\n",
        "        print(\"--- Loading and Inspecting Data ---\")\n",
        "        for file_name in file_list:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path + file_name, on_bad_lines=\"skip\")\n",
        "                # normalize subject_id\n",
        "                if \"subject_id\" not in df.columns:\n",
        "                    potential = [c for c in df.columns if \"subject\" in c.lower()]\n",
        "                    if potential:\n",
        "                        df = df.rename(columns={potential[0]: \"subject_id\"})\n",
        "                # coerce delta-like numeric columns\n",
        "                for c in df.columns:\n",
        "                    if \"delta\" in c.lower() or \"day\" in c.lower():\n",
        "                        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "                datasets[file_name] = df\n",
        "                print(f\"✓ {file_name}: {df.shape}\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"✗ {file_name}: File not found (skipped).\")\n",
        "        return datasets\n",
        "\n",
        "    # --------- Outcome ---------\n",
        "\n",
        "    def calculate_alsfrs_slope(self, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Outcome = slope between FIRST-after-3mo and FIRST-after-12mo ALSFRS totals,\n",
        "        with time anchored to first ALSFRS visit.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "        if \"ALSFRS_Total\" not in df.columns:\n",
        "            raise ValueError(\"ALSFRS_Total missing in ALSFRS table.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        # Anchor\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        df[\"days_from_anchor\"] = df[\"alsfrs_delta\"] - df[\"subject_id\"].map(anchor_map)\n",
        "        df[\"months\"] = df[\"days_from_anchor\"] / 30.44\n",
        "\n",
        "        df = df.sort_values([\"subject_id\", \"months\"])\n",
        "        slopes = {}\n",
        "\n",
        "        for sid, g in df.groupby(\"subject_id\", sort=False):\n",
        "            g = g.dropna(subset=[\"months\", \"ALSFRS_Total\"])\n",
        "            t1 = g[g[\"months\"] > 3.0].head(1)\n",
        "            t2 = g[g[\"months\"] > 12.0].head(1)\n",
        "            if not t1.empty and not t2.empty:\n",
        "                t1m = float(t1[\"months\"].iloc[0])\n",
        "                t2m = float(t2[\"months\"].iloc[0])\n",
        "                t1v = float(t1[\"ALSFRS_Total\"].iloc[0])\n",
        "                t2v = float(t2[\"ALSFRS_Total\"].iloc[0])\n",
        "                if t2m > t1m:\n",
        "                    slopes[sid] = (t2v - t1v) / (t2m - t1m)\n",
        "\n",
        "        return pd.DataFrame({\"subject_id\": list(slopes.keys()), \"alsfrs_slope\": list(slopes.values())})\n",
        "\n",
        "    # --------- FVC collapse ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def _fvc_collapse_trials(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Reduce FVC per row/time to the max across trials before summarization.\n",
        "        Tries to detect typical trial columns; falls back gracefully.\n",
        "        \"\"\"\n",
        "        d = df.copy()\n",
        "        # Find obvious trial columns\n",
        "        trial_cols = [c for c in d.columns if \"trial\" in c.lower()]\n",
        "        # Some datasets have explicit liters columns per trial name\n",
        "        if trial_cols:\n",
        "            d[\"FVC_Liters\"] = pd.to_numeric(d[trial_cols].max(axis=1), errors=\"coerce\")\n",
        "            keep = [\"subject_id\", time_col, \"FVC_Liters\"]\n",
        "            return d[keep]\n",
        "        # Fallbacks: look for liters column names\n",
        "        liter_like = [c for c in d.columns if \"liter\" in c.lower() or \"fvc\" in c.lower()]\n",
        "        if liter_like:\n",
        "            # If multiple, take row-wise max\n",
        "            d[\"FVC_Liters\"] = pd.to_numeric(d[liter_like].max(axis=1), errors=\"coerce\")\n",
        "            keep = [\"subject_id\", time_col, \"FVC_Liters\"]\n",
        "            return d[keep]\n",
        "        # Last resort: return as-is\n",
        "        return d\n",
        "\n",
        "    # --------- Longitudinal summarization ---------\n",
        "\n",
        "    def create_longitudinal_features(self, df: pd.DataFrame, time_col: str, prefix: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create 7 summaries over [0, 90] days from ALSFRS anchor:\n",
        "          min, max, median, std, first, last, slope(first→last)\n",
        "        Slope remains NaN if only one observation or zero time span.\n",
        "        \"\"\"\n",
        "        if time_col not in df.columns:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        d = df.copy()\n",
        "        # Coerce numerics (but keep subject_id/time cols)\n",
        "        for c in d.columns:\n",
        "            if c not in {\"subject_id\", time_col}:\n",
        "                d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
        "\n",
        "        # Ensure window is 0..90 days from ALSFRS anchor (already anchored)\n",
        "        d = d[(d[time_col] >= 0) & (d[time_col] <= 90)].copy()\n",
        "        if d.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Value columns (exclude identifiers/derived delta/time)\n",
        "        val_cols = [\n",
        "            c\n",
        "            for c in d.select_dtypes(include=[np.number]).columns\n",
        "            if c not in self.id_and_delta_cols and c not in {\"subject_id\", time_col}\n",
        "        ]\n",
        "        if not val_cols:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        out = []\n",
        "        g = d.groupby(\"subject_id\", as_index=True)\n",
        "        for col in val_cols:\n",
        "            agg = g[col].agg([\"min\", \"max\", \"median\", \"first\", \"last\"])\n",
        "            std_ = g[col].std(ddof=0).rename(\"std\")\n",
        "            slope = g.apply(\n",
        "                lambda x: (x[col].iloc[-1] - x[col].iloc[0]) / max(1e-9, (x[time_col].iloc[-1] - x[time_col].iloc[0]))\n",
        "                if len(x) > 1 and (x[time_col].iloc[-1] - x[time_col].iloc[0]) > 0\n",
        "                else np.nan\n",
        "            ).rename(\"slope\")\n",
        "            feat = pd.concat([agg, std_, slope], axis=1)\n",
        "            feat.columns = [f\"{prefix}{col}_{cname}\" for cname in feat.columns]\n",
        "            out.append(feat)\n",
        "\n",
        "        return pd.concat(out, axis=1).reset_index()\n",
        "\n",
        "    # --------- Static table processing (no encoding here to avoid leakage) ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def process_static_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        CV-safe: DO NOT encode here. Just keep one row per subject.\n",
        "        (Do categorical encoding in your modeling pipeline.)\n",
        "        \"\"\"\n",
        "        if \"subject_id\" not in df.columns:\n",
        "            return pd.DataFrame()\n",
        "        # Keep first non-duplicated row per subject_id\n",
        "        return df.drop_duplicates(subset=[\"subject_id\"]).copy()\n",
        "\n",
        "    # --------- Merge features ---------\n",
        "\n",
        "    def merge_all_features(self, datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "        if \"PROACT_DEMOGRAPHICS.csv\" not in datasets:\n",
        "            raise ValueError(\"Demographics file is missing.\")\n",
        "\n",
        "        # Build ALSFRS anchor map\n",
        "        alsfrs = datasets[\"PROACT_ALSFRS.csv\"]\n",
        "        anchor_map = self._alsfrs_anchor_days(alsfrs)\n",
        "\n",
        "        # Start with demographics (static)\n",
        "        final_df = self.process_static_data(datasets[\"PROACT_DEMOGRAPHICS.csv\"])\n",
        "\n",
        "        # Add static-ish other tables (keep CV-safe; no encodings)\n",
        "        for file in [\"PROACT_RILUZOLE.csv\", \"PROACT_ALSHISTORY.csv\"]:\n",
        "            if file in datasets:\n",
        "                static_df = self.process_static_data(datasets[file])\n",
        "                final_df = pd.merge(final_df, static_df, on=\"subject_id\", how=\"left\")\n",
        "\n",
        "        # Longitudinal configs\n",
        "        longitudinal = {\n",
        "            \"PROACT_ALSFRS.csv\": \"alsfrs_\",\n",
        "            \"PROACT_FVC.csv\": \"fvc_\",\n",
        "            \"PROACT_VITALSIGNS.csv\": \"vitals_\",\n",
        "            \"PROACT_LABS.csv\": \"labs_\",\n",
        "            \"PROACT_HANDGRIPSTRENGTH.csv\": \"grip_\",\n",
        "            \"PROACT_MUSCLESTRENGTH.csv\": \"muscle_\",\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Generating Longitudinal Features (anchored to first ALSFRS; window = 0–90 days) ---\")\n",
        "        for file, prefix in longitudinal.items():\n",
        "            if file not in datasets:\n",
        "                continue\n",
        "\n",
        "            df = datasets[file].copy()\n",
        "            tcol = self._find_time_col(df)\n",
        "            if tcol is None:\n",
        "                print(f\"Warning: No time delta/days column in {file}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Anchor this table to ALSFRS first visit\n",
        "            df[\"anchor_days\"] = df[\"subject_id\"].map(anchor_map)\n",
        "            df = df[~df[\"anchor_days\"].isna()].copy()\n",
        "            df[\"days_from_alsfrs_anchor\"] = pd.to_numeric(df[tcol], errors=\"coerce\") - df[\"anchor_days\"]\n",
        "\n",
        "            # FVC special handling: collapse to max-of-trials BEFORE summarization\n",
        "            if file == \"PROACT_FVC.csv\":\n",
        "                df = self._fvc_collapse_trials(df, time_col=\"days_from_alsfrs_anchor\")\n",
        "\n",
        "            # Attempt to pivot long-form measurement tables (best effort)\n",
        "            if file in {\"PROACT_LABS.csv\", \"PROACT_MUSCLESTRENGTH.csv\", \"PROACT_HANDGRIPSTRENGTH.csv\"}:\n",
        "                try:\n",
        "                    test_cols = [\n",
        "                        c\n",
        "                        for c in df.columns\n",
        "                        if c not in {\"subject_id\", \"days_from_alsfrs_anchor\", \"anchor_days\"}\n",
        "                        and any(k in c.lower() for k in [\"test\", \"exam\", \"muscle\", \"site\", \"name\", \"strength_test\"])\n",
        "                    ]\n",
        "                    value_cols = [\n",
        "                        c\n",
        "                        for c in df.columns\n",
        "                        if c not in {\"subject_id\", \"days_from_alsfrs_anchor\", \"anchor_days\"}\n",
        "                        and any(k in c.lower() for k in [\"result\", \"value\", \"strength\", \"score\"])\n",
        "                    ]\n",
        "                    if test_cols and value_cols:\n",
        "                        tcol_name = test_cols[0]\n",
        "                        vcol_name = value_cols[0]\n",
        "                        df[vcol_name] = pd.to_numeric(df[vcol_name], errors=\"coerce\")\n",
        "                        df = (\n",
        "                            df.pivot_table(\n",
        "                                index=[\"subject_id\", \"days_from_alsfrs_anchor\"],\n",
        "                                columns=tcol_name,\n",
        "                                values=vcol_name,\n",
        "                                aggfunc=\"mean\",\n",
        "                            )\n",
        "                            .reset_index()\n",
        "                        )\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Pivoting failed for {file}: {e}\")\n",
        "\n",
        "            feats = self.create_longitudinal_features(df, \"days_from_alsfrs_anchor\", prefix)\n",
        "            if not feats.empty:\n",
        "                final_df = pd.merge(final_df, feats, on=\"subject_id\", how=\"left\")\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    # --------- Eligibility ---------\n",
        "\n",
        "    def filter_eligible_patients(self, feature_df: pd.DataFrame, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Keep subjects who have ANY ALSFRS >3 months AND >12 months AFTER the ALSFRS anchor.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        df[\"days_from_anchor\"] = df[\"alsfrs_delta\"] - df[\"subject_id\"].map(anchor_map)\n",
        "        df[\"months\"] = df[\"days_from_anchor\"] / 30.44\n",
        "\n",
        "        g = df.groupby(\"subject_id\")[\"months\"]\n",
        "        has_t1 = g.apply(lambda s: (s > 3.0).any())\n",
        "        has_t2 = g.apply(lambda s: (s > 12.0).any())\n",
        "        eligible_ids = has_t1[has_t1].index.intersection(has_t2[has_t2].index)\n",
        "\n",
        "        print(f\"\\nEligible patients: {len(eligible_ids)} / {df['subject_id'].nunique()}\")\n",
        "        return feature_df[feature_df[\"subject_id\"].isin(eligible_ids)].copy()\n",
        "\n",
        "    # --------- Orchestration ---------\n",
        "\n",
        "    def run_pipeline(self, file_path: str = \"\") -> Optional[Dict[str, pd.DataFrame]]:\n",
        "        \"\"\"\n",
        "        End-to-end EDA (CV-safe) that writes 'final_processed_als_data.csv'.\n",
        "        No imputation/scaling/feature selection here — do that inside your CV pipeline.\n",
        "        \"\"\"\n",
        "        print(\"====== Starting ALS Data Preprocessing Pipeline ======\")\n",
        "        datasets = self.load_and_inspect_data(file_path)\n",
        "        if \"PROACT_ALSFRS.csv\" not in datasets:\n",
        "            print(\"CRITICAL ERROR: PROACT_ALSFRS.csv not found. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        # ALSFRS prep + anchor\n",
        "        datasets[\"PROACT_ALSFRS.csv\"] = self._convert_alsfrs_r(datasets[\"PROACT_ALSFRS.csv\"])\n",
        "\n",
        "        # Outcome\n",
        "        target_df = self.calculate_alsfrs_slope(datasets[\"PROACT_ALSFRS.csv\"])\n",
        "        print(f\"\\nCalculated ALSFRS slope for {len(target_df)} patients.\")\n",
        "\n",
        "        # Features\n",
        "        full_features = self.merge_all_features(datasets)\n",
        "\n",
        "        # Eligibility\n",
        "        eligible_features = self.filter_eligible_patients(full_features, datasets[\"PROACT_ALSFRS.csv\"])\n",
        "\n",
        "        # Join features + target\n",
        "        final_df = pd.merge(eligible_features, target_df, on=\"subject_id\", how=\"inner\")\n",
        "\n",
        "        # Drop features with >30% missing\n",
        "        print(\"\\n--- Handling Missing Values (Dropping cols with >30% missing) ---\")\n",
        "        initial_cols = len(final_df.columns)\n",
        "        missing_thresh = 0.30\n",
        "        min_non_na = int(np.ceil(len(final_df) * (1 - missing_thresh)))\n",
        "        final_df = final_df.dropna(axis=1, thresh=min_non_na)\n",
        "        dropped = initial_cols - len(final_df.columns)\n",
        "        print(f\"Dropped {dropped} columns for >{int(missing_thresh*100)}% missingness.\")\n",
        "\n",
        "        # Separate X/y (no transforms here to avoid leakage)\n",
        "        if \"alsfrs_slope\" not in final_df.columns:\n",
        "            print(\"No target available after merges. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        y = final_df[\"alsfrs_slope\"]\n",
        "        valid = y.notna()\n",
        "        final_df = final_df.loc[valid].reset_index(drop=True)\n",
        "\n",
        "        subject_ids = final_df[\"subject_id\"]\n",
        "        y = final_df[\"alsfrs_slope\"]\n",
        "        X = final_df.drop(columns=[\"subject_id\", \"alsfrs_slope\"])\n",
        "\n",
        "        # Save CV-safe engineered dataset (raw features)\n",
        "        out = pd.concat([subject_ids, y, X], axis=1)\n",
        "        out.to_csv(\"final_processed_als_data.csv\", index=False)\n",
        "        print(\"\\n✅ Saved CV-safe engineered data to 'final_processed_als_data.csv'\")\n",
        "        print(f\"Feature matrix shape: {X.shape} | Target length: {len(y)}\")\n",
        "\n",
        "        return {\"X\": X, \"y\": y, \"subject_ids\": subject_ids, \"raw_frame\": out}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # If your CSVs live elsewhere, set file_path accordingly (e.g., \"C:/data/PROACT/\")\n",
        "    file_path = \"\"\n",
        "    processor = ALSDataProcessor()\n",
        "    processed = processor.run_pipeline(file_path=file_path)\n",
        "    if processed is not None:\n",
        "        print(\"\\nPreview of columns:\", list(processed[\"X\"].columns)[:10])\n",
        "        print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fbZFYZYn5f"
      },
      "source": [
        "## Cell 5 — FAST classical baselines (successive halving + pipeline caching)\n",
        "\n",
        "**Purpose:** Train strong-but-quick RF & SVR baselines using **successive halving** (prunes weak configs early) and **joblib cache** (reuses preprocessing across CV folds).\n",
        "\n",
        "### What this cell does\n",
        "- Loads `final_processed_als_data.csv`, quick **80/20** split, optional shuffle for fold homogeneity.\n",
        "- Auto-detects **numeric vs categorical** columns.\n",
        "- Builds **in-pipeline** preprocessing (impute; scaling only for SVR; OHE for categoricals).\n",
        "- Runs **HalvingGridSearchCV (cv=3)** for:\n",
        "  - **RandomForestRegressor** (no scaling needed).\n",
        "  - **SVR (RBF)** (with scaling).\n",
        "- Evaluates on the held-out test set and prints **RMSE, PCC** with **95% bootstrap CIs**.\n",
        "- Prints a quick **RF+SVR 50–50 ensemble** as a sanity check.\n",
        "\n",
        "### Why it’s fast\n",
        "- **Successive halving** cuts off underperformers early → far fewer total fits.\n",
        "- **joblib.Memory** caches transformers inside the pipeline → repeated CV splits don’t recompute imputation/encoding from scratch.\n",
        "\n",
        "### Outputs (printed)\n",
        "- Best params for RF & SVR (FAST grids).\n",
        "- **Test Set Performance (FAST mode)** table with RMSE & PCC + CIs.\n",
        "- Optional **RF+SVR Ensemble** metrics.\n",
        "\n",
        "> Tip: If you’re only benchmarking speed, comment out the SVR block; RF alone is often a strong baseline here.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EtbWc-T3HO",
        "outputId": "a191b2a0-59c2-4d02-c21a-e43673d0cf8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== FAST Classical Baselines (successive halving, cached) ======\n",
            "✓ Loaded engineered dataset: (1897, 348)\n",
            "Split: train=1517, test=380\n",
            "Detected numeric=343, categorical=3\n",
            "\n",
            "--- Fitting RandomForest (HalvingGridSearchCV, cv=3) ---\n",
            "RF best params: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__n_estimators': 250, 'select__k': 50}\n",
            "\n",
            "--- Fitting SVR (HalvingGridSearchCV, cv=3) ---\n",
            "SVR best params: {'model__C': 1.0, 'model__epsilon': 0.1, 'model__gamma': 'scale', 'select__k': 'all'}\n",
            "\n",
            "====== Test Set Performance (FAST mode) ======\n",
            "                 RMSE  RMSE 95% CI Low  RMSE 95% CI High     PCC  \\\n",
            "Model                                                              \n",
            "Random Forest  0.5905           0.5467            0.6386  0.1918   \n",
            "SVR (RBF)      0.5907           0.5405            0.6404  0.2131   \n",
            "\n",
            "               PCC 95% CI Low  PCC 95% CI High  \n",
            "Model                                           \n",
            "Random Forest          0.0909           0.2972  \n",
            "SVR (RBF)              0.1179           0.3153  \n",
            "\n",
            "--- Simple RF+SVR Avg Ensemble (FAST) ---\n",
            "                   RMSE  RMSE 95% CI Low  RMSE 95% CI High     PCC  \\\n",
            "RF+SVR Ensemble  0.5825           0.5355            0.6296  0.2248   \n",
            "\n",
            "                 PCC 95% CI Low  PCC 95% CI High  \n",
            "RF+SVR Ensemble          0.1234            0.322  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Halving search (successive halving)\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "\n",
        "# caching\n",
        "from joblib import Memory\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- Metrics ----------\n",
        "def rmse(y_true, y_pred) -> float:\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def safe_pcc(y_true, y_pred) -> float:\n",
        "    yt = np.asarray(y_true, dtype=float).ravel()\n",
        "    yp = np.asarray(y_pred, dtype=float).ravel()\n",
        "    if yt.std() < 1e-12 or yp.std() < 1e-12:\n",
        "        return 0.0\n",
        "    return float(np.corrcoef(yt, yp)[0, 1])\n",
        "\n",
        "def bootstrap_ci(y_true, y_pred, metric_fn, n_boot=800, alpha=0.95, seed=42) -> Tuple[float, float]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    y_true = np.asarray(y_true).ravel()\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    n = len(y_true)\n",
        "    stats = []\n",
        "    idx = np.arange(n)\n",
        "    for _ in range(n_boot):\n",
        "        b = rng.choice(idx, size=n, replace=True)\n",
        "        stats.append(metric_fn(y_true[b], y_pred[b]))\n",
        "    lo = float(np.percentile(stats, (1 - alpha) / 2 * 100))\n",
        "    hi = float(np.percentile(stats, (1 + alpha) / 2 * 100))\n",
        "    return lo, hi\n",
        "\n",
        "# ---------- Main ----------\n",
        "def run_classical_pipeline_fast() -> pd.DataFrame:\n",
        "    print(\"====== FAST Classical Baselines (successive halving, cached) ======\")\n",
        "\n",
        "    # 1) Load engineered data\n",
        "    df = pd.read_csv(\"final_processed_als_data.csv\")\n",
        "    print(f\"✓ Loaded engineered dataset: {df.shape}\")\n",
        "\n",
        "    X = df.drop(columns=[\"subject_id\", \"alsfrs_slope\"])\n",
        "    y = df[\"alsfrs_slope\"].astype(float)\n",
        "\n",
        "    # Optional quick shuffle for better fold homogeneity\n",
        "    X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "    # 80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42\n",
        "    )\n",
        "    print(f\"Split: train={X_train.shape[0]}, test={X_test.shape[0]}\")\n",
        "\n",
        "    # 2) Column typing\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    print(f\"Detected numeric={len(num_cols)}, categorical={len(cat_cols)}\")\n",
        "\n",
        "    # Pipeline cache\n",
        "    memory = Memory(location=\"sk_cache\", verbose=0)\n",
        "\n",
        "    # Preprocessors\n",
        "    # Numeric: impute → (optional scaler in SVR branch)\n",
        "    num_rf = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ], memory=memory)\n",
        "\n",
        "    num_svr = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ], memory=memory)\n",
        "\n",
        "    if len(cat_cols) > 0:\n",
        "        cat_common = Pipeline(steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ], memory=memory)\n",
        "        preproc_rf = ColumnTransformer(\n",
        "            transformers=[(\"num\", num_rf, num_cols), (\"cat\", cat_common, cat_cols)],\n",
        "            remainder=\"drop\"\n",
        "        )\n",
        "        preproc_svr = ColumnTransformer(\n",
        "            transformers=[(\"num\", num_svr, num_cols), (\"cat\", cat_common, cat_cols)],\n",
        "            remainder=\"drop\"\n",
        "        )\n",
        "    else:\n",
        "        # No categoricals → simpler (faster) preprocessors\n",
        "        preproc_rf = num_rf\n",
        "        preproc_svr = num_svr\n",
        "\n",
        "    # 3) Pipelines (with small but effective grids)\n",
        "    rf_pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preproc_rf),\n",
        "        (\"select\", SelectKBest(score_func=f_regression, k=\"all\")),\n",
        "        (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "    ], memory=memory)\n",
        "\n",
        "    rf_grid: Dict[str, list] = {\n",
        "        # keep imputer fixed (median) to avoid recomputing transforms\n",
        "        \"select__k\": [\"all\", 50],              # feature count toggle\n",
        "        \"model__n_estimators\": [250],          # enough trees, faster than 500\n",
        "        \"model__max_depth\": [None, 12],\n",
        "        \"model__min_samples_leaf\": [1, 2],\n",
        "        \"model__max_features\": [\"sqrt\"],       # stable setting\n",
        "    }\n",
        "\n",
        "    # Successive halving (aggressive elimination reduces fits)\n",
        "    rf_search = HalvingGridSearchCV(\n",
        "        rf_pipe,\n",
        "        rf_grid,\n",
        "        factor=3,\n",
        "        resource=\"n_samples\",\n",
        "        min_resources=\"exhaust\",\n",
        "        cv=3,\n",
        "        scoring=\"neg_root_mean_squared_error\",   # optimize RMSE directly\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        refit=True\n",
        "    )\n",
        "    print(\"\\n--- Fitting RandomForest (HalvingGridSearchCV, cv=3) ---\")\n",
        "    rf_search.fit(X_train, y_train)\n",
        "    print(f\"RF best params: {rf_search.best_params_}\")\n",
        "\n",
        "    # SVR (trimmed grid; if you need even faster, comment this whole block)\n",
        "    svr_pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preproc_svr),\n",
        "        (\"select\", SelectKBest(score_func=f_regression, k=\"all\")),\n",
        "        (\"model\", SVR(kernel=\"rbf\"))\n",
        "    ], memory=memory)\n",
        "\n",
        "    svr_grid: Dict[str, list] = {\n",
        "        \"select__k\": [\"all\", 50],\n",
        "        \"model__C\": [1.0, 3.0],\n",
        "        \"model__epsilon\": [0.1],\n",
        "        \"model__gamma\": [\"scale\"],\n",
        "    }\n",
        "\n",
        "    svr_search = HalvingGridSearchCV(\n",
        "        svr_pipe,\n",
        "        svr_grid,\n",
        "        factor=3,\n",
        "        resource=\"n_samples\",\n",
        "        min_resources=\"exhaust\",\n",
        "        cv=3,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        refit=True\n",
        "    )\n",
        "    print(\"\\n--- Fitting SVR (HalvingGridSearchCV, cv=3) ---\")\n",
        "    svr_search.fit(X_train, y_train)\n",
        "    print(f\"SVR best params: {svr_search.best_params_}\")\n",
        "\n",
        "    # 4) Test-set evaluation + (faster) bootstrap CIs\n",
        "    results = []\n",
        "\n",
        "    for name, est in [(\"Random Forest\", rf_search), (\"SVR (RBF)\", svr_search)]:\n",
        "        y_pred = est.best_estimator_.predict(X_test)\n",
        "        test_rmse = rmse(y_test, y_pred)\n",
        "        test_pcc  = safe_pcc(y_test.values, y_pred)\n",
        "\n",
        "        rmse_lo, rmse_hi = bootstrap_ci(y_test.values, y_pred, rmse, n_boot=800, alpha=0.95, seed=123)\n",
        "        pcc_lo,  pcc_hi  = bootstrap_ci(y_test.values, y_pred, safe_pcc, n_boot=800, alpha=0.95, seed=456)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": name,\n",
        "            \"RMSE\": test_rmse,\n",
        "            \"RMSE 95% CI Low\": rmse_lo,\n",
        "            \"RMSE 95% CI High\": rmse_hi,\n",
        "            \"PCC\": test_pcc,\n",
        "            \"PCC 95% CI Low\": pcc_lo,\n",
        "            \"PCC 95% CI High\": pcc_hi,\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results).set_index(\"Model\")\n",
        "    print(\"\\n====== Test Set Performance (FAST mode) ======\")\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    # Optional quick 50–50 blend (no extra CV)\n",
        "    rf_pred = rf_search.best_estimator_.predict(X_test)\n",
        "    svr_pred = svr_search.best_estimator_.predict(X_test)\n",
        "    ens_pred = 0.5 * (rf_pred + svr_pred)\n",
        "\n",
        "    ens_rmse = rmse(y_test, ens_pred)\n",
        "    ens_pcc  = safe_pcc(y_test.values, ens_pred)\n",
        "    ens_rmse_ci = bootstrap_ci(y_test.values, ens_pred, rmse, n_boot=800, alpha=0.95, seed=789)\n",
        "    ens_pcc_ci  = bootstrap_ci(y_test.values, ens_pred, safe_pcc, n_boot=800, alpha=0.95, seed=101112)\n",
        "\n",
        "    print(\"\\n--- Simple RF+SVR Avg Ensemble (FAST) ---\")\n",
        "    print(pd.DataFrame({\n",
        "        \"RMSE\": [ens_rmse],\n",
        "        \"RMSE 95% CI Low\": [ens_rmse_ci[0]],\n",
        "        \"RMSE 95% CI High\": [ens_rmse_ci[1]],\n",
        "        \"PCC\": [ens_pcc],\n",
        "        \"PCC 95% CI Low\": [ens_pcc_ci[0]],\n",
        "        \"PCC 95% CI High\": [ens_pcc_ci[1]],\n",
        "    }, index=[\"RF+SVR Ensemble\"]).round(4))\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_classical_pipeline_fast()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zRkw69vZeq4"
      },
      "source": [
        "## Cell 4 — Variational Quantum Regressor (VQC) trained with SPSA\n",
        "\n",
        "**Purpose:** End-to-end quantum regressor that predicts ALSFRS slope from engineered features.  \n",
        "It builds a circuit **ZZFeatureMap → EfficientSU2**, measures the **average Z** observable, and fits a tiny linear head `ŷ = α·⟨O⟩ + β`. Parameters **θ (circuit), α, β** are trained with **SPSA** on MSE using an Aer/Estimator backend.\n",
        "\n",
        "### Pipeline (quick map)\n",
        "1) **Load** `final_processed_als_data.csv` → keep **numeric** columns only.  \n",
        "2) **Train/test split** (80/20).  \n",
        "3) **Feature pick** inside train: **RF top-K** numeric features (default **K=16**).  \n",
        "4) **Train/val split** within training set for **early stopping**.  \n",
        "5) **Impute + Standardize** (train-only fit) → **PLSRegression** → reduce to `n_qubits`.  \n",
        "6) **Min-max to angles** `[0, π]` per component (angle encoding).  \n",
        "7) **Circuit**: `ZZFeatureMap(n_qubits, reps) ∘ EfficientSU2(n_qubits, reps, entanglement=\"linear\")`.  \n",
        "8) **Observable**: mean of Z on each qubit: \\( O = \\frac{1}{n}\\sum_i Z_i \\).  \n",
        "9) **Train with SPSA** on MSE, early stop on `val_rmse + (1 - val_pcc)`.  \n",
        "10) **Test**: RMSE, PCC + **95% bootstrap CIs**.\n",
        "\n",
        "### Key knobs (you can pass via `run_vqc`)\n",
        "- `n_qubits` (e.g., 4) & `pls_components` (**must match** `n_qubits`).\n",
        "- `topk` features before PLS (default 16).\n",
        "- Circuit depth: `fmap_reps`, `ansatz_reps`.\n",
        "- SPSA schedule: `a, c, A, alpha, gamma`; steps & `batch_size`.\n",
        "- Early stopping `patience`. Backend auto-selects **AerEstimator** if available.\n",
        "\n",
        "> **Tip:** For faster smoke tests, try `spsa_steps=150` and `batch_size=64`.\n",
        "\n",
        "<details>\n",
        "<summary>SPSA in 30 seconds</summary>\n",
        "At step *k*, build **one random ± perturbation** of the whole parameter vector, compute two losses `L+` and `L−`, estimate a stochastic gradient with a finite difference, and apply a scheduled step size. It needs **2 objective calls per step**, independent of parameter count.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Encoding & Observable</summary>\n",
        "- **Encoding:** PLS projects scaled features into `n_qubits` components, then min-max maps them into **[0, π]** angles to feed the `ZZFeatureMap`.  \n",
        "- **Observable:** ⟨O⟩ is between −1 and 1; the linear head `(α, β)` rescales it to the target range.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Gotchas</summary>\n",
        "- **`pls_components == n_qubits`** is asserted.  \n",
        "- If **qiskit-aer** isn’t present, it falls back to the built-in `Estimator` (slower).  \n",
        "- `topk` must be ≤ number of numeric features in the training split.  \n",
        "- Seeds are set for numpy and RF; quantum backends may still introduce sampling noise.\n",
        "</details>\n",
        "\n",
        "**Printed outputs:** split sizes, top-K list, live train/val metrics during SPSA, and final test **RMSE/PCC + 95% CIs**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8R8_BcrxWYf",
        "outputId": "59235d1f-a309-46b9-db43-3ce897960d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data: X_num=(1897, 343) (from 346 total), y=(1897,)\n",
            "Split: train=1517 test=380\n",
            "✓ Top-16 numeric features: ['alsfrs_ALSFRS_Total_slope', 'fvc_FVC_Liters_slope', 'fvc_FVC_Liters_std', 'alsfrs_ALSFRS_Total_std', 'vitals_Blood_Pressure_Systolic_slope', 'Age', 'vitals_Weight_slope', 'alsfrs_ALSFRS_Delta_std', 'labs_White Blood Cell (WBC)_first', 'labs_Phosphorus_median', 'vitals_Blood_Pressure_Diastolic_std', 'vitals_Respiratory_Rate_slope', 'vitals_Pulse_median', 'vitals_Blood_Pressure_Diastolic_slope', 'alsfrs_Q3_Swallowing_slope', 'alsfrs_ALSFRS_Total_median']\n",
            "\n",
            "--- Training VQC (seed=13) ---\n",
            "[   1] TR rmse=0.9877 pcc=0.1782 | VA rmse=1.0336 pcc=-0.0126\n",
            "[  10] TR rmse=0.9874 pcc=0.1792 | VA rmse=1.0331 pcc=-0.0109\n",
            "[  20] TR rmse=0.9873 pcc=0.1797 | VA rmse=1.0331 pcc=-0.0106\n",
            "[  30] TR rmse=0.9871 pcc=0.1805 | VA rmse=1.0329 pcc=-0.0091\n",
            "[  40] TR rmse=0.9868 pcc=0.1807 | VA rmse=1.0324 pcc=-0.0082\n",
            "[  50] TR rmse=0.9867 pcc=0.1805 | VA rmse=1.0321 pcc=-0.0085\n",
            "[  60] TR rmse=0.9864 pcc=0.1815 | VA rmse=1.0318 pcc=-0.0070\n",
            "[  70] TR rmse=0.9861 pcc=0.1831 | VA rmse=1.0319 pcc=-0.0071\n",
            "[  80] TR rmse=0.9856 pcc=0.1839 | VA rmse=1.0310 pcc=-0.0049\n",
            "[  90] TR rmse=0.9854 pcc=0.1852 | VA rmse=1.0309 pcc=-0.0041\n",
            "[ 100] TR rmse=0.9851 pcc=0.1859 | VA rmse=1.0308 pcc=-0.0042\n",
            "[ 110] TR rmse=0.9850 pcc=0.1868 | VA rmse=1.0309 pcc=-0.0041\n",
            "[ 120] TR rmse=0.9850 pcc=0.1866 | VA rmse=1.0306 pcc=-0.0037\n",
            "[ 130] TR rmse=0.9848 pcc=0.1867 | VA rmse=1.0307 pcc=-0.0048\n",
            "[ 140] TR rmse=0.9845 pcc=0.1880 | VA rmse=1.0303 pcc=-0.0028\n",
            "[ 150] TR rmse=0.9843 pcc=0.1884 | VA rmse=1.0300 pcc=-0.0019\n",
            "[ 160] TR rmse=0.9841 pcc=0.1894 | VA rmse=1.0300 pcc=-0.0017\n",
            "[ 170] TR rmse=0.9840 pcc=0.1900 | VA rmse=1.0299 pcc=-0.0012\n",
            "[ 180] TR rmse=0.9837 pcc=0.1904 | VA rmse=1.0295 pcc=-0.0014\n",
            "[ 190] TR rmse=0.9836 pcc=0.1907 | VA rmse=1.0292 pcc=-0.0009\n",
            "[ 200] TR rmse=0.9833 pcc=0.1913 | VA rmse=1.0288 pcc=0.0011\n"
          ]
        }
      ],
      "source": [
        "# vqc_multiobs_spsa_ckpt.py\n",
        "# End-to-end variational quantum regressor with multi-observable readout (no classical model on QRF).\n",
        "# Trains θ (circuit) + w (readout vector) + b with SPSA.\n",
        "# Includes checkpointing (best + latest) and resume support for Colab robustness.\n",
        "# Evaluates on 80/20 split with RMSE/PCC + 95% CIs (paper-style).\n",
        "\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# keep CPU libs tame & reproducible\n",
        "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
        "    os.environ.setdefault(k, \"1\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# stats\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# qiskit\n",
        "from qiskit.circuit.library import ZZFeatureMap, EfficientSU2\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from qiskit.primitives import Estimator\n",
        "try:\n",
        "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
        "    AER_OK = True\n",
        "except Exception:\n",
        "    AER_OK = False\n",
        "\n",
        "\n",
        "# -------------------- utilities --------------------\n",
        "def safe_pcc(a, b):\n",
        "    a, b = np.asarray(a).ravel(), np.asarray(b).ravel()\n",
        "    if a.std() == 0 or b.std() == 0:\n",
        "        return 0.0\n",
        "    v = pearsonr(a, b)[0]\n",
        "    return float(v) if np.isfinite(v) else 0.0\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def bootstrap_ci(y_true, y_pred, metric_fn, n_boot=5000, alpha=0.95, seed=42) -> Tuple[float, float]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true); idx = np.arange(n)\n",
        "    vals = []\n",
        "    for _ in range(n_boot):\n",
        "        b = rng.choice(idx, size=n, replace=True)\n",
        "        vals.append(metric_fn(y_true[b], y_pred[b]))\n",
        "    lo, hi = np.percentile(vals, [(1-alpha)/2*100, (1+alpha)/2*100])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def zscore_fit(y):\n",
        "    mu, sd = float(np.mean(y)), float(np.std(y) + 1e-12)\n",
        "    return mu, sd\n",
        "\n",
        "def zscore_apply(y, mu, sd):\n",
        "    return (y - mu) / sd\n",
        "\n",
        "def zscore_invert(yz, mu, sd):\n",
        "    return yz * sd + mu\n",
        "\n",
        "def load_numeric_xy(path=\"final_processed_als_data.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    X_all = df.drop(columns=[\"subject_id\",\"alsfrs_slope\"], errors=\"ignore\")\n",
        "    X = X_all.select_dtypes(include=[np.number]).copy()\n",
        "    y = df[\"alsfrs_slope\"].astype(float).values\n",
        "    m = ~np.isnan(y)\n",
        "    X, y = X.loc[m].reset_index(drop=True), y[m]\n",
        "    print(f\"✓ Data: X_num={X.shape} (from {X_all.shape[1]} total), y={y.shape}\")\n",
        "    return X, y\n",
        "\n",
        "def select_topk_by_rf(X_df, y, k=16):\n",
        "    imp = SimpleImputer(strategy=\"median\")\n",
        "    Xn = imp.fit_transform(X_df)\n",
        "    rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1).fit(Xn, y)\n",
        "    idx = np.argsort(rf.feature_importances_)[::-1][:k]\n",
        "    cols = [X_df.columns[i] for i in idx]\n",
        "    print(f\"✓ Top-{k} numeric features: {cols}\")\n",
        "    return idx, cols\n",
        "\n",
        "def _idx_from_param_name(name: str) -> int:\n",
        "    if '[' in name and ']' in name:\n",
        "        return int(name.split('[')[1].split(']')[0])\n",
        "    if '_' in name:\n",
        "        tail = name.split('_')[-1]\n",
        "        if tail.isdigit(): return int(tail)\n",
        "    digits = ''.join(ch for ch in name if ch.isdigit())\n",
        "    return int(digits) if digits else 0\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    if path and not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def ckpt_paths(ckpt_dir: str, seed: int) -> Dict[str, str]:\n",
        "    ensure_dir(ckpt_dir)\n",
        "    latest = os.path.join(ckpt_dir, f\"seed-{seed}-latest.npz\")\n",
        "    best   = os.path.join(ckpt_dir, f\"seed-{seed}-best.npz\")\n",
        "    return {\"latest\": latest, \"best\": best}\n",
        "\n",
        "def save_ckpt(path: str, step: int, theta: np.ndarray, w: np.ndarray, b: float,\n",
        "              best_metric: float, extras: Optional[dict] = None):\n",
        "    payload = {\n",
        "        \"step\": np.array(step, dtype=np.int64),\n",
        "        \"theta\": np.array(theta, dtype=np.float64),\n",
        "        \"w\": np.array(w, dtype=np.float64),\n",
        "        \"b\": np.array(b, dtype=np.float64),\n",
        "        \"best_metric\": np.array(best_metric, dtype=np.float64),\n",
        "    }\n",
        "    if extras:\n",
        "        for k, v in extras.items():\n",
        "            try:\n",
        "                payload[k] = np.array(v)\n",
        "            except Exception:\n",
        "                pass\n",
        "    np.savez(path, **payload)\n",
        "\n",
        "def load_ckpt(path: str):\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    step = int(data[\"step\"])\n",
        "    theta = np.array(data[\"theta\"], dtype=np.float64)\n",
        "    w = np.array(data[\"w\"], dtype=np.float64)\n",
        "    b = float(np.array(data[\"b\"], dtype=np.float64))\n",
        "    best_metric = float(np.array(data[\"best_metric\"], dtype=np.float64))\n",
        "    return step, theta, w, b, best_metric\n",
        "\n",
        "\n",
        "# -------------------- observables --------------------\n",
        "def make_observables(n_qubits=4, basis=\"Z\", use_pairs=True):\n",
        "    \"\"\"\n",
        "    basis:\n",
        "      'Z'   -> {Z_i} + (optional) {Z_i Z_j}\n",
        "      'ZX'  -> {Z_i, X_i} + (optional) {Z_i Z_j}\n",
        "      'ZXY' -> {Z_i, X_i, Y_i} + (optional) {Z_i Z_j}\n",
        "    Returns list[SparsePauliOp].\n",
        "    \"\"\"\n",
        "    obs: List[SparsePauliOp] = []\n",
        "    for i in range(n_qubits):\n",
        "        p = ['I']*n_qubits; p[i] = 'Z'\n",
        "        obs.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "        if basis in (\"ZX\",\"ZXY\"):\n",
        "            p = ['I']*n_qubits; p[i] = 'X'\n",
        "            obs.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "        if basis == \"ZXY\":\n",
        "            p = ['I']*n_qubits; p[i] = 'Y'\n",
        "            obs.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "    if use_pairs:\n",
        "        for i in range(n_qubits):\n",
        "            for j in range(i+1, n_qubits):\n",
        "                p = ['I']*n_qubits; p[i] = p[j] = 'Z'\n",
        "                obs.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "    return obs\n",
        "\n",
        "\n",
        "# -------------------- VQC with multi-observable readout + checkpointing --------------------\n",
        "class VQRegressorSPSA_MultiObs:\n",
        "    \"\"\"\n",
        "    y_hat = w^T E(x; θ) + b,   where E returns expectations of a set of observables.\n",
        "    Trains θ (variational circuit), w (readout weights), b (bias) jointly with SPSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_qubits=4, fmap_reps=2, ansatz_reps=2, basis=\"Z\", use_pairs=True,\n",
        "                 estimator=None, seed=13, l2=0.0):\n",
        "        self.nq = n_qubits\n",
        "        self.fmap = ZZFeatureMap(feature_dimension=n_qubits, reps=fmap_reps)\n",
        "        self.ans  = EfficientSU2(num_qubits=n_qubits, reps=ansatz_reps, entanglement=\"linear\")\n",
        "        self.circ = self.fmap.compose(self.ans)\n",
        "\n",
        "        # variational params & feature params\n",
        "        self.theta_params = sorted(list(self.ans.parameters), key=lambda p: _idx_from_param_name(p.name))\n",
        "        self.x_params = sorted([p for p in self.circ.parameters if p.name.startswith(\"x\")],\n",
        "                               key=lambda p: _idx_from_param_name(p.name))\n",
        "        assert len(self.x_params) == self.nq, \"Feature parameters mismatch\"\n",
        "        self.p = len(self.theta_params)\n",
        "\n",
        "        # observables for readout\n",
        "        self.obs_list: List[SparsePauliOp] = make_observables(n_qubits, basis=basis, use_pairs=use_pairs)\n",
        "        self.D = len(self.obs_list)\n",
        "\n",
        "        # estimator\n",
        "        self.est = estimator if estimator is not None else (AerEstimator() if AER_OK else Estimator())\n",
        "\n",
        "        # parameters (set on fit)\n",
        "        self.theta = None                 # shape [p]\n",
        "        self.w = None                     # shape [D]\n",
        "        self.b = None                     # scalar\n",
        "        self.l2 = float(l2)               # L2 penalty on w\n",
        "\n",
        "    def _bind_circuits_batch(self, X_theta: np.ndarray, theta_vec: np.ndarray):\n",
        "        circuits = []\n",
        "        bind_theta = {self.theta_params[i]: float(theta_vec[i]) for i in range(self.p)}\n",
        "        for i in range(X_theta.shape[0]):\n",
        "            bind_feats = {self.x_params[k]: float(X_theta[i, k]) for k in range(self.nq)}\n",
        "            cb = self.circ.assign_parameters({**bind_theta, **bind_feats}, inplace=False)\n",
        "            circuits.append(cb)\n",
        "        return circuits\n",
        "\n",
        "    def _expectations_multi(self, X_theta: np.ndarray, theta_vec: np.ndarray, batch=128) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Return matrix E in R^{N x D} of expectation values per observable.\n",
        "        \"\"\"\n",
        "        N = X_theta.shape[0]; D = self.D\n",
        "        out = np.empty((N, D), dtype=float)\n",
        "        for s in range(0, N, batch):\n",
        "            e = min(N, s+batch)\n",
        "            circs = self._bind_circuits_batch(X_theta[s:e], theta_vec)\n",
        "            # evaluate each sample against all observables\n",
        "            circuits = []; obs_all = []\n",
        "            for cb in circs:\n",
        "                circuits.extend([cb]*D)\n",
        "                obs_all.extend(self.obs_list)\n",
        "            vals = self.est.run(circuits, obs_all).result().values\n",
        "            out[s:e] = np.asarray(vals, dtype=float).reshape(e-s, D)\n",
        "        return out\n",
        "\n",
        "    def predict_z(self, X_theta: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict in z-scored target space (since training minimizes MSE there).\n",
        "        \"\"\"\n",
        "        E = self._expectations_multi(X_theta, self.theta)\n",
        "        return E @ self.w + self.b\n",
        "\n",
        "    def fit(self, X_tr_th: np.ndarray, yz_tr: np.ndarray,\n",
        "            X_val_th: np.ndarray=None, yz_val: np.ndarray=None,\n",
        "            steps=800, batch_size=160,\n",
        "            a=0.12, c=0.15, A=50, alpha=0.602, gamma=0.101,\n",
        "            theta_init=None, w_init=None, b_init=None,\n",
        "            seed=13, verbose=True, patience=100,\n",
        "            # checkpointing\n",
        "            checkpoint_dir: str = \"checkpoints_vqc\",\n",
        "            checkpoint_every: int = 50,\n",
        "            resume: bool = True):\n",
        "        \"\"\"\n",
        "        SPSA on params P = [theta (p), w (D), b (1)].\n",
        "        Loss = MSE(ŷ, y) + l2 * ||w||^2 / D.\n",
        "        Early stopping on val objective: RMSE + (1-PCC).\n",
        "        Saves 'latest' every `checkpoint_every` steps and 'best' on improvement.\n",
        "        If resume and latest checkpoint exists, loads and continues from next step.\n",
        "        \"\"\"\n",
        "        rng = np.random.default_rng(seed)\n",
        "        N = len(yz_tr)\n",
        "        cp = ckpt_paths(checkpoint_dir, seed)\n",
        "\n",
        "        start_step = 1\n",
        "        best_val = np.inf\n",
        "        # ----- resume if allowed -----\n",
        "        if resume and os.path.exists(cp[\"latest\"]):\n",
        "            try:\n",
        "                s, th, wv, bv, best_metric = load_ckpt(cp[\"latest\"])\n",
        "                self.theta, self.w, self.b = th, wv, bv\n",
        "                best_val = best_metric\n",
        "                start_step = int(s) + 1\n",
        "                if verbose:\n",
        "                    tqdm.write(f\"[Resume] Loaded latest checkpoint at step {s} (best_metric={best_metric:.6f}).\")\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    tqdm.write(f\"[Resume] Failed to load latest checkpoint: {e}. Starting fresh.\")\n",
        "\n",
        "        # ----- fresh init if needed -----\n",
        "        if self.theta is None:\n",
        "            if theta_init is None:\n",
        "                theta_init = rng.normal(0, 0.25, size=self.p)\n",
        "            self.theta = theta_init.astype(float)\n",
        "\n",
        "        if self.w is None or self.b is None:\n",
        "            if w_init is None:\n",
        "                # quick LS init for (w,b) on a small subset with current theta\n",
        "                idx0 = np.arange(min(256, N))\n",
        "                E0 = self._expectations_multi(X_tr_th[idx0], self.theta, batch=min(batch_size, 128))\n",
        "                y0 = yz_tr[idx0]\n",
        "                A_ = np.c_[E0, np.ones(len(idx0))]\n",
        "                wls, *_ = np.linalg.lstsq(A_, y0, rcond=None)\n",
        "                self.w = wls[:-1]\n",
        "                self.b = float(wls[-1])\n",
        "            else:\n",
        "                self.w = w_init.astype(float)\n",
        "                self.b = float(b_init if b_init is not None else 0.0)\n",
        "\n",
        "        if best_val is np.inf:\n",
        "            # initialize best_val from current validation if available\n",
        "            if (X_val_th is not None) and (yz_val is not None):\n",
        "                yv_hat = self.predict_z(X_val_th)\n",
        "                va_rm, va_pc = rmse(yz_val, yv_hat), safe_pcc(yz_val, yv_hat)\n",
        "                best_val = va_rm + (1.0 - va_pc)\n",
        "\n",
        "        hist = []\n",
        "        dim = self.p + self.D + 1\n",
        "\n",
        "        try:\n",
        "            for k in range(start_step, steps+1):\n",
        "                # SPSA gains\n",
        "                ak = a / ((k + A)**alpha)\n",
        "                ck = c / (k**gamma)\n",
        "\n",
        "                # minibatch\n",
        "                mb = min(batch_size, N)\n",
        "                mb_idx = rng.choice(np.arange(N), size=mb, replace=False)\n",
        "                Xb, yb = X_tr_th[mb_idx], yz_tr[mb_idx]\n",
        "\n",
        "                delta = rng.choice([-1.0, 1.0], size=dim)\n",
        "\n",
        "                # unpack deltas\n",
        "                d_theta = delta[:self.p]\n",
        "                d_w     = delta[self.p:self.p+self.D]\n",
        "                d_b     = delta[-1]\n",
        "\n",
        "                # plus/minus params\n",
        "                th_plus,  th_minus  = self.theta + ck*d_theta, self.theta - ck*d_theta\n",
        "                w_plus,   w_minus   = self.w     + ck*d_w,     self.w     - ck*d_w\n",
        "                b_plus,   b_minus   = self.b     + ck*d_b,     self.b     - ck*d_b\n",
        "\n",
        "                # expectations\n",
        "                Eb = self._expectations_multi(Xb, th_plus,  batch=mb)\n",
        "                Fb = self._expectations_multi(Xb, th_minus, batch=mb)\n",
        "\n",
        "                # predictions in z-space\n",
        "                y_plus  = Eb @ w_plus + b_plus\n",
        "                y_minus = Fb @ w_minus + b_minus\n",
        "\n",
        "                # MSE + L2(w)\n",
        "                L_plus  = np.mean((y_plus  - yb)**2) + self.l2 * (np.sum(w_plus**2) / self.D)\n",
        "                L_minus = np.mean((y_minus - yb)**2) + self.l2 * (np.sum(w_minus**2) / self.D)\n",
        "\n",
        "                # gradient estimate\n",
        "                ghat = (L_plus - L_minus) / (2.0 * ck) * (1.0 / delta)\n",
        "\n",
        "                # slice gradients\n",
        "                g_theta = ghat[:self.p]\n",
        "                g_w     = ghat[self.p:self.p+self.D]\n",
        "                g_b     = ghat[-1]\n",
        "\n",
        "                # update\n",
        "                self.theta = self.theta - ak * g_theta\n",
        "                self.w     = self.w     - ak * g_w\n",
        "                self.b     = self.b     - ak * g_b\n",
        "\n",
        "                # monitor + early stopping + checkpointing\n",
        "                do_print = (k % 10 == 0) or (k == 1)\n",
        "                if do_print or (k % checkpoint_every == 0):\n",
        "                    with np.errstate(all=\"ignore\"):\n",
        "                        ytr_hat = self.predict_z(X_tr_th)\n",
        "                        tr_rm, tr_pc = rmse(yz_tr, ytr_hat), safe_pcc(yz_tr, ytr_hat)\n",
        "\n",
        "                        if X_val_th is not None:\n",
        "                            yv_hat = self.predict_z(X_val_th)\n",
        "                            va_rm, va_pc = rmse(yz_val, yv_hat), safe_pcc(yz_val, yv_hat)\n",
        "                            obj = va_rm + (1.0 - va_pc)\n",
        "                            hist.append((k, tr_rm, tr_pc, va_rm, va_pc))\n",
        "\n",
        "                            if do_print:\n",
        "                                tqdm.write(f\"[{k:4d}] TR rmse={tr_rm:.4f} pcc={tr_pc:.4f} | VA rmse={va_rm:.4f} pcc={va_pc:.4f}\")\n",
        "\n",
        "                            # checkpoint: latest (rolling)\n",
        "                            save_ckpt(cp[\"latest\"], k, self.theta, self.w, self.b, best_val)\n",
        "\n",
        "                            # checkpoint: best\n",
        "                            if obj < best_val:\n",
        "                                best_val = obj\n",
        "                                save_ckpt(cp[\"best\"], k, self.theta, self.w, self.b, best_val)\n",
        "                                # reset patience\n",
        "                                patience = max(10, patience)  # ensure positive\n",
        "                                patience_left = patience\n",
        "                            else:\n",
        "                                # patience accounting\n",
        "                                if 'patience_left' not in locals():\n",
        "                                    patience_left = patience\n",
        "                                patience_left -= 1\n",
        "                                if patience_left <= 0:\n",
        "                                    tqdm.write(\"Early stopping (patience).\")\n",
        "                                    break\n",
        "                        else:\n",
        "                            if do_print:\n",
        "                                tqdm.write(f\"[{k:4d}] TR rmse={tr_rm:.4f} pcc={tr_pc:.4f}\")\n",
        "\n",
        "                # periodic checkpoint even without val (safety)\n",
        "                if (k % checkpoint_every == 0) and (X_val_th is None):\n",
        "                    save_ckpt(cp[\"latest\"], k, self.theta, self.w, self.b, best_val)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            tqdm.write(\"Interrupted — saving latest checkpoint before exit.\")\n",
        "            save_ckpt(cp[\"latest\"], k, self.theta, self.w, self.b, best_val)\n",
        "\n",
        "        # load best if exists\n",
        "        if os.path.exists(cp[\"best\"]):\n",
        "            _, thb, wb, bb, _ = load_ckpt(cp[\"best\"])\n",
        "            self.theta, self.w, self.b = thb, wb, bb\n",
        "\n",
        "        return hist\n",
        "\n",
        "\n",
        "# -------------------- pipeline & run --------------------\n",
        "def run_vqc_multiobs(\n",
        "    data_path=\"final_processed_als_data.csv\",\n",
        "    n_qubits=4, pls_components=4, topk=16,\n",
        "    fmap_reps=2, ansatz_reps=2, basis=\"Z\", use_pairs=True,\n",
        "    spsa_steps=900, batch_size=160, patience=120,\n",
        "    spsa_a=0.10, spsa_c=0.12, spsa_A=80, spsa_alpha=0.602, spsa_gamma=0.101,\n",
        "    l2=0.001, seeds=(13, 37, 91), n_boot=5000,\n",
        "    # checkpointing\n",
        "    checkpoint_dir: str = \"checkpoints_vqc\",\n",
        "    checkpoint_every: int = 50,\n",
        "    resume: bool = True\n",
        "):\n",
        "    t0 = time.time()\n",
        "    X, y = load_numeric_xy(data_path)\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "    print(f\"Split: train={X_tr.shape[0]} test={X_te.shape[0]}\")\n",
        "\n",
        "    # z-score target on TRAIN ONLY (and apply to val/test)\n",
        "    y_mu, y_sd = zscore_fit(y_tr)\n",
        "    yz_tr = zscore_apply(y_tr, y_mu, y_sd)\n",
        "    yz_te = zscore_apply(y_te, y_mu, y_sd)\n",
        "\n",
        "    # feature selection (train-only)\n",
        "    idxK, _ = select_topk_by_rf(X_tr, y_tr, k=topk)\n",
        "\n",
        "    # train/val split inside train for early stopping\n",
        "    X_trA, X_val, y_trA, y_val = train_test_split(X_tr, y_tr, test_size=0.20, random_state=123)\n",
        "    yz_trA = zscore_apply(y_trA, y_mu, y_sd)\n",
        "    yz_val = zscore_apply(y_val, y_mu, y_sd)\n",
        "\n",
        "    # PLS -> angles [0, π] (fit on TR only)\n",
        "    imp = SimpleImputer(strategy=\"median\").fit(X_trA.iloc[:, idxK])\n",
        "    std = StandardScaler().fit(imp.transform(X_trA.iloc[:, idxK]))\n",
        "    XtrK = std.transform(imp.transform(X_trA.iloc[:, idxK]))\n",
        "    XvaK = std.transform(imp.transform(X_val.iloc[:, idxK]))\n",
        "    XteK = std.transform(imp.transform(X_te.iloc[:, idxK]))\n",
        "\n",
        "    assert pls_components == n_qubits, \"pls_components must equal n_qubits\"\n",
        "    pls = PLSRegression(n_components=pls_components, scale=False).fit(XtrK, y_trA)\n",
        "    TrP = pls.transform(XtrK); VaP = pls.transform(XvaK); TeP = pls.transform(XteK)\n",
        "\n",
        "    ang = MinMaxScaler(feature_range=(0.0, np.pi)).fit(TrP)\n",
        "    TrTH = ang.transform(TrP); VaTH = ang.transform(VaP); TeTH = ang.transform(TeP)\n",
        "\n",
        "    # try multiple random starts; keep best val\n",
        "    best = None\n",
        "    est = AerEstimator() if AER_OK else Estimator()\n",
        "\n",
        "    for sd in seeds:\n",
        "        print(f\"\\n--- Training VQC (seed={sd}) ---\")\n",
        "        vqc = VQRegressorSPSA_MultiObs(\n",
        "            n_qubits=n_qubits, fmap_reps=fmap_reps, ansatz_reps=ansatz_reps,\n",
        "            basis=basis, use_pairs=use_pairs, estimator=est, seed=sd, l2=l2\n",
        "        )\n",
        "        vqc.fit(\n",
        "            TrTH, yz_trA, X_val_th=VaTH, yz_val=yz_val,\n",
        "            steps=spsa_steps, batch_size=batch_size,\n",
        "            a=spsa_a, c=spsa_c, A=spsa_A, alpha=spsa_alpha, gamma=spsa_gamma,\n",
        "            theta_init=None, seed=sd, verbose=True, patience=patience,\n",
        "            checkpoint_dir=checkpoint_dir, checkpoint_every=checkpoint_every, resume=resume\n",
        "        )\n",
        "\n",
        "        # val performance (original units)\n",
        "        yv_hat_z = vqc.predict_z(VaTH)\n",
        "        yv_hat   = zscore_invert(yv_hat_z, y_mu, y_sd)\n",
        "        va_rm, va_pc = rmse(y_val, yv_hat), safe_pcc(y_val, yv_hat)\n",
        "        print(f\"(seed={sd}) VAL  RMSE={va_rm:.4f}  PCC={va_pc:.4f}\")\n",
        "\n",
        "        item = (va_rm, -(va_pc), vqc, sd)   # sort by rmse asc, pcc desc\n",
        "        if (best is None) or (item < best):\n",
        "            best = item\n",
        "\n",
        "    _, _, vqc_best, best_seed = best\n",
        "    print(f\"\\n>>> Best seed: {best_seed}\")\n",
        "\n",
        "    # test once with best model (paper-style)\n",
        "    yte_hat_z = vqc_best.predict_z(TeTH)\n",
        "    yte_hat   = zscore_invert(yte_hat_z, y_mu, y_sd)\n",
        "    test_rmse = rmse(y_te, yte_hat)\n",
        "    test_pcc  = safe_pcc(y_te, yte_hat)\n",
        "    ci_rm = bootstrap_ci(y_te, yte_hat, rmse,     n_boot=n_boot, seed=202)\n",
        "    ci_pc = bootstrap_ci(y_te, yte_hat, safe_pcc, n_boot=n_boot, seed=303)\n",
        "\n",
        "    print(\"\\n===== VQC (Multi-Obs) TEST PERFORMANCE =====\")\n",
        "    print(f\"RMSE={test_rmse:.4f}  PCC={test_pcc:.4f}\")\n",
        "    print(f\"RMSE 95% CI [{ci_rm[0]:.4f}, {ci_rm[1]:.4f}]\")\n",
        "    print(f\"PCC  95% CI [{ci_pc[0]:.4f}, {ci_pc[1]:.4f}]\")\n",
        "\n",
        "    D = vqc_best.D\n",
        "    print(\"\\nModel: n_qubits=%d  fmap_reps=%d  ansatz_reps=%d  basis=%s  pairs=%s  observables=%d\"\n",
        "          % (n_qubits, fmap_reps, ansatz_reps, basis, str(use_pairs), D))\n",
        "    print(\"Time: %.1fs   AER=%s\" % (time.time()-t0, str(AER_OK)))\n",
        "\n",
        "    return dict(rmse=test_rmse, pcc=test_pcc, rmse_ci=ci_rm, pcc_ci=ci_pc, seed=best_seed)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ==== STRONGER, paper-chasing run (slower) ====\n",
        "    # _ = run_vqc_multiobs(\n",
        "    #     data_path=\"final_processed_als_data.csv\",\n",
        "    #     n_qubits=6, pls_components=6, topk=24,\n",
        "    #     fmap_reps=3, ansatz_reps=3, basis=\"ZX\", use_pairs=True,\n",
        "    #     spsa_steps=1200, batch_size=192, patience=180,\n",
        "    #     spsa_a=0.06, spsa_c=0.12, spsa_A=150, spsa_alpha=0.602, spsa_gamma=0.101,\n",
        "    #     l2=0.003, seeds=(13, 37, 91), n_boot=5000,\n",
        "    #     checkpoint_dir=\"checkpoints_vqc\", checkpoint_every=50, resume=True\n",
        "    # )\n",
        "\n",
        "    # ==== Faster sanity run (fits in typical Colab session) ====\n",
        "    _ = run_vqc_multiobs(\n",
        "        data_path=\"final_processed_als_data.csv\",\n",
        "        n_qubits=4, pls_components=4, topk=16,\n",
        "        fmap_reps=2, ansatz_reps=3, basis=\"ZX\", use_pairs=True,\n",
        "        spsa_steps=500, batch_size=192, patience=140,\n",
        "        spsa_a=0.08, spsa_c=0.12, spsa_A=120, spsa_alpha=0.602, spsa_gamma=0.101,\n",
        "        l2=0.002, seeds=(13, 37), n_boot=2000,\n",
        "        checkpoint_dir=\"checkpoints_vqc\", checkpoint_every=50, resume=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOMfLHauvaVvEAfRJ5vPfsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}