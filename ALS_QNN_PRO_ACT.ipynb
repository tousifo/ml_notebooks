{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON98qBqb4TzQBfn7ylrCD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/ALS_QNN_PRO_ACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3KCwYyArHq",
        "outputId": "055a96b2-2f69-4e9b-cb30-3eccf716e53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Starting ALS Data Preprocessing Pipeline ======\n",
            "--- Loading and Inspecting Data ---\n",
            "✓ PROACT_ALSFRS.csv: Loaded successfully with shape (73845, 20)\n",
            "✓ PROACT_FVC.csv: Loaded successfully with shape (49110, 10)\n",
            "✓ PROACT_VITALSIGNS.csv: Loaded successfully with shape (84721, 36)\n",
            "✓ PROACT_RILUZOLE.csv: Loaded successfully with shape (10363, 3)\n",
            "✓ PROACT_DEMOGRAPHICS.csv: Loaded successfully with shape (12504, 14)\n",
            "✓ PROACT_LABS.csv: Loaded successfully with shape (2937162, 5)\n",
            "✓ PROACT_DEATHDATA.csv: Loaded successfully with shape (5043, 3)\n",
            "✓ PROACT_HANDGRIPSTRENGTH.csv: Loaded successfully with shape (19032, 11)\n",
            "✓ PROACT_MUSCLESTRENGTH.csv: Loaded successfully with shape (204875, 10)\n",
            "\n",
            "Calculated ALSFRS slope for 2023 patients.\n",
            "\n",
            "--- Generating Longitudinal Features (from first 3 months) ---\n",
            "Processing PROACT_ALSFRS.csv...\n",
            "Processing PROACT_FVC.csv...\n",
            "Processing PROACT_VITALSIGNS.csv...\n",
            "Processing PROACT_LABS.csv...\n",
            "Processing PROACT_HANDGRIPSTRENGTH.csv...\n",
            "Processing PROACT_MUSCLESTRENGTH.csv...\n",
            "\n",
            "Found 3475 eligible patients out of 8538.\n",
            "\n",
            "--- Handling Missing Values ---\n",
            "Dropped 56 features with >30.0% missing values.\n",
            "\n",
            "--- Performing Feature Selection (Top 30 via Random Forest) ---\n",
            "\n",
            "====== Pipeline Complete ======\n",
            "Final feature matrix shape: (2022, 30)\n",
            "Final target vector shape: (2022,)\n",
            "\n",
            "✅ Successfully saved processed data to 'final_processed_als_data.csv'\n",
            "\n",
            "--- Top 15 Most Important Features ---\n",
            "                                  feature  importance\n",
            "91              alsfrs_ALSFRS_Total_slope    0.042661\n",
            "126      fvc_Subject_Liters_Trial_1_slope    0.035232\n",
            "125        fvc_Subject_Liters_Trial_1_std    0.020531\n",
            "224                   vitals_Weight_slope    0.018335\n",
            "181   vitals_Blood_Pressure_Diastolic_std    0.018144\n",
            "90                alsfrs_ALSFRS_Total_std    0.016505\n",
            "189  vitals_Blood_Pressure_Systolic_slope    0.015675\n",
            "188    vitals_Blood_Pressure_Systolic_std    0.015489\n",
            "199                   vitals_Pulse_median    0.015250\n",
            "202                      vitals_Pulse_std    0.015042\n",
            "227       vitals_Vital_Signs_Delta_median    0.014051\n",
            "223                     vitals_Weight_std    0.014038\n",
            "1                                     Age    0.013940\n",
            "203                    vitals_Pulse_slope    0.012908\n",
            "230          vitals_Vital_Signs_Delta_std    0.012469\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ALSDataProcessor:\n",
        "    \"\"\"\n",
        "    A robust class to load, clean, and process PRO-ACT data for predicting ALSFRS slope,\n",
        "    replicating the methodology from the \"Deep learning methods to predict amyotrophic\n",
        "    lateral sclerosis disease progression\" paper.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        # A list of columns to exclude from feature engineering\n",
        "        self.id_and_delta_cols = [\n",
        "            'subject_id', 'alsfrs_delta', 'fvc_delta', 'vitals_delta',\n",
        "            'labs_delta', 'grip_delta', 'muscle_delta', 'onset_delta',\n",
        "            'death_delta', 'history_delta'\n",
        "        ]\n",
        "\n",
        "    def _convert_alsfrs_r(self, alsfrs_df):\n",
        "        \"\"\"Convert ALSFRS-R questions to the original ALSFRS format.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        # Ensure ALSFRS_Total is numeric, coercing errors\n",
        "        df['ALSFRS_Total'] = pd.to_numeric(df['ALSFRS_Total'], errors='coerce')\n",
        "        return df\n",
        "\n",
        "    def load_and_inspect_data(self, file_path=''):\n",
        "        \"\"\"Load all datasets and inspect their structure.\"\"\"\n",
        "        datasets = {}\n",
        "        file_list = [\n",
        "            'PROACT_ALSFRS.csv', 'PROACT_FVC.csv', 'PROACT_VITALSIGNS.csv',\n",
        "            'PROACT_RILUZOLE.csv', 'PROACT_DEMOGRAPHICS.csv', 'PROACT_LABS.csv',\n",
        "            'PROACT_DEATHDATA.csv', 'PROACT_HANDGRIPSTRENGTH.csv',\n",
        "            'PROACT_MUSCLESTRENGTH.csv'\n",
        "        ]\n",
        "        print(\"--- Loading and Inspecting Data ---\")\n",
        "        for file_name in file_list:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path + file_name)\n",
        "\n",
        "                # --- CORRECTED RENAMING LOGIC ---\n",
        "                # Check if 'subject_id' already exists. If not, find a candidate and rename only the first one found.\n",
        "                if 'subject_id' not in df.columns:\n",
        "                    potential_id_cols = [col for col in df.columns if 'subject' in col.lower()]\n",
        "                    if potential_id_cols:\n",
        "                        df.rename(columns={potential_id_cols[0]: 'subject_id'}, inplace=True)\n",
        "                # --- END CORRECTION ---\n",
        "\n",
        "                datasets[file_name] = df\n",
        "                print(f\"✓ {file_name}: Loaded successfully with shape {df.shape}\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"✗ {file_name}: File not found. Will be skipped.\")\n",
        "        return datasets\n",
        "\n",
        "    def calculate_alsfrs_slope(self, alsfrs_df):\n",
        "        \"\"\"Calculate the primary target variable: ALSFRS slope between months 3-12.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        df.rename(columns={c:'alsfrs_delta' for c in df.columns if 'delta' in c.lower()}, inplace=True)\n",
        "        df['months'] = df['alsfrs_delta'] / 30.44\n",
        "        df.sort_values(['subject_id', 'months'], inplace=True)\n",
        "\n",
        "        slopes = {}\n",
        "        for subject_id, subject_data in df.groupby('subject_id'):\n",
        "            t1_candidates = subject_data[(subject_data['months'] > 3) & (subject_data['months'] <= 12)]\n",
        "            t2_candidates = subject_data[subject_data['months'] >= 12]\n",
        "\n",
        "            if not t1_candidates.empty and not t2_candidates.empty:\n",
        "                t1_row = t1_candidates.iloc[0]\n",
        "                t2_row = t2_candidates.iloc[0]\n",
        "\n",
        "                t1, alsfrs_t1 = t1_row['months'], t1_row['ALSFRS_Total']\n",
        "                t2, alsfrs_t2 = t2_row['months'], t2_row['ALSFRS_Total']\n",
        "\n",
        "                if t2 > t1 and pd.notna(alsfrs_t1) and pd.notna(alsfrs_t2):\n",
        "                    slope = (alsfrs_t2 - alsfrs_t1) / (t2 - t1)\n",
        "                    slopes[subject_id] = slope\n",
        "\n",
        "        return pd.DataFrame(list(slopes.items()), columns=['subject_id', 'alsfrs_slope'])\n",
        "\n",
        "    def create_longitudinal_features(self, df, time_col, prefix):\n",
        "        \"\"\"Create the seven summary statistics from longitudinal data (first 3 months).\"\"\"\n",
        "        df_sorted = df.sort_values(['subject_id', time_col])\n",
        "        df_filtered = df_sorted[df_sorted[time_col] <= 90].copy()\n",
        "\n",
        "        value_cols = [col for col in df_filtered.select_dtypes(include=np.number).columns\n",
        "                      if col.lower() not in self.id_and_delta_cols]\n",
        "\n",
        "        if not value_cols:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        summary_dfs = []\n",
        "        for value_col in value_cols:\n",
        "            grouped = df_filtered.groupby('subject_id')\n",
        "            summary = grouped[value_col].agg(['min', 'max', 'median', 'first', 'last']).join(\n",
        "                grouped[value_col].std(ddof=0).rename('std')\n",
        "            )\n",
        "\n",
        "            slope_df = grouped.apply(\n",
        "                lambda g: (g[value_col].iloc[-1] - g[value_col].iloc[0]) / (g[time_col].iloc[-1] - g[time_col].iloc[0])\n",
        "                if len(g) > 1 and (g[time_col].iloc[-1] - g[time_col].iloc[0]) > 0 else np.nan\n",
        "            ).rename('slope')\n",
        "\n",
        "            summary = summary.join(slope_df).fillna(0)\n",
        "            summary.columns = [f\"{prefix}{value_col}_{stat}\" for stat in summary.columns]\n",
        "            summary_dfs.append(summary)\n",
        "\n",
        "        return pd.concat(summary_dfs, axis=1).reset_index()\n",
        "\n",
        "    def process_static_data(self, df):\n",
        "        \"\"\"Process static data files (like demographics, riluzole).\"\"\"\n",
        "        processed = df.copy()\n",
        "        for col in processed.select_dtypes(include=['object', 'category']).columns:\n",
        "            if col != 'subject_id':\n",
        "                le = self.label_encoders.setdefault(col, LabelEncoder())\n",
        "                processed[col] = le.fit_transform(processed[col].astype(str))\n",
        "        return processed.drop_duplicates(subset=['subject_id'])\n",
        "\n",
        "    def merge_all_features(self, datasets):\n",
        "        \"\"\"Merge all static and longitudinal features into a single dataframe.\"\"\"\n",
        "        if 'PROACT_DEMOGRAPHICS.csv' not in datasets:\n",
        "            raise ValueError(\"Demographics file is missing.\")\n",
        "\n",
        "        final_df = self.process_static_data(datasets['PROACT_DEMOGRAPHICS.csv'])\n",
        "\n",
        "        static_files = ['PROACT_RILUZOLE.csv']\n",
        "        for file in static_files:\n",
        "            if file in datasets:\n",
        "                static_df = self.process_static_data(datasets[file])\n",
        "                final_df = pd.merge(final_df, static_df, on='subject_id', how='left')\n",
        "\n",
        "        longitudinal_configs = {\n",
        "            'PROACT_ALSFRS.csv': 'alsfrs_',\n",
        "            'PROACT_FVC.csv': 'fvc_',\n",
        "            'PROACT_VITALSIGNS.csv': 'vitals_',\n",
        "            'PROACT_LABS.csv': 'labs_',\n",
        "            'PROACT_HANDGRIPSTRENGTH.csv': 'grip_',\n",
        "            'PROACT_MUSCLESTRENGTH.csv': 'muscle_'\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Generating Longitudinal Features (from first 3 months) ---\")\n",
        "        for file, prefix in longitudinal_configs.items():\n",
        "            if file in datasets:\n",
        "                df = datasets[file].copy()\n",
        "                time_col_actual = next((c for c in df.columns if 'delta' in c.lower()), None)\n",
        "                if not time_col_actual:\n",
        "                    print(f\"Warning: No time delta column found in {file}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"Processing {file}...\")\n",
        "                summary_features = self.create_longitudinal_features(df, time_col_actual, prefix)\n",
        "                if not summary_features.empty:\n",
        "                    final_df = pd.merge(final_df, summary_features, on='subject_id', how='left')\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    def filter_eligible_patients(self, feature_df, alsfrs_df):\n",
        "        \"\"\"Filter for patients meeting the paper's criteria.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        df.rename(columns={c:'alsfrs_delta' for c in df.columns if 'delta' in c.lower()}, inplace=True)\n",
        "        df['months'] = df['alsfrs_delta'] / 30.44\n",
        "\n",
        "        eligibility = df.groupby('subject_id')['months'].agg(['min', 'max'])\n",
        "        eligible_ids = eligibility[(eligibility['min'] <= 3) & (eligibility['max'] >= 12)].index\n",
        "\n",
        "        print(f\"\\nFound {len(eligible_ids)} eligible patients out of {df['subject_id'].nunique()}.\")\n",
        "        return feature_df[feature_df['subject_id'].isin(eligible_ids)]\n",
        "\n",
        "    def run_pipeline(self, file_path=''):\n",
        "        \"\"\"Execute the complete data preprocessing pipeline.\"\"\"\n",
        "        print(\"====== Starting ALS Data Preprocessing Pipeline ======\")\n",
        "        datasets = self.load_and_inspect_data(file_path)\n",
        "\n",
        "        if 'PROACT_ALSFRS.csv' not in datasets:\n",
        "            print(\"CRITICAL ERROR: PROACT_ALSFRS.csv not found. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        datasets['PROACT_ALSFRS.csv'] = self._convert_alsfrs_r(datasets['PROACT_ALSFRS.csv'])\n",
        "\n",
        "        target_df = self.calculate_alsfrs_slope(datasets['PROACT_ALSFRS.csv'])\n",
        "        print(f\"\\nCalculated ALSFRS slope for {len(target_df)} patients.\")\n",
        "\n",
        "        full_features = self.merge_all_features(datasets)\n",
        "\n",
        "        eligible_features = self.filter_eligible_patients(full_features, datasets['PROACT_ALSFRS.csv'])\n",
        "\n",
        "        final_df = pd.merge(eligible_features, target_df, on='subject_id', how='inner')\n",
        "\n",
        "        print(\"\\n--- Handling Missing Values ---\")\n",
        "        missing_thresh = 0.30\n",
        "        initial_cols = len(final_df.columns)\n",
        "        max_missing = len(final_df) * (1 - missing_thresh)\n",
        "        final_df.dropna(axis=1, thresh=max_missing, inplace=True)\n",
        "        print(f\"Dropped {initial_cols - len(final_df.columns)} features with >{missing_thresh*100}% missing values.\")\n",
        "\n",
        "        X = final_df.drop(columns=['subject_id', 'alsfrs_slope'])\n",
        "        y = final_df['alsfrs_slope']\n",
        "\n",
        "        valid_y_mask = y.notna()\n",
        "        X = X[valid_y_mask]\n",
        "        y = y[valid_y_mask]\n",
        "        subject_ids = final_df.loc[valid_y_mask, 'subject_id']\n",
        "\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "        print(\"\\n--- Performing Feature Selection (Top 30 via Random Forest) ---\")\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_imputed, y)\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': rf.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        selected_features = importance_df['feature'].head(30).tolist()\n",
        "        X_selected = X_imputed[selected_features]\n",
        "\n",
        "        print(\"\\n====== Pipeline Complete ======\")\n",
        "        print(f\"Final feature matrix shape: {X_selected.shape}\")\n",
        "        print(f\"Final target vector shape: {y.shape}\")\n",
        "\n",
        "        # Save the final data for the next step\n",
        "        final_output = pd.concat([subject_ids.reset_index(drop=True),\n",
        "                                  y.reset_index(drop=True),\n",
        "                                  X_selected.reset_index(drop=True)], axis=1)\n",
        "        final_output.to_csv(\"final_processed_als_data.csv\", index=False)\n",
        "        print(\"\\n✅ Successfully saved processed data to 'final_processed_als_data.csv'\")\n",
        "\n",
        "        return {\n",
        "            'X': X_selected,\n",
        "            'y': y,\n",
        "            'subject_ids': subject_ids,\n",
        "            'feature_importance': importance_df,\n",
        "        }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- IMPORTANT ---\n",
        "    # If your CSV files are in a different folder, change this path.\n",
        "    # For example: file_path = \"C:/Users/YourUser/Downloads/PROACT_data/\"\n",
        "    file_path = \"\"\n",
        "\n",
        "    processor = ALSDataProcessor()\n",
        "    processed_data = processor.run_pipeline(file_path=file_path)\n",
        "\n",
        "    if processed_data:\n",
        "        print(\"\\n--- Top 15 Most Important Features ---\")\n",
        "        print(processed_data['feature_importance'].head(15))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculates RMSD and PCC.\"\"\"\n",
        "    rmsd = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    pcc, _ = pearsonr(y_true, y_pred)\n",
        "    return rmsd, pcc\n",
        "\n",
        "def run_classical_pipeline():\n",
        "    \"\"\"\n",
        "    Loads the processed data, trains baseline models, and evaluates their performance.\n",
        "    \"\"\"\n",
        "    print(\"====== Starting Classical Baseline Model Pipeline ======\")\n",
        "\n",
        "    # --- 1. Load Data ---\n",
        "    try:\n",
        "        data = pd.read_csv(\"final_processed_als_data.csv\")\n",
        "        print(f\"✓ Successfully loaded 'final_processed_als_data.csv' with shape {data.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"✗ ERROR: 'final_processed_als_data.csv' not found. Please run the preprocessing script first.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Prepare Data ---\n",
        "    X = data.drop(columns=['subject_id', 'alsfrs_slope'])\n",
        "    y = data['alsfrs_slope']\n",
        "\n",
        "    # 80/20 Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(f\"Data split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples).\")\n",
        "\n",
        "    # Scale data for SVR\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # --- 3. Train and Evaluate Models ---\n",
        "    results = {}\n",
        "\n",
        "    # Model 1: Random Forest Regressor\n",
        "    print(\"\\n--- Training Random Forest Regressor ---\")\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    rf_rmsd, rf_pcc = calculate_metrics(y_test, rf_preds)\n",
        "    results['Random Forest'] = {'RMSD': rf_rmsd, 'PCC': rf_pcc}\n",
        "    print(\"✓ Training and evaluation complete.\")\n",
        "\n",
        "    # Model 2: Support Vector Regressor\n",
        "    print(\"\\n--- Training Support Vector Regressor (SVR) ---\")\n",
        "    svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "    svr_model.fit(X_train_scaled, y_train)\n",
        "    svr_preds = svr_model.predict(X_test_scaled)\n",
        "    svr_rmsd, svr_pcc = calculate_metrics(y_test, svr_preds)\n",
        "    results['Support Vector Regressor'] = {'RMSD': svr_rmsd, 'PCC': svr_pcc}\n",
        "    print(\"✓ Training and evaluation complete.\")\n",
        "\n",
        "    # --- 4. Display Results ---\n",
        "    print(\"\\n====== Classical Model Performance ======\")\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(results_df)\n",
        "    print(\"\\nReminder:\")\n",
        "    print(\"  - RMSD (Root Mean Squared Deviation): Lower is better.\")\n",
        "    print(\"  - PCC (Pearson Correlation Coefficient): Higher is better (closer to 1.0).\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_classical_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EtbWc-T3HO",
        "outputId": "053f8f8e-5033-4c6f-e06f-cbf9b8abb3c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Starting Classical Baseline Model Pipeline ======\n",
            "✓ Successfully loaded 'final_processed_als_data.csv' with shape (2022, 32)\n",
            "Data split into training (1617 samples) and testing (405 samples).\n",
            "\n",
            "--- Training Random Forest Regressor ---\n",
            "✓ Training and evaluation complete.\n",
            "\n",
            "--- Training Support Vector Regressor (SVR) ---\n",
            "✓ Training and evaluation complete.\n",
            "\n",
            "====== Classical Model Performance ======\n",
            "                              RMSD       PCC\n",
            "Random Forest             0.566234  0.228837\n",
            "Support Vector Regressor  0.578394  0.212229\n",
            "\n",
            "Reminder:\n",
            "  - RMSD (Root Mean Squared Deviation): Lower is better.\n",
            "  - PCC (Pearson Correlation Coefficient): Higher is better (closer to 1.0).\n"
          ]
        }
      ]
    }
  ]
}