{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGnQu7BiejRf5R8UuGLVlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/ALS_QNN_PRO_ACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "%pip install qiskit_algorithms\n",
        "\n",
        "# Qiskit Imports\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.regressors import VQR\n",
        "from qiskit.primitives import Sampler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2alDHQpyaLS",
        "outputId": "fdac2b24-3b12-46d0-97b5-79bfd7b7848a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit_algorithms in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: qiskit>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit_algorithms) (1.4.4)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.12/dist-packages (from qiskit_algorithms) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit_algorithms) (2.0.2)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (0.17.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit_algorithms) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit>=1.0->qiskit_algorithms) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit>=1.0->qiskit_algorithms) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3KCwYyArHq",
        "outputId": "a40c3e1f-ac03-4a18-c0e0-6fcda3e27351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Starting ALS Data Preprocessing Pipeline ======\n",
            "--- Loading and Inspecting Data ---\n",
            "✓ PROACT_ALSFRS.csv: Loaded successfully with shape (73845, 20)\n",
            "✓ PROACT_FVC.csv: Loaded successfully with shape (49110, 10)\n",
            "✓ PROACT_VITALSIGNS.csv: Loaded successfully with shape (84721, 36)\n",
            "✓ PROACT_RILUZOLE.csv: Loaded successfully with shape (10363, 3)\n",
            "✓ PROACT_DEMOGRAPHICS.csv: Loaded successfully with shape (12504, 14)\n",
            "✓ PROACT_LABS.csv: Loaded successfully with shape (2937162, 5)\n",
            "✓ PROACT_DEATHDATA.csv: Loaded successfully with shape (5043, 3)\n",
            "✓ PROACT_HANDGRIPSTRENGTH.csv: Loaded successfully with shape (19032, 11)\n",
            "✓ PROACT_MUSCLESTRENGTH.csv: Loaded successfully with shape (204875, 10)\n",
            "✓ PROACT_ALSHISTORY.csv: Loaded successfully with shape (13765, 16)\n",
            "\n",
            "Calculated ALSFRS slope for 2023 patients.\n",
            "\n",
            "--- Generating Longitudinal Features (from first 3 months) ---\n",
            "Processing PROACT_ALSFRS.csv...\n",
            "Processing PROACT_FVC.csv...\n",
            "Processing PROACT_VITALSIGNS.csv...\n",
            "Processing PROACT_LABS.csv...\n",
            "Processing PROACT_HANDGRIPSTRENGTH.csv...\n",
            "Processing PROACT_MUSCLESTRENGTH.csv...\n",
            "\n",
            "Found 3475 eligible patients out of 8538.\n",
            "\n",
            "--- Handling Missing Values ---\n",
            "Dropped 65 features with >30.0% missing values.\n",
            "\n",
            "--- Performing Feature Selection (Top 30 via Random Forest) ---\n",
            "\n",
            "====== Pipeline Complete ======\n",
            "Final feature matrix shape: (2022, 30)\n",
            "Final target vector shape: (2022,)\n",
            "\n",
            "✅ Successfully saved processed data to 'final_processed_als_data.csv'\n",
            "\n",
            "--- Top 15 Most Important Features ---\n",
            "                                  feature  importance\n",
            "97              alsfrs_ALSFRS_Total_slope    0.042145\n",
            "132      fvc_Subject_Liters_Trial_1_slope    0.035096\n",
            "131        fvc_Subject_Liters_Trial_1_std    0.020553\n",
            "230                   vitals_Weight_slope    0.018605\n",
            "187   vitals_Blood_Pressure_Diastolic_std    0.017642\n",
            "195  vitals_Blood_Pressure_Systolic_slope    0.016184\n",
            "96                alsfrs_ALSFRS_Total_std    0.015429\n",
            "194    vitals_Blood_Pressure_Systolic_std    0.015366\n",
            "208                      vitals_Pulse_std    0.014863\n",
            "205                   vitals_Pulse_median    0.014735\n",
            "1                                     Age    0.014160\n",
            "233       vitals_Vital_Signs_Delta_median    0.014041\n",
            "229                     vitals_Weight_std    0.013922\n",
            "209                    vitals_Pulse_slope    0.012839\n",
            "216         vitals_Respiratory_Rate_slope    0.012822\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ALSDataProcessor:\n",
        "    \"\"\"\n",
        "    A robust class to load, clean, and process PRO-ACT data for predicting ALSFRS slope,\n",
        "    replicating the methodology from the \"Deep learning methods to predict amyotrophic\n",
        "    lateral sclerosis disease progression\" paper.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        # A list of columns to exclude from feature engineering\n",
        "        self.id_and_delta_cols = [\n",
        "            'subject_id', 'alsfrs_delta', 'fvc_delta', 'vitals_delta',\n",
        "            'labs_delta', 'grip_delta', 'muscle_delta', 'onset_delta',\n",
        "            'death_delta', 'history_delta'\n",
        "        ]\n",
        "\n",
        "    def _convert_alsfrs_r(self, alsfrs_df):\n",
        "        \"\"\"Convert ALSFRS-R questions to the original ALSFRS format.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        # Ensure ALSFRS_Total is numeric, coercing errors\n",
        "        df['ALSFRS_Total'] = pd.to_numeric(df['ALSFRS_Total'], errors='coerce')\n",
        "        return df\n",
        "\n",
        "    def load_and_inspect_data(self, file_path=''):\n",
        "        \"\"\"Load all datasets and inspect their structure.\"\"\"\n",
        "        datasets = {}\n",
        "        file_list = [\n",
        "            'PROACT_ALSFRS.csv', 'PROACT_FVC.csv', 'PROACT_VITALSIGNS.csv',\n",
        "            'PROACT_RILUZOLE.csv', 'PROACT_DEMOGRAPHICS.csv', 'PROACT_LABS.csv',\n",
        "            'PROACT_DEATHDATA.csv', 'PROACT_HANDGRIPSTRENGTH.csv',\n",
        "            'PROACT_MUSCLESTRENGTH.csv', 'PROACT_ALSHISTORY.csv' # Added missing file\n",
        "        ]\n",
        "        print(\"--- Loading and Inspecting Data ---\")\n",
        "        for file_name in file_list:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path + file_name)\n",
        "\n",
        "                # --- CORRECTED RENAMING LOGIC ---\n",
        "                # Check if 'subject_id' already exists. If not, find a candidate and rename only the first one found.\n",
        "                if 'subject_id' not in df.columns:\n",
        "                    potential_id_cols = [col for col in df.columns if 'subject' in col.lower()]\n",
        "                    if potential_id_cols:\n",
        "                        df.rename(columns={potential_id_cols[0]: 'subject_id'}, inplace=True)\n",
        "                # --- END CORRECTION ---\n",
        "\n",
        "                datasets[file_name] = df\n",
        "                print(f\"✓ {file_name}: Loaded successfully with shape {df.shape}\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"✗ {file_name}: File not found. Will be skipped.\")\n",
        "        return datasets\n",
        "\n",
        "    def calculate_alsfrs_slope(self, alsfrs_df):\n",
        "        \"\"\"Calculate the primary target variable: ALSFRS slope between months 3-12.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        df.rename(columns={c:'alsfrs_delta' for c in df.columns if 'delta' in c.lower()}, inplace=True)\n",
        "        df['months'] = df['alsfrs_delta'] / 30.44\n",
        "        df.sort_values(['subject_id', 'months'], inplace=True)\n",
        "\n",
        "        slopes = {}\n",
        "        for subject_id, subject_data in df.groupby('subject_id'):\n",
        "            t1_candidates = subject_data[(subject_data['months'] > 3) & (subject_data['months'] <= 12)]\n",
        "            t2_candidates = subject_data[subject_data['months'] >= 12]\n",
        "\n",
        "            if not t1_candidates.empty and not t2_candidates.empty:\n",
        "                t1_row = t1_candidates.iloc[0]\n",
        "                t2_row = t2_candidates.iloc[0]\n",
        "\n",
        "                t1, alsfrs_t1 = t1_row['months'], t1_row['ALSFRS_Total']\n",
        "                t2, alsfrs_t2 = t2_row['months'], t2_row['ALSFRS_Total']\n",
        "\n",
        "                if t2 > t1 and pd.notna(alsfrs_t1) and pd.notna(alsfrs_t2):\n",
        "                    slope = (alsfrs_t2 - alsfrs_t1) / (t2 - t1)\n",
        "                    slopes[subject_id] = slope\n",
        "\n",
        "        return pd.DataFrame(list(slopes.items()), columns=['subject_id', 'alsfrs_slope'])\n",
        "\n",
        "\n",
        "    def create_longitudinal_features(self, df, time_col, prefix):\n",
        "        \"\"\"Create the seven summary statistics from longitudinal data (first 3 months).\"\"\"\n",
        "        df_sorted = df.sort_values(['subject_id', time_col])\n",
        "        df_filtered = df_sorted[df_sorted[time_col] <= 90].copy()\n",
        "\n",
        "        value_cols = [col for col in df_filtered.select_dtypes(include=np.number).columns\n",
        "                      if col.lower() not in self.id_and_delta_cols]\n",
        "\n",
        "        if not value_cols:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        summary_dfs = []\n",
        "        for value_col in value_cols:\n",
        "            grouped = df_filtered.groupby('subject_id')\n",
        "            summary = grouped[value_col].agg(['min', 'max', 'median', 'first', 'last']).join(\n",
        "                grouped[value_col].std(ddof=0).rename('std')\n",
        "            )\n",
        "\n",
        "            # Ensure there are at least two data points for slope calculation\n",
        "            slope_df = grouped.apply(\n",
        "                lambda g: (g[value_col].iloc[-1] - g[value_col].iloc[0]) / (g[time_col].iloc[-1] - g[time_col].iloc[0])\n",
        "                if len(g) > 1 and (g[time_col].iloc[-1] - g[time_col].iloc[0]) > 0 else np.nan\n",
        "            ).rename('slope')\n",
        "\n",
        "            summary = summary.join(slope_df).fillna(0) # Fill NaN slopes with 0\n",
        "            summary.columns = [f\"{prefix}{value_col}_{stat}\" for stat in summary.columns]\n",
        "            summary_dfs.append(summary)\n",
        "\n",
        "\n",
        "        return pd.concat(summary_dfs, axis=1).reset_index()\n",
        "\n",
        "    def process_static_data(self, df):\n",
        "        \"\"\"Process static data files (like demographics, riluzole).\"\"\"\n",
        "        processed = df.copy()\n",
        "        for col in processed.select_dtypes(include=['object', 'category']).columns:\n",
        "            if col != 'subject_id':\n",
        "                le = self.label_encoders.setdefault(col, LabelEncoder())\n",
        "                processed[col] = le.fit_transform(processed[col].astype(str))\n",
        "        return processed.drop_duplicates(subset=['subject_id'])\n",
        "\n",
        "    def merge_all_features(self, datasets):\n",
        "        \"\"\"Merge all static and longitudinal features into a single dataframe.\"\"\"\n",
        "        if 'PROACT_DEMOGRAPHICS.csv' not in datasets:\n",
        "            raise ValueError(\"Demographics file is missing.\")\n",
        "\n",
        "        final_df = self.process_static_data(datasets['PROACT_DEMOGRAPHICS.csv'])\n",
        "\n",
        "        static_files = ['PROACT_RILUZOLE.csv', 'PROACT_ALSHISTORY.csv'] # Added ALSHISTORY\n",
        "        for file in static_files:\n",
        "            if file in datasets:\n",
        "                static_df = self.process_static_data(datasets[file])\n",
        "                final_df = pd.merge(final_df, static_df, on='subject_id', how='left')\n",
        "\n",
        "        longitudinal_configs = {\n",
        "            'PROACT_ALSFRS.csv': 'alsfrs_',\n",
        "            'PROACT_FVC.csv': 'fvc_',\n",
        "            'PROACT_VITALSIGNS.csv': 'vitals_',\n",
        "            'PROACT_LABS.csv': 'labs_',\n",
        "            'PROACT_HANDGRIPSTRENGTH.csv': 'grip_',\n",
        "            'PROACT_MUSCLESTRENGTH.csv': 'muscle_'\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Generating Longitudinal Features (from first 3 months) ---\")\n",
        "        for file, prefix in longitudinal_configs.items():\n",
        "            if file in datasets:\n",
        "                df = datasets[file].copy()\n",
        "                time_col_actual = next((c for c in df.columns if 'delta' in c.lower()), None)\n",
        "                if not time_col_actual:\n",
        "                    print(f\"Warning: No time delta column found in {file}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"Processing {file}...\")\n",
        "                summary_features = self.create_longitudinal_features(df, time_col_actual, prefix)\n",
        "                if not summary_features.empty:\n",
        "                    final_df = pd.merge(final_df, summary_features, on='subject_id', how='left')\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    def filter_eligible_patients(self, feature_df, alsfrs_df):\n",
        "        \"\"\"Filter for patients meeting the paper's criteria.\"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        df.rename(columns={c:'alsfrs_delta' for c in df.columns if 'delta' in c.lower()}, inplace=True)\n",
        "        df['months'] = df['alsfrs_delta'] / 30.44\n",
        "\n",
        "        eligibility = df.groupby('subject_id')['months'].agg(['min', 'max'])\n",
        "        eligible_ids = eligibility[(eligibility['min'] <= 3) & (eligibility['max'] >= 12)].index\n",
        "\n",
        "        print(f\"\\nFound {len(eligible_ids)} eligible patients out of {df['subject_id'].nunique()}.\")\n",
        "        return feature_df[feature_df['subject_id'].isin(eligible_ids)]\n",
        "\n",
        "    def run_pipeline(self, file_path=''):\n",
        "        \"\"\"Execute the complete data preprocessing pipeline.\"\"\"\n",
        "        print(\"====== Starting ALS Data Preprocessing Pipeline ======\")\n",
        "        datasets = self.load_and_inspect_data(file_path)\n",
        "\n",
        "        if 'PROACT_ALSFRS.csv' not in datasets:\n",
        "            print(\"CRITICAL ERROR: PROACT_ALSFRS.csv not found. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        datasets['PROACT_ALSFRS.csv'] = self._convert_alsfrs_r(datasets['PROACT_ALSFRS.csv'])\n",
        "\n",
        "        target_df = self.calculate_alsfrs_slope(datasets['PROACT_ALSFRS.csv'])\n",
        "        print(f\"\\nCalculated ALSFRS slope for {len(target_df)} patients.\")\n",
        "\n",
        "        full_features = self.merge_all_features(datasets)\n",
        "\n",
        "        eligible_features = self.filter_eligible_patients(full_features, datasets['PROACT_ALSFRS.csv'])\n",
        "\n",
        "        final_df = pd.merge(eligible_features, target_df, on='subject_id', how='inner')\n",
        "\n",
        "        print(\"\\n--- Handling Missing Values ---\")\n",
        "        missing_thresh = 0.30\n",
        "        initial_cols = len(final_df.columns)\n",
        "        max_missing = len(final_df) * (1 - missing_thresh)\n",
        "        final_df.dropna(axis=1, thresh=max_missing, inplace=True)\n",
        "        print(f\"Dropped {initial_cols - len(final_df.columns)} features with >{missing_thresh*100}% missing values.\")\n",
        "\n",
        "        X = final_df.drop(columns=['subject_id', 'alsfrs_slope'])\n",
        "        y = final_df['alsfrs_slope']\n",
        "\n",
        "        valid_y_mask = y.notna()\n",
        "        X = X[valid_y_mask]\n",
        "        y = y[valid_y_mask]\n",
        "        subject_ids = final_df.loc[valid_y_mask, 'subject_id']\n",
        "\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "        print(\"\\n--- Performing Feature Selection (Top 30 via Random Forest) ---\")\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_imputed, y)\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': rf.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        selected_features = importance_df['feature'].head(30).tolist()\n",
        "        X_selected = X_imputed[selected_features]\n",
        "\n",
        "        print(\"\\n====== Pipeline Complete ======\")\n",
        "        print(f\"Final feature matrix shape: {X_selected.shape}\")\n",
        "        print(f\"Final target vector shape: {y.shape}\")\n",
        "\n",
        "        # Save the final data for the next step\n",
        "        final_output = pd.concat([subject_ids.reset_index(drop=True),\n",
        "                                  y.reset_index(drop=True),\n",
        "                                  X_selected.reset_index(drop=True)], axis=1)\n",
        "        final_output.to_csv(\"final_processed_als_data.csv\", index=False)\n",
        "        print(\"\\n✅ Successfully saved processed data to 'final_processed_als_data.csv'\")\n",
        "\n",
        "        return {\n",
        "            'X': X_selected,\n",
        "            'y': y,\n",
        "            'subject_ids': subject_ids,\n",
        "            'feature_importance': importance_df,\n",
        "        }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- IMPORTANT ---\n",
        "    # If your CSV files are in a different folder, change this path.\n",
        "    # For example: file_path = \"C:/Users/YourUser/Downloads/PROACT_data/\"\n",
        "    file_path = \"\"\n",
        "\n",
        "    processor = ALSDataProcessor()\n",
        "    processed_data = processor.run_pipeline(file_path=file_path)\n",
        "\n",
        "    if processed_data:\n",
        "        print(\"\\n--- Top 15 Most Important Features ---\")\n",
        "        print(processed_data['feature_importance'].head(15))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculates RMSD and PCC.\"\"\"\n",
        "    rmsd = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    pcc, _ = pearsonr(y_true, y_pred)\n",
        "    return rmsd, pcc\n",
        "\n",
        "def run_classical_pipeline():\n",
        "    \"\"\"\n",
        "    Loads the processed data, trains baseline models, and evaluates their performance.\n",
        "    \"\"\"\n",
        "    print(\"====== Starting Classical Baseline Model Pipeline ======\")\n",
        "\n",
        "    # --- 1. Load Data ---\n",
        "    try:\n",
        "        data = pd.read_csv(\"final_processed_als_data.csv\")\n",
        "        print(f\"✓ Successfully loaded 'final_processed_als_data.csv' with shape {data.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"✗ ERROR: 'final_processed_als_data.csv' not found. Please run the preprocessing script first.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Prepare Data ---\n",
        "    X = data.drop(columns=['subject_id', 'alsfrs_slope'])\n",
        "    y = data['alsfrs_slope']\n",
        "\n",
        "    # 80/20 Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(f\"Data split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples).\")\n",
        "\n",
        "    # Scale data for SVR\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # --- 3. Train and Evaluate Models ---\n",
        "    results = {}\n",
        "\n",
        "    # Model 1: Random Forest Regressor\n",
        "    print(\"\\n--- Training Random Forest Regressor ---\")\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    rf_rmsd, rf_pcc = calculate_metrics(y_test, rf_preds)\n",
        "    results['Random Forest'] = {'RMSD': rf_rmsd, 'PCC': rf_pcc}\n",
        "    print(\"✓ Training and evaluation complete.\")\n",
        "\n",
        "    # Model 2: Support Vector Regressor\n",
        "    print(\"\\n--- Training Support Vector Regressor (SVR) ---\")\n",
        "    svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "    svr_model.fit(X_train_scaled, y_train)\n",
        "    svr_preds = svr_model.predict(X_test_scaled)\n",
        "    svr_rmsd, svr_pcc = calculate_metrics(y_test, svr_preds)\n",
        "    results['Support Vector Regressor'] = {'RMSD': svr_rmsd, 'PCC': svr_pcc}\n",
        "    print(\"✓ Training and evaluation complete.\")\n",
        "\n",
        "    # --- 4. Display Results ---\n",
        "    print(\"\\n====== Classical Model Performance ======\")\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(results_df)\n",
        "    print(\"\\nReminder:\")\n",
        "    print(\"  - RMSD (Root Mean Squared Deviation): Lower is better.\")\n",
        "    print(\"  - PCC (Pearson Correlation Coefficient): Higher is better (closer to 1.0).\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_classical_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EtbWc-T3HO",
        "outputId": "4139bdb2-6f8a-488f-f235-c3fc57565d42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Starting Classical Baseline Model Pipeline ======\n",
            "✓ Successfully loaded 'final_processed_als_data.csv' with shape (2022, 32)\n",
            "Data split into training (1617 samples) and testing (405 samples).\n",
            "\n",
            "--- Training Random Forest Regressor ---\n",
            "✓ Training and evaluation complete.\n",
            "\n",
            "--- Training Support Vector Regressor (SVR) ---\n",
            "✓ Training and evaluation complete.\n",
            "\n",
            "====== Classical Model Performance ======\n",
            "                              RMSD       PCC\n",
            "Random Forest             0.561191  0.256830\n",
            "Support Vector Regressor  0.576861  0.225288\n",
            "\n",
            "Reminder:\n",
            "  - RMSD (Root Mean Squared Deviation): Lower is better.\n",
            "  - PCC (Pearson Correlation Coefficient): Higher is better (closer to 1.0).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qml_als_dynamic_safe.py\n",
        "import os, time, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Force unbuffered tqdm in stubborn terminals\n",
        "TQDM_KW = dict(disable=False, mininterval=0.0, dynamic_ncols=True, leave=False)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Qiskit (optional AER)\n",
        "try:\n",
        "    from qiskit_aer.primitives import Estimator as AerEstimator\n",
        "    AER_OK = True\n",
        "except Exception:\n",
        "    AER_OK = False\n",
        "try:\n",
        "    from qiskit.primitives import Estimator as RefEstimator\n",
        "    from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "    from qiskit_machine_learning.algorithms.regressors import VQR\n",
        "    from qiskit_algorithms.optimizers import COBYLA, SPSA\n",
        "    QISKIT_OK = True\n",
        "except Exception:\n",
        "    QISKIT_OK = False\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def safe_pcc(y_true, y_pred):\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    if y_pred.std() == 0 or np.asarray(y_true).std() == 0:\n",
        "        return 0.0\n",
        "    v = pearsonr(y_true, y_pred)[0]\n",
        "    return float(v) if np.isfinite(v) else 0.0\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    rmsd = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    return rmsd, safe_pcc(y_true, y_pred), float(r2_score(y_true, y_pred))\n",
        "\n",
        "def heartbeat(msg):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "class DynamicQMLALS:\n",
        "    def __init__(self, data_path=\"final_processed_als_data.csv\",\n",
        "                 max_qubits=4, time_budget_sec=600):\n",
        "        self.data_path = data_path\n",
        "        self.max_qubits = max_qubits\n",
        "        self.time_budget_sec = time_budget_sec\n",
        "        self.results = {}\n",
        "\n",
        "    def load_data(self):\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        if \"alsfrs_slope\" not in df.columns:\n",
        "            raise ValueError(\"Target column 'alsfrs_slope' not found.\")\n",
        "        X = df.drop(columns=[\"subject_id\", \"alsfrs_slope\"], errors=\"ignore\")\n",
        "        y = df[\"alsfrs_slope\"].values\n",
        "        m = ~np.isnan(y)\n",
        "        X, y = X.loc[m].reset_index(drop=True), y[m]\n",
        "        heartbeat(f\"✓ Data loaded: X={X.shape}, y={y.shape}\")\n",
        "        return X, y\n",
        "\n",
        "    def select_features(self, X, y, k=12):\n",
        "        imp = SimpleImputer(strategy=\"median\")\n",
        "        Xn = imp.fit_transform(X)\n",
        "        rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1).fit(Xn, y)\n",
        "        rf_rank = rf.feature_importances_\n",
        "        mi = mutual_info_regression(Xn, y, random_state=42)\n",
        "        corr = np.array([abs(np.corrcoef(Xn[:, i], y)[0, 1]) if Xn[:, i].std()>0 else 0.0\n",
        "                         for i in range(Xn.shape[1])])\n",
        "        def nz(v): m=v.max(); return v/(m+1e-8) if m>0 else v\n",
        "        score = nz(rf_rank)+nz(mi)+nz(corr)\n",
        "        idx = np.argsort(score)[::-1][:k]\n",
        "        cols = [X.columns[i] for i in idx]\n",
        "        heartbeat(f\"✓ Top-{k} features selected: {cols[:5]} ...\")\n",
        "        return idx, cols\n",
        "\n",
        "    # ---------- FAST QFE fallback (no Aer) ----------\n",
        "    def qfe_approx(self, X01):\n",
        "        \"\"\"Fast interaction expansion: [X | X^2 | pairwise X_i*X_j].\"\"\"\n",
        "        n, d = X01.shape\n",
        "        feats = [X01, X01**2]\n",
        "        # pairwise products (upper triangle)\n",
        "        pairs = []\n",
        "        for i in range(d):\n",
        "            for j in range(i+1, d):\n",
        "                pairs.append((X01[:, i] * X01[:, j])[:, None])\n",
        "        if pairs:\n",
        "            feats.append(np.hstack(pairs))\n",
        "        Z = np.hstack(feats)\n",
        "        return Z\n",
        "\n",
        "    # ---------- True QFE (Aer preferred) ----------\n",
        "    def qfe_true(self, X01, reps=2, batch=64):\n",
        "        from qiskit.quantum_info import SparsePauliOp\n",
        "        Est = AerEstimator if AER_OK else RefEstimator\n",
        "        est = Est()\n",
        "        n, d = X01.shape\n",
        "        from qiskit.circuit.library import ZZFeatureMap\n",
        "        fmap = ZZFeatureMap(feature_dimension=d, reps=reps)\n",
        "\n",
        "        # observables Z_i and Z_iZ_j\n",
        "        z_ops, zz_ops = [], []\n",
        "        for i in range(d):\n",
        "            p = ['I']*d; p[i]='Z'\n",
        "            z_ops.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "        for i in range(d):\n",
        "            for j in range(i+1, d):\n",
        "                p = ['I']*d; p[i]=p[j]='Z'\n",
        "                zz_ops.append(SparsePauliOp.from_list([(\"\".join(p[::-1]), 1.0)]))\n",
        "        observables = z_ops + zz_ops\n",
        "\n",
        "        out = []\n",
        "        rng = range(0, n, batch)\n",
        "        for s in tqdm(rng, desc=f\"QFE true (q={d}, Aer={AER_OK})\", **TQDM_KW):\n",
        "            e = min(s+batch, n)\n",
        "            circs = [fmap.assign_parameters(X01[i]*np.pi, inplace=False) for i in range(s, e)]\n",
        "            # run per-sample to keep mem stable, but at least bar moves\n",
        "            for c in circs:\n",
        "                vals = est.run([c]*len(observables), observables).result().values\n",
        "                out.append(vals)\n",
        "        return np.array(out)\n",
        "\n",
        "    def ridge_baselines(self, X_tr, X_te, y_tr, y_te):\n",
        "        imp = SimpleImputer(strategy=\"median\")\n",
        "        std = StandardScaler()\n",
        "        Xtr = std.fit_transform(imp.fit_transform(X_tr))\n",
        "        Xte = std.transform(imp.transform(X_te))\n",
        "        ridge = Ridge(alpha=1.0, random_state=42).fit(Xtr, y_tr)\n",
        "        ypr = ridge.predict(Xte)\n",
        "        self.results[\"Ridge_std_full\"] = dict(zip([\"rmsd\",\"pcc\",\"r2\"], metrics(y_te, ypr)))\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\\\n",
        "             .fit(imp.transform(X_tr), y_tr)\n",
        "        ypr = rf.predict(imp.transform(X_te))\n",
        "        self.results[\"RF_200_full\"] = dict(zip([\"rmsd\",\"pcc\",\"r2\"], metrics(y_te, ypr)))\n",
        "\n",
        "    def run(self, use_true_qfe=True, vqr_maxiter=20, vqr_restarts=1):\n",
        "        start = time.perf_counter()\n",
        "        X, y = self.load_data()\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        self.ridge_baselines(X_tr, X_te, y_tr, y_te)\n",
        "\n",
        "        # ----- QFE block(s) -----\n",
        "        idx12, _ = self.select_features(X_tr, y_tr, k=12)\n",
        "        imp = SimpleImputer(strategy=\"median\")\n",
        "        Xtr_top = imp.fit_transform(X_tr.iloc[:, idx12])\n",
        "        Xte_top = imp.transform(X_te.iloc[:, idx12])\n",
        "        scaler01 = MinMaxScaler()\n",
        "        Xtr_top01 = scaler01.fit_transform(Xtr_top)\n",
        "        Xte_top01 = scaler01.transform(Xte_top)\n",
        "\n",
        "        # split 12 -> 3 blocks of 4 dims\n",
        "        blocks = [(0,4), (4,8), (8,12)]\n",
        "        Z_tr_list, Z_te_list = [], []\n",
        "        for (a,b) in blocks:\n",
        "            xb_tr, xb_te = Xtr_top01[:, a:b], Xte_top01[:, a:b]\n",
        "            if use_true_qfe and QISKIT_OK and (AER_OK or b-a <= 4):\n",
        "                heartbeat(f\"→ True QFE for block {a}:{b} (dims={b-a})\")\n",
        "                Z_tr_list.append(self.qfe_true(xb_tr, reps=2))\n",
        "                Z_te_list.append(self.qfe_true(xb_te, reps=2))\n",
        "            else:\n",
        "                heartbeat(f\"→ FAST QFE-approx for block {a}:{b} (dims={b-a})\")\n",
        "                Z_tr_list.append(self.qfe_approx(xb_tr))\n",
        "                Z_te_list.append(self.qfe_approx(xb_te))\n",
        "\n",
        "        Z_tr = np.hstack(Z_tr_list)\n",
        "        Z_te = np.hstack(Z_te_list)\n",
        "        ridge_qfe = Ridge(alpha=1.0, random_state=42).fit(Z_tr, y_tr)\n",
        "        ypr_qfe = ridge_qfe.predict(Z_te)\n",
        "        self.results[\"QFEbag(Ridge)\"] = dict(zip([\"rmsd\",\"pcc\",\"r2\"], metrics(y_te, ypr_qfe)))\n",
        "\n",
        "        # ----- VQR (short, visible) -----\n",
        "        if QISKIT_OK:\n",
        "            # 6 -> PCA->4 for VQR\n",
        "            idx6, _ = self.select_features(X_tr, y_tr, k=6)\n",
        "            Xtr6 = imp.fit_transform(X_tr.iloc[:, idx6])\n",
        "            Xte6 = imp.transform(X_te.iloc[:, idx6])\n",
        "            from sklearn.decomposition import PCA\n",
        "            pca4 = PCA(n_components=min(self.max_qubits, Xtr6.shape[1]), random_state=42)\n",
        "            Xtr_q = MinMaxScaler().fit_transform(pca4.fit_transform(Xtr6)) * np.pi\n",
        "            Xte_q = MinMaxScaler().fit_transform(pca4.transform(Xte6)) * np.pi\n",
        "\n",
        "            y_scaler = MinMaxScaler()\n",
        "            ytr_s = y_scaler.fit_transform(y_tr.reshape(-1,1)).ravel()\n",
        "\n",
        "            best = None\n",
        "            for r in tqdm(range(vqr_restarts), desc=\"VQR restarts\", **TQDM_KW):\n",
        "                for name, (rf, ra, opt) in [\n",
        "                    (\"SPSA\", (1,1,\"SPSA\")),\n",
        "                    (\"COBYLA\",(1,2,\"COBYLA\"))\n",
        "                ]:\n",
        "                    if time.perf_counter() - start > self.time_budget_sec:\n",
        "                        heartbeat(\"⏱ Time budget hit; skipping remaining VQR.\")\n",
        "                        break\n",
        "                    # Attach callback on optimizer to tick tqdm\n",
        "                    try:\n",
        "                        opt_obj = SPSA(maxiter=vqr_maxiter, callback=lambda *a, **k: tqdm.write(\"·\", end=\"\", flush=True)) \\\n",
        "                                  if opt==\"SPSA\" else COBYLA(maxiter=vqr_maxiter, callback=lambda *a, **k: tqdm.write(\"·\", end=\"\", flush=True))\n",
        "                    except TypeError:\n",
        "                        opt_obj = SPSA(maxiter=vqr_maxiter) if opt==\"SPSA\" else COBYLA(maxiter=vqr_maxiter)\n",
        "                    fmap = ZZFeatureMap(feature_dimension=Xtr_q.shape[1], reps=rf)\n",
        "                    ans = RealAmplitudes(num_qubits=Xtr_q.shape[1], reps=ra, entanglement=\"linear\")\n",
        "                    vqr = VQR(feature_map=fmap, ansatz=ans, optimizer=opt_obj)\n",
        "                    print(f\"Training VQR[{name}] (iter={vqr_maxiter})...\", flush=True)\n",
        "                    vqr.fit(Xtr_q, ytr_s)\n",
        "                    ypr_s = vqr.predict(Xte_q)\n",
        "                    ypr = y_scaler.inverse_transform(ypr_s.reshape(-1,1)).ravel()\n",
        "                    key = f\"VQR[{name}]-q{Xtr_q.shape[1]}-rf{rf}-ra{ra}-r{r+1}\"\n",
        "                    self.results[key] = dict(zip([\"rmsd\",\"pcc\",\"r2\"], metrics(y_te, ypr)))\n",
        "                    if (best is None) or (self.results[key][\"rmsd\"] < best[1][\"rmsd\"]):\n",
        "                        best = (key, self.results[key])\n",
        "            if best:\n",
        "                heartbeat(f\"Best VQR: {best[0]} -> RMSD={best[1]['rmsd']:.4f}, PCC={best[1]['pcc']:.4f}\")\n",
        "\n",
        "        # ----- Summary -----\n",
        "        rows = []\n",
        "        for k,v in self.results.items():\n",
        "            if isinstance(v, dict) and \"rmsd\" in v:\n",
        "                rows.append([k, v[\"rmsd\"], v[\"pcc\"], v[\"r2\"]])\n",
        "        df = pd.DataFrame(rows, columns=[\"Model\",\"RMSD\",\"PCC\",\"R2\"]).sort_values(\"RMSD\")\n",
        "        print(\"\\n=== COMPARISON (lower RMSD better; higher PCC/R2 better) ===\", flush=True)\n",
        "        print(df.to_string(index=False, float_format=\"%.4f\"), flush=True)\n",
        "        return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ensure unbuffered output in many shells\n",
        "    os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "    runner = DynamicQMLALS(max_qubits=4, time_budget_sec=600)\n",
        "    _ = runner.run(use_true_qfe=AER_OK, vqr_maxiter=20, vqr_restarts=1)\n"
      ],
      "metadata": {
        "id": "e8R8_BcrxWYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}