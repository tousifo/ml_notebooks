{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/ALS_QNN_PRO_ACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJL4ko9VLqi"
      },
      "source": [
        "## This first cell\n",
        "grabs the basics we need: it installs Qiskit, imports the pieces for the circuit and optimizer, and brings in simple tools to split and scale the data plus quick checks for error and correlation. If installing `qiskit_algorithms` fails, try `qiskit-algorithms` instead. We scale the inputs so the angles stay in a small range, which makes training smoother. Last thing: set the feature map to match the number of features you end up with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2alDHQpyaLS",
        "outputId": "fa60eaf0-82ce-452d-ae70-6eac3176373e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit~=1.0 in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
            "Requirement already satisfied: qiskit-machine-learning~=0.8.1 in /usr/local/lib/python3.12/dist-packages (0.8.4)\n",
            "Requirement already satisfied: qiskit_algorithms in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (0.17.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit~=1.0) (0.13.0)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning~=0.8.1) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning~=0.8.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit~=1.0) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning~=0.8.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning~=0.8.1) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit~=1.0) (1.3.0)\n",
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.42.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: pennylane-lightning[gpu] in /usr/local/lib/python3.12/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray<0.8,>=0.6.11 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.7.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning[gpu]) (0.3.30.0.2)\n",
            "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning[gpu]) (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: custatevec-cu12 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (1.10.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split  # quick train/validation split\n",
        "from sklearn.preprocessing import MinMaxScaler        # keep features in a compact range for angle encoding\n",
        "from sklearn.metrics import mean_squared_error        # regression loss (lower is better)\n",
        "from scipy.stats import pearsonr                      # correlation between predictions and targets (closer to 1 is better)\n",
        "%pip install qiskit~=1.0 qiskit-machine-learning~=0.8.1 qiskit_algorithms  # pinned install; if it fails, try 'qiskit-algorithms' manually\n",
        "\n",
        "# Qiskit Imports\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes  # feature map + ansatz for the variational circuit\n",
        "from qiskit_algorithms.optimizers import COBYLA                  # gradient-free optimizer suited to noisy objectives\n",
        "from qiskit_machine_learning.algorithms.regressors import VQR    # variational quantum regressor wrapper\n",
        "from qiskit.primitives import Sampler\n",
        "# Install pennylane and lightning plugins\n",
        "%pip install pennylane pennylane-lightning[gpu] torch torchvision torchaudio                          # primitive that evaluates circuits (shot-based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LFlmvTIW-NT"
      },
      "source": [
        "# 2nd Cell\n",
        "takes the raw PRO-ACT CSVs and turns them into a clean, ready-to-use dataset. It lines up each person’s timeline by their first ALSFRS visit, builds simple summary features from the first 0–90 days across the available tables (ALSFRS, FVC, vitals, labs, grip, muscle), tidies ALSFRS-R items, and uses the best FVC trial at each test time. For every signal it makes seven summaries (min, max, median, std, first, last, slope) and drops columns with lots of missing values (>30%). It doesn’t do any scaling or encoding here—you’ll do that later while training. Only people with ALSFRS measurements after both 3 months and 12 months are kept, and the target is the slope between the first record after 3 months and the first after 12 months. Tables without a time column are skipped, messy wide/long layouts are handled as best as possible, and if your time isn’t in days, make sure the column names include “day” or “delta.” The cell saves `final_processed_als_data.csv` and also returns the features (`X`), target (`y`), subject IDs, and the joined data frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3KCwYyArHq",
        "outputId": "a15b64be-f9df-4240-c754-cfb0a7ff8c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Starting ALS Data Preprocessing Pipeline ======\n",
            "--- Loading and Inspecting Data ---\n",
            "✓ PROACT_ALSFRS.csv: (73845, 20)\n",
            "✓ PROACT_FVC.csv: (49110, 10)\n",
            "✓ PROACT_VITALSIGNS.csv: (84721, 36)\n",
            "✓ PROACT_RILUZOLE.csv: (10363, 3)\n",
            "✓ PROACT_DEMOGRAPHICS.csv: (12504, 14)\n",
            "✓ PROACT_LABS.csv: (2937162, 5)\n",
            "✓ PROACT_DEATHDATA.csv: (5043, 3)\n",
            "✓ PROACT_HANDGRIPSTRENGTH.csv: (19032, 11)\n",
            "✓ PROACT_MUSCLESTRENGTH.csv: (204875, 10)\n",
            "✓ PROACT_ALSHISTORY.csv: (13765, 16)\n",
            "\n",
            "Calculated ALSFRS slope for 1897 patients.\n",
            "\n",
            "--- Generating Longitudinal Features (anchored to first ALSFRS; window = 0–90 days) ---\n",
            "\n",
            "Eligible patients: 3317 / 8538\n",
            "\n",
            "--- Handling Missing Values (Dropping cols with >30% missing) ---\n",
            "Dropped 1413 columns for >30% missingness.\n",
            "\n",
            "✅ Saved CV-safe engineered data to 'final_processed_als_data.csv'\n",
            "Feature matrix shape: (1897, 346) | Target length: 1897\n",
            "\n",
            "Preview of columns: ['Demographics_Delta', 'Age', 'Race_Caucasian', 'Sex', 'Subject_used_Riluzole', 'Riluzole_use_Delta', 'Subject_ALS_History_Delta', 'Site_of_Onset', 'alsfrs_Q1_Speech_min', 'alsfrs_Q1_Speech_max']\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # keep notebook output tidy; data has mixed types/old columns\n",
        "\n",
        "\n",
        "class ALSDataProcessor:\n",
        "    \"\"\"\n",
        "    CV-safe preprocessing for PRO-ACT to reproduce the paper's EDA:\n",
        "      - Anchor to FIRST ALSFRS visit (t=0) per subject\n",
        "      - Inputs: first 3 months (0–90 days from anchor) for all longitudinal tables\n",
        "      - Outcome: ALSFRS Total slope between FIRST-after-3mo and FIRST-after-12mo\n",
        "      - ALSFRS-R harmonization hooks (Q10 from 10a; merge Q5a/Q5b)\n",
        "      - FVC reduced to max-of-trials per test before summarization\n",
        "      - Seven summaries: min, max, median, std, first, last, slope (slope=NaN if only 1 obs)\n",
        "      - Drop features with >30% missing (no other transforms here — avoid leakage)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # identifiers/time-like columns we should NOT summarize as numeric features\n",
        "        self.id_and_delta_cols = {\n",
        "            \"subject_id\",\n",
        "            \"alsfrs_delta\",\n",
        "            \"fvc_delta\",\n",
        "            \"vitals_delta\",\n",
        "            \"labs_delta\",\n",
        "            \"grip_delta\",\n",
        "            \"muscle_delta\",\n",
        "            \"onset_delta\",\n",
        "            \"death_delta\",\n",
        "            \"history_delta\",\n",
        "            \"anchor_days\",\n",
        "            \"days_from_alsfrs_anchor\",\n",
        "        }\n",
        "\n",
        "    # --------- Utilities ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_time_col(df: pd.DataFrame) -> Optional[str]:\n",
        "        \"\"\"Find a time column that represents days since baseline in a table.\"\"\"\n",
        "        # Prefer delta\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if \"delta\" in lc:\n",
        "                return c\n",
        "        # Fallback to 'days' if present\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if \"day\" in lc:\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    # --------- ALSFRS-R harmonization ---------\n",
        "\n",
        "    def _convert_alsfrs_r(self, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prepare ALSFRS table. If ALSFRS-R subitems exist, map per paper:\n",
        "          - Q10 <- 10a (dyspnea). Ignore 10b/10c.\n",
        "          - Merge Q5a/Q5b into Q5 if present.\n",
        "        If only totals exist, this is a no-op aside from coercions.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "\n",
        "        if \"ALSFRS_Total\" in df.columns:\n",
        "            df[\"ALSFRS_Total\"] = pd.to_numeric(df[\"ALSFRS_Total\"], errors=\"coerce\")\n",
        "\n",
        "        # Try to locate subitems by loose names\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "\n",
        "        # Q10 from 10a (dyspnea)\n",
        "        for candidate in [\"alsfrs_r_q10a\", \"q10a\", \"dyspnea\", \"alsfrs_q10a\"]:\n",
        "            if candidate in cols:\n",
        "                df[\"Q10\"] = pd.to_numeric(df[cols[candidate]], errors=\"coerce\")\n",
        "                break\n",
        "\n",
        "        # Merge Q5a/Q5b\n",
        "        q5a = next(\n",
        "            (cols[k] for k in [\"alsfrs_r_q5a\", \"q5a\", \"cutting_wout_gastrostomy\"] if k in cols),\n",
        "            None,\n",
        "        )\n",
        "        q5b = next(\n",
        "            (cols[k] for k in [\"alsfrs_r_q5b\", \"q5b\", \"cutting_with_gastrostomy\"] if k in cols),\n",
        "            None,\n",
        "        )\n",
        "        if q5a and q5b:\n",
        "            q5a_vals = pd.to_numeric(df[q5a], errors=\"coerce\").values\n",
        "            q5b_vals = pd.to_numeric(df[q5b], errors=\"coerce\").values\n",
        "            df[\"Q5\"] = np.nanmax(np.vstack([q5a_vals, q5b_vals]), axis=0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # --------- Anchoring ---------\n",
        "\n",
        "    def _alsfrs_anchor_days(self, alsfrs_df: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Compute per-subject anchor day = first ALSFRS visit (min delta/days).\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        return anchor_map\n",
        "\n",
        "    # --------- Data I/O ---------\n",
        "\n",
        "    def load_and_inspect_data(self, file_path: str = \"\") -> Dict[str, pd.DataFrame]:\n",
        "        datasets: Dict[str, pd.DataFrame] = {}\n",
        "        file_list = [\n",
        "            \"PROACT_ALSFRS.csv\",\n",
        "            \"PROACT_FVC.csv\",\n",
        "            \"PROACT_VITALSIGNS.csv\",\n",
        "            \"PROACT_RILUZOLE.csv\",\n",
        "            \"PROACT_DEMOGRAPHICS.csv\",\n",
        "            \"PROACT_LABS.csv\",\n",
        "            \"PROACT_DEATHDATA.csv\",\n",
        "            \"PROACT_HANDGRIPSTRENGTH.csv\",\n",
        "            \"PROACT_MUSCLESTRENGTH.csv\",\n",
        "            \"PROACT_ALSHISTORY.csv\",\n",
        "        ]\n",
        "        print(\"--- Loading and Inspecting Data ---\")\n",
        "        for file_name in file_list:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path + file_name, on_bad_lines=\"skip\")\n",
        "                # normalize subject_id\n",
        "                if \"subject_id\" not in df.columns:\n",
        "                    potential = [c for c in df.columns if \"subject\" in c.lower()]\n",
        "                    if potential:\n",
        "                        df = df.rename(columns={potential[0]: \"subject_id\"})\n",
        "                # coerce delta-like numeric columns\n",
        "                for c in df.columns:\n",
        "                    if \"delta\" in c.lower() or \"day\" in c.lower():\n",
        "                        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "                datasets[file_name] = df\n",
        "                print(f\"✓ {file_name}: {df.shape}\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"✗ {file_name}: File not found (skipped).\")\n",
        "        return datasets\n",
        "\n",
        "    # --------- Outcome ---------\n",
        "\n",
        "    def calculate_alsfrs_slope(self, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Outcome = slope between FIRST-after-3mo and FIRST-after-12mo ALSFRS totals,\n",
        "        with time anchored to first ALSFRS visit.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "        if \"ALSFRS_Total\" not in df.columns:\n",
        "            raise ValueError(\"ALSFRS_Total missing in ALSFRS table.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        # Anchor\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        df[\"days_from_anchor\"] = df[\"alsfrs_delta\"] - df[\"subject_id\"].map(anchor_map)\n",
        "        df[\"months\"] = df[\"days_from_anchor\"] / 30.44\n",
        "\n",
        "        df = df.sort_values([\"subject_id\", \"months\"])\n",
        "        slopes = {}\n",
        "\n",
        "        for sid, g in df.groupby(\"subject_id\", sort=False):\n",
        "            g = g.dropna(subset=[\"months\", \"ALSFRS_Total\"])\n",
        "            t1 = g[g[\"months\"] > 3.0].head(1)\n",
        "            t2 = g[g[\"months\"] > 12.0].head(1)\n",
        "            if not t1.empty and not t2.empty:\n",
        "                t1m = float(t1[\"months\"].iloc[0])\n",
        "                t2m = float(t2[\"months\"].iloc[0])\n",
        "                t1v = float(t1[\"ALSFRS_Total\"].iloc[0])\n",
        "                t2v = float(t2[\"ALSFRS_Total\"].iloc[0])\n",
        "                if t2m > t1m:\n",
        "                    slopes[sid] = (t2v - t1v) / (t2m - t1m)\n",
        "\n",
        "        return pd.DataFrame({\"subject_id\": list(slopes.keys()), \"alsfrs_slope\": list(slopes.values())})\n",
        "\n",
        "    # --------- FVC collapse ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def _fvc_collapse_trials(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Reduce FVC per row/time to the max across trials before summarization.\n",
        "        Tries to detect typical trial columns; falls back gracefully.\n",
        "        \"\"\"\n",
        "        d = df.copy()\n",
        "        # Find obvious trial columns\n",
        "        trial_cols = [c for c in d.columns if \"trial\" in c.lower()]\n",
        "        # Some datasets have explicit liters columns per trial name\n",
        "        if trial_cols:\n",
        "            d[\"FVC_Liters\"] = pd.to_numeric(d[trial_cols].max(axis=1), errors=\"coerce\")\n",
        "            keep = [\"subject_id\", time_col, \"FVC_Liters\"]\n",
        "            return d[keep]\n",
        "        # Fallbacks: look for liters column names\n",
        "        liter_like = [c for c in d.columns if \"liter\" in c.lower() or \"fvc\" in c.lower()]\n",
        "        if liter_like:\n",
        "            # If multiple, take row-wise max\n",
        "            d[\"FVC_Liters\"] = pd.to_numeric(d[liter_like].max(axis=1), errors=\"coerce\")\n",
        "            keep = [\"subject_id\", time_col, \"FVC_Liters\"]\n",
        "            return d[keep]\n",
        "        # Last resort: return as-is\n",
        "        return d\n",
        "\n",
        "    # --------- Longitudinal summarization ---------\n",
        "\n",
        "    def create_longitudinal_features(self, df: pd.DataFrame, time_col: str, prefix: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create 7 summaries over [0, 90] days from ALSFRS anchor:\n",
        "          min, max, median, std, first, last, slope(first→last)\n",
        "        Slope remains NaN if only one observation or zero time span.\n",
        "        \"\"\"\n",
        "        if time_col not in df.columns:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        d = df.copy()\n",
        "        # Coerce numerics (but keep subject_id/time cols)\n",
        "        for c in d.columns:\n",
        "            if c not in {\"subject_id\", time_col}:\n",
        "                d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
        "\n",
        "        # Ensure window is 0..90 days from ALSFRS anchor (already anchored)\n",
        "        d = d[(d[time_col] >= 0) & (d[time_col] <= 90)].copy()\n",
        "        if d.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Value columns (exclude identifiers/derived delta/time)\n",
        "        val_cols = [\n",
        "            c\n",
        "            for c in d.select_dtypes(include=[np.number]).columns\n",
        "            if c not in self.id_and_delta_cols and c not in {\"subject_id\", time_col}\n",
        "        ]\n",
        "        if not val_cols:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        out = []\n",
        "        g = d.groupby(\"subject_id\", as_index=True)\n",
        "        for col in val_cols:\n",
        "            agg = g[col].agg([\"min\", \"max\", \"median\", \"first\", \"last\"])\n",
        "            std_ = g[col].std(ddof=0).rename(\"std\")\n",
        "            slope = g.apply(\n",
        "                lambda x: (x[col].iloc[-1] - x[col].iloc[0]) / max(1e-9, (x[time_col].iloc[-1] - x[time_col].iloc[0]))\n",
        "                if len(x) > 1 and (x[time_col].iloc[-1] - x[time_col].iloc[0]) > 0\n",
        "                else np.nan\n",
        "            ).rename(\"slope\")\n",
        "            feat = pd.concat([agg, std_, slope], axis=1)\n",
        "            feat.columns = [f\"{prefix}{col}_{cname}\" for cname in feat.columns]\n",
        "            out.append(feat)\n",
        "\n",
        "        return pd.concat(out, axis=1).reset_index()\n",
        "\n",
        "    # --------- Static table processing (no encoding here to avoid leakage) ---------\n",
        "\n",
        "    @staticmethod\n",
        "    def process_static_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        CV-safe: DO NOT encode here. Just keep one row per subject.\n",
        "        (Do categorical encoding in your modeling pipeline.)\n",
        "        \"\"\"\n",
        "        if \"subject_id\" not in df.columns:\n",
        "            return pd.DataFrame()\n",
        "        # Keep first non-duplicated row per subject_id\n",
        "        return df.drop_duplicates(subset=[\"subject_id\"]).copy()\n",
        "\n",
        "    # --------- Merge features ---------\n",
        "\n",
        "    def merge_all_features(self, datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "        if \"PROACT_DEMOGRAPHICS.csv\" not in datasets:\n",
        "            raise ValueError(\"Demographics file is missing.\")\n",
        "\n",
        "        # Build ALSFRS anchor map\n",
        "        alsfrs = datasets[\"PROACT_ALSFRS.csv\"]\n",
        "        anchor_map = self._alsfrs_anchor_days(alsfrs)\n",
        "\n",
        "        # Start with demographics (static)\n",
        "        final_df = self.process_static_data(datasets[\"PROACT_DEMOGRAPHICS.csv\"])\n",
        "\n",
        "        # Add static-ish other tables (keep CV-safe; no encodings)\n",
        "        for file in [\"PROACT_RILUZOLE.csv\", \"PROACT_ALSHISTORY.csv\"]:\n",
        "            if file in datasets:\n",
        "                static_df = self.process_static_data(datasets[file])\n",
        "                final_df = pd.merge(final_df, static_df, on=\"subject_id\", how=\"left\")\n",
        "\n",
        "        # Longitudinal configs\n",
        "        longitudinal = {\n",
        "            \"PROACT_ALSFRS.csv\": \"alsfrs_\",\n",
        "            \"PROACT_FVC.csv\": \"fvc_\",\n",
        "            \"PROACT_VITALSIGNS.csv\": \"vitals_\",\n",
        "            \"PROACT_LABS.csv\": \"labs_\",\n",
        "            \"PROACT_HANDGRIPSTRENGTH.csv\": \"grip_\",\n",
        "            \"PROACT_MUSCLESTRENGTH.csv\": \"muscle_\",\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Generating Longitudinal Features (anchored to first ALSFRS; window = 0–90 days) ---\")\n",
        "        for file, prefix in longitudinal.items():\n",
        "            if file not in datasets:\n",
        "                continue\n",
        "\n",
        "            df = datasets[file].copy()\n",
        "            tcol = self._find_time_col(df)\n",
        "            if tcol is None:\n",
        "                print(f\"Warning: No time delta/days column in {file}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Anchor this table to ALSFRS first visit\n",
        "            df[\"anchor_days\"] = df[\"subject_id\"].map(anchor_map)\n",
        "            df = df[~df[\"anchor_days\"].isna()].copy()\n",
        "            df[\"days_from_alsfrs_anchor\"] = pd.to_numeric(df[tcol], errors=\"coerce\") - df[\"anchor_days\"]\n",
        "\n",
        "            # FVC special handling: collapse to max-of-trials BEFORE summarization\n",
        "            if file == \"PROACT_FVC.csv\":\n",
        "                df = self._fvc_collapse_trials(df, time_col=\"days_from_alsfrs_anchor\")\n",
        "\n",
        "            # Attempt to pivot long-form measurement tables (best effort)\n",
        "            if file in {\"PROACT_LABS.csv\", \"PROACT_MUSCLESTRENGTH.csv\", \"PROACT_HANDGRIPSTRENGTH.csv\"}:\n",
        "                try:\n",
        "                    test_cols = [\n",
        "                        c\n",
        "                        for c in df.columns\n",
        "                        if c not in {\"subject_id\", \"days_from_alsfrs_anchor\", \"anchor_days\"}\n",
        "                        and any(k in c.lower() for k in [\"test\", \"exam\", \"muscle\", \"site\", \"name\", \"strength_test\"])\n",
        "                    ]\n",
        "                    value_cols = [\n",
        "                        c\n",
        "                        for c in df.columns\n",
        "                        if c not in {\"subject_id\", \"days_from_alsfrs_anchor\", \"anchor_days\"}\n",
        "                        and any(k in c.lower() for k in [\"result\", \"value\", \"strength\", \"score\"])\n",
        "                    ]\n",
        "                    if test_cols and value_cols:\n",
        "                        tcol_name = test_cols[0]\n",
        "                        vcol_name = value_cols[0]\n",
        "                        df[vcol_name] = pd.to_numeric(df[vcol_name], errors=\"coerce\")\n",
        "                        df = (\n",
        "                            df.pivot_table(\n",
        "                                index=[\"subject_id\", \"days_from_alsfrs_anchor\"],\n",
        "                                columns=tcol_name,\n",
        "                                values=vcol_name,\n",
        "                                aggfunc=\"mean\",\n",
        "                            )\n",
        "                            .reset_index()\n",
        "                        )\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Pivoting failed for {file}: {e}\")\n",
        "\n",
        "            feats = self.create_longitudinal_features(df, \"days_from_alsfrs_anchor\", prefix)\n",
        "            if not feats.empty:\n",
        "                final_df = pd.merge(final_df, feats, on=\"subject_id\", how=\"left\")\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    # --------- Eligibility ---------\n",
        "\n",
        "    def filter_eligible_patients(self, feature_df: pd.DataFrame, alsfrs_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Keep subjects who have ANY ALSFRS >3 months AND >12 months AFTER the ALSFRS anchor.\n",
        "        \"\"\"\n",
        "        df = alsfrs_df.copy()\n",
        "        tcol = self._find_time_col(df)\n",
        "        if tcol is None:\n",
        "            raise ValueError(\"ALSFRS table lacks a time delta/days column.\")\n",
        "\n",
        "        df.rename(columns={tcol: \"alsfrs_delta\"}, inplace=True)\n",
        "        anchor_map = df.groupby(\"subject_id\")[\"alsfrs_delta\"].min()\n",
        "        df[\"days_from_anchor\"] = df[\"alsfrs_delta\"] - df[\"subject_id\"].map(anchor_map)\n",
        "        df[\"months\"] = df[\"days_from_anchor\"] / 30.44\n",
        "\n",
        "        g = df.groupby(\"subject_id\")[\"months\"]\n",
        "        has_t1 = g.apply(lambda s: (s > 3.0).any())\n",
        "        has_t2 = g.apply(lambda s: (s > 12.0).any())\n",
        "        eligible_ids = has_t1[has_t1].index.intersection(has_t2[has_t2].index)\n",
        "\n",
        "        print(f\"\\nEligible patients: {len(eligible_ids)} / {df['subject_id'].nunique()}\")\n",
        "        return feature_df[feature_df[\"subject_id\"].isin(eligible_ids)].copy()\n",
        "\n",
        "    # --------- Orchestration ---------\n",
        "\n",
        "    def run_pipeline(self, file_path: str = \"\") -> Optional[Dict[str, pd.DataFrame]]:\n",
        "        \"\"\"\n",
        "        End-to-end EDA (CV-safe) that writes 'final_processed_als_data.csv'.\n",
        "        No imputation/scaling/feature selection here — do that inside your CV pipeline.\n",
        "        \"\"\"\n",
        "        print(\"====== Starting ALS Data Preprocessing Pipeline ======\")\n",
        "        datasets = self.load_and_inspect_data(file_path)\n",
        "        if \"PROACT_ALSFRS.csv\" not in datasets:\n",
        "            print(\"CRITICAL ERROR: PROACT_ALSFRS.csv not found. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        # ALSFRS prep + anchor\n",
        "        datasets[\"PROACT_ALSFRS.csv\"] = self._convert_alsfrs_r(datasets[\"PROACT_ALSFRS.csv\"])\n",
        "\n",
        "        # Outcome\n",
        "        target_df = self.calculate_alsfrs_slope(datasets[\"PROACT_ALSFRS.csv\"])\n",
        "        print(f\"\\nCalculated ALSFRS slope for {len(target_df)} patients.\")\n",
        "\n",
        "        # Features\n",
        "        full_features = self.merge_all_features(datasets)\n",
        "\n",
        "        # Eligibility\n",
        "        eligible_features = self.filter_eligible_patients(full_features, datasets[\"PROACT_ALSFRS.csv\"])\n",
        "\n",
        "        # Join features + target\n",
        "        final_df = pd.merge(eligible_features, target_df, on=\"subject_id\", how=\"inner\")\n",
        "\n",
        "        # Drop features with >30% missing\n",
        "        print(\"\\n--- Handling Missing Values (Dropping cols with >30% missing) ---\")\n",
        "        initial_cols = len(final_df.columns)\n",
        "        missing_thresh = 0.30\n",
        "        min_non_na = int(np.ceil(len(final_df) * (1 - missing_thresh)))\n",
        "        final_df = final_df.dropna(axis=1, thresh=min_non_na)\n",
        "        dropped = initial_cols - len(final_df.columns)\n",
        "        print(f\"Dropped {dropped} columns for >{int(missing_thresh*100)}% missingness.\")\n",
        "\n",
        "        # Separate X/y (no transforms here to avoid leakage)\n",
        "        if \"alsfrs_slope\" not in final_df.columns:\n",
        "            print(\"No target available after merges. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        y = final_df[\"alsfrs_slope\"]\n",
        "        valid = y.notna()\n",
        "        final_df = final_df.loc[valid].reset_index(drop=True)\n",
        "\n",
        "        subject_ids = final_df[\"subject_id\"]\n",
        "        y = final_df[\"alsfrs_slope\"]\n",
        "        X = final_df.drop(columns=[\"subject_id\", \"alsfrs_slope\"])\n",
        "\n",
        "        # Save CV-safe engineered dataset (raw features)\n",
        "        out = pd.concat([subject_ids, y, X], axis=1)\n",
        "        out.to_csv(\"final_processed_als_data.csv\", index=False)\n",
        "        print(\"\\n✅ Saved CV-safe engineered data to 'final_processed_als_data.csv'\")\n",
        "        print(f\"Feature matrix shape: {X.shape} | Target length: {len(y)}\")\n",
        "\n",
        "        return {\"X\": X, \"y\": y, \"subject_ids\": subject_ids, \"raw_frame\": out}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # If your CSVs live elsewhere, set file_path accordingly (e.g., \"C:/data/PROACT/\")\n",
        "    file_path = \"\"\n",
        "    processor = ALSDataProcessor()\n",
        "    processed = processor.run_pipeline(file_path=file_path)\n",
        "    if processed is not None:\n",
        "        print(\"\\nPreview of columns:\", list(processed[\"X\"].columns)[:10])\n",
        "        print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fbZFYZYn5f"
      },
      "source": [
        "# 3rd cell\n",
        "trains two quick, reliable baselines on `final_processed_als_data.csv`. It does a simple 80/20 split, figures out which columns are numbers or categories, fills missing values, scales only for SVR, and one-hot encodes any categories. It then tries many parameter settings but stops weak ones early, using a cache so repeated steps don’t run again, and tunes both a Random Forest (no scaling needed) and an RBF SVR (with scaling). After tuning, it tests on the hold-out set and prints error and correlation with 95% confidence ranges, plus the best settings it found. It also prints a quick 50/50 blend of RF and SVR as a sanity check. If you just want speed, you can skip the SVR block—Random Forest alone is often strong here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6EtbWc-T3HO",
        "outputId": "83e2a21d-6364-4f01-9786-a13435202740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== FAST Classical Baselines (successive halving, cached) ======\n",
            "✓ Loaded engineered dataset: (1897, 348)\n",
            "Split: train=1517, test=380\n",
            "Detected numeric=343, categorical=3\n",
            "\n",
            "--- Fitting RandomForest (HalvingGridSearchCV, cv=3) ---\n",
            "RF best params: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__n_estimators': 250, 'select__k': 50}\n",
            "\n",
            "--- Fitting SVR (HalvingGridSearchCV, cv=3) ---\n",
            "SVR best params: {'model__C': 1.0, 'model__epsilon': 0.1, 'model__gamma': 'scale', 'select__k': 'all'}\n",
            "\n",
            "====== Test Set Performance (FAST mode) ======\n",
            "                 RMSE  RMSE 95% CI Low  RMSE 95% CI High     PCC  \\\n",
            "Model                                                              \n",
            "Random Forest  0.5905           0.5467            0.6386  0.1918   \n",
            "SVR (RBF)      0.5907           0.5405            0.6404  0.2131   \n",
            "\n",
            "               PCC 95% CI Low  PCC 95% CI High  \n",
            "Model                                           \n",
            "Random Forest          0.0909           0.2972  \n",
            "SVR (RBF)              0.1179           0.3153  \n",
            "\n",
            "--- Simple RF+SVR Avg Ensemble (FAST) ---\n",
            "                   RMSE  RMSE 95% CI Low  RMSE 95% CI High     PCC  \\\n",
            "RF+SVR Ensemble  0.5825           0.5355            0.6296  0.2248   \n",
            "\n",
            "                 PCC 95% CI Low  PCC 95% CI High  \n",
            "RF+SVR Ensemble          0.1234            0.322  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Halving search (successive halving)\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "\n",
        "# caching\n",
        "from joblib import Memory\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- Metrics ----------\n",
        "def rmse(y_true, y_pred) -> float:\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def safe_pcc(y_true, y_pred) -> float:\n",
        "    yt = np.asarray(y_true, dtype=float).ravel()\n",
        "    yp = np.asarray(y_pred, dtype=float).ravel()\n",
        "    if yt.std() < 1e-12 or yp.std() < 1e-12:\n",
        "        return 0.0\n",
        "    return float(np.corrcoef(yt, yp)[0, 1])\n",
        "\n",
        "def bootstrap_ci(y_true, y_pred, metric_fn, n_boot=800, alpha=0.95, seed=42) -> Tuple[float, float]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    y_true = np.asarray(y_true).ravel()\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    n = len(y_true)\n",
        "    stats = []\n",
        "    idx = np.arange(n)\n",
        "    for _ in range(n_boot):\n",
        "        b = rng.choice(idx, size=n, replace=True)\n",
        "        stats.append(metric_fn(y_true[b], y_pred[b]))\n",
        "    lo = float(np.percentile(stats, (1 - alpha) / 2 * 100))\n",
        "    hi = float(np.percentile(stats, (1 + alpha) / 2 * 100))\n",
        "    return lo, hi\n",
        "\n",
        "# ---------- Main ----------\n",
        "def run_classical_pipeline_fast() -> pd.DataFrame:\n",
        "    print(\"====== FAST Classical Baselines (successive halving, cached) ======\")\n",
        "\n",
        "    # 1) Load engineered data\n",
        "    df = pd.read_csv(\"final_processed_als_data.csv\")\n",
        "    print(f\"✓ Loaded engineered dataset: {df.shape}\")\n",
        "\n",
        "    X = df.drop(columns=[\"subject_id\", \"alsfrs_slope\"])\n",
        "    y = df[\"alsfrs_slope\"].astype(float)\n",
        "\n",
        "    # Optional quick shuffle for better fold homogeneity\n",
        "    X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "    # 80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42\n",
        "    )\n",
        "    print(f\"Split: train={X_train.shape[0]}, test={X_test.shape[0]}\")\n",
        "\n",
        "    # 2) Column typing\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    print(f\"Detected numeric={len(num_cols)}, categorical={len(cat_cols)}\")\n",
        "\n",
        "    # Pipeline cache\n",
        "    memory = Memory(location=\"sk_cache\", verbose=0)\n",
        "\n",
        "    # Preprocessors\n",
        "    # Numeric: impute → (optional scaler in SVR branch)\n",
        "    num_rf = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ], memory=memory)\n",
        "\n",
        "    num_svr = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ], memory=memory)\n",
        "\n",
        "    if len(cat_cols) > 0:\n",
        "        cat_common = Pipeline(steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ], memory=memory)\n",
        "        preproc_rf = ColumnTransformer(\n",
        "            transformers=[(\"num\", num_rf, num_cols), (\"cat\", cat_common, cat_cols)],\n",
        "            remainder=\"drop\"\n",
        "        )\n",
        "        preproc_svr = ColumnTransformer(\n",
        "            transformers=[(\"num\", num_svr, num_cols), (\"cat\", cat_common, cat_cols)],\n",
        "            remainder=\"drop\"\n",
        "        )\n",
        "    else:\n",
        "        # No categoricals → simpler (faster) preprocessors\n",
        "        preproc_rf = num_rf\n",
        "        preproc_svr = num_svr\n",
        "\n",
        "    # 3) Pipelines (with small but effective grids)\n",
        "    rf_pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preproc_rf),\n",
        "        (\"select\", SelectKBest(score_func=f_regression, k=\"all\")),\n",
        "        (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "    ], memory=memory)\n",
        "\n",
        "    rf_grid: Dict[str, list] = {\n",
        "        # keep imputer fixed (median) to avoid recomputing transforms\n",
        "        \"select__k\": [\"all\", 50],              # feature count toggle\n",
        "        \"model__n_estimators\": [250],          # enough trees, faster than 500\n",
        "        \"model__max_depth\": [None, 12],\n",
        "        \"model__min_samples_leaf\": [1, 2],\n",
        "        \"model__max_features\": [\"sqrt\"],       # stable setting\n",
        "    }\n",
        "\n",
        "    # Successive halving (aggressive elimination reduces fits)\n",
        "    rf_search = HalvingGridSearchCV(\n",
        "        rf_pipe,\n",
        "        rf_grid,\n",
        "        factor=3,\n",
        "        resource=\"n_samples\",\n",
        "        min_resources=\"exhaust\",\n",
        "        cv=3,\n",
        "        scoring=\"neg_root_mean_squared_error\",   # optimize RMSE directly\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        refit=True\n",
        "    )\n",
        "    print(\"\\n--- Fitting RandomForest (HalvingGridSearchCV, cv=3) ---\")\n",
        "    rf_search.fit(X_train, y_train)\n",
        "    print(f\"RF best params: {rf_search.best_params_}\")\n",
        "\n",
        "    # SVR (trimmed grid; if you need even faster, comment this whole block)\n",
        "    svr_pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preproc_svr),\n",
        "        (\"select\", SelectKBest(score_func=f_regression, k=\"all\")),\n",
        "        (\"model\", SVR(kernel=\"rbf\"))\n",
        "    ], memory=memory)\n",
        "\n",
        "    svr_grid: Dict[str, list] = {\n",
        "        \"select__k\": [\"all\", 50],\n",
        "        \"model__C\": [1.0, 3.0],\n",
        "        \"model__epsilon\": [0.1],\n",
        "        \"model__gamma\": [\"scale\"],\n",
        "    }\n",
        "\n",
        "    svr_search = HalvingGridSearchCV(\n",
        "        svr_pipe,\n",
        "        svr_grid,\n",
        "        factor=3,\n",
        "        resource=\"n_samples\",\n",
        "        min_resources=\"exhaust\",\n",
        "        cv=3,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        refit=True\n",
        "    )\n",
        "    print(\"\\n--- Fitting SVR (HalvingGridSearchCV, cv=3) ---\")\n",
        "    svr_search.fit(X_train, y_train)\n",
        "    print(f\"SVR best params: {svr_search.best_params_}\")\n",
        "\n",
        "    # 4) Test-set evaluation + (faster) bootstrap CIs\n",
        "    results = []\n",
        "\n",
        "    for name, est in [(\"Random Forest\", rf_search), (\"SVR (RBF)\", svr_search)]:\n",
        "        y_pred = est.best_estimator_.predict(X_test)\n",
        "        test_rmse = rmse(y_test, y_pred)\n",
        "        test_pcc  = safe_pcc(y_test.values, y_pred)\n",
        "\n",
        "        rmse_lo, rmse_hi = bootstrap_ci(y_test.values, y_pred, rmse, n_boot=800, alpha=0.95, seed=123)\n",
        "        pcc_lo,  pcc_hi  = bootstrap_ci(y_test.values, y_pred, safe_pcc, n_boot=800, alpha=0.95, seed=456)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": name,\n",
        "            \"RMSE\": test_rmse,\n",
        "            \"RMSE 95% CI Low\": rmse_lo,\n",
        "            \"RMSE 95% CI High\": rmse_hi,\n",
        "            \"PCC\": test_pcc,\n",
        "            \"PCC 95% CI Low\": pcc_lo,\n",
        "            \"PCC 95% CI High\": pcc_hi,\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results).set_index(\"Model\")\n",
        "    print(\"\\n====== Test Set Performance (FAST mode) ======\")\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    # Optional quick 50–50 blend (no extra CV)\n",
        "    rf_pred = rf_search.best_estimator_.predict(X_test)\n",
        "    svr_pred = svr_search.best_estimator_.predict(X_test)\n",
        "    ens_pred = 0.5 * (rf_pred + svr_pred)\n",
        "\n",
        "    ens_rmse = rmse(y_test, ens_pred)\n",
        "    ens_pcc  = safe_pcc(y_test.values, ens_pred)\n",
        "    ens_rmse_ci = bootstrap_ci(y_test.values, ens_pred, rmse, n_boot=800, alpha=0.95, seed=789)\n",
        "    ens_pcc_ci  = bootstrap_ci(y_test.values, ens_pred, safe_pcc, n_boot=800, alpha=0.95, seed=101112)\n",
        "\n",
        "    print(\"\\n--- Simple RF+SVR Avg Ensemble (FAST) ---\")\n",
        "    print(pd.DataFrame({\n",
        "        \"RMSE\": [ens_rmse],\n",
        "        \"RMSE 95% CI Low\": [ens_rmse_ci[0]],\n",
        "        \"RMSE 95% CI High\": [ens_rmse_ci[1]],\n",
        "        \"PCC\": [ens_pcc],\n",
        "        \"PCC 95% CI Low\": [ens_pcc_ci[0]],\n",
        "        \"PCC 95% CI High\": [ens_pcc_ci[1]],\n",
        "    }, index=[\"RF+SVR Ensemble\"]).round(4))\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_classical_pipeline_fast()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zRkw69vZeq4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pure QNN"
      ],
      "metadata": {
        "id": "9CCgnvc_bjjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pure_qnn_strong_v2.py  — same pure QNN, but with runtime guards + more frequent logging\n",
        "import os, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# ---------- utils ----------\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed); pnp.random.seed(seed)\n",
        "\n",
        "def rmse(y_true, y_pred): return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def safe_pcc(a, b):\n",
        "    a = np.asarray(a).ravel(); b = np.asarray(b).ravel()\n",
        "    if a.std()==0 or b.std()==0: return 0.0\n",
        "    v = pearsonr(a, b)[0]\n",
        "    return float(v) if np.isfinite(v) else 0.0\n",
        "\n",
        "def load_als(path=\"final_processed_als_data.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    X = df.drop(columns=[\"subject_id\", \"alsfrs_slope\"], errors=\"ignore\")\n",
        "    y = df[\"alsfrs_slope\"].astype(float).values\n",
        "    m = ~np.isnan(y)\n",
        "    X, y = X.loc[m].reset_index(drop=True), y[m]\n",
        "    print(f\"✓ Loaded: X={X.shape}, y={y.shape}\")\n",
        "    print(f\"  Target: mean={y.mean():.3f}, std={y.std():.3f}, range=[{y.min():.3f},{y.max():.3f}]\")\n",
        "    return X, y\n",
        "\n",
        "def sanitize_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for c in out.columns:\n",
        "        if out[c].dtype == object:\n",
        "            try: out[c] = pd.to_numeric(out[c])\n",
        "            except Exception: out[c] = pd.factorize(out[c].astype(str))[0]\n",
        "    return out\n",
        "\n",
        "# ---------- feature selection ----------\n",
        "def improved_feature_selection(X_df, y, k=12, seed=42):\n",
        "    imp = SimpleImputer(strategy=\"median\")\n",
        "    Xn = imp.fit_transform(X_df)\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=400, max_features=\"sqrt\", min_samples_leaf=3,\n",
        "        random_state=seed, n_jobs=1\n",
        "    ).fit(Xn, y)\n",
        "    rf_scores = rf.feature_importances_\n",
        "\n",
        "    pearson_scores = np.array([\n",
        "        abs(np.corrcoef(Xn[:, i], y)[0,1]) if Xn[:, i].std()>0 else 0.0\n",
        "        for i in range(Xn.shape[1])\n",
        "    ])\n",
        "    spearman_scores = np.array([\n",
        "        abs(spearmanr(Xn[:, i], y)[0]) if Xn[:, i].std()>0 else 0.0\n",
        "        for i in range(Xn.shape[1])\n",
        "    ])\n",
        "    spearman_scores = np.nan_to_num(spearman_scores)\n",
        "\n",
        "    def norm(v):\n",
        "        vmin, vmax = v.min(), v.max()\n",
        "        return (v - vmin)/(vmax - vmin + 1e-12) if vmax>vmin else np.zeros_like(v)\n",
        "\n",
        "    fin = 0.45*norm(rf_scores) + 0.35*norm(pearson_scores) + 0.20*norm(spearman_scores)\n",
        "    idx = np.argsort(fin)[::-1][:k]\n",
        "    cols = [X_df.columns[i] for i in idx]\n",
        "    print(f\"✓ Selected top-{k}: {cols}\")\n",
        "    return idx.tolist(), cols\n",
        "\n",
        "# ---------- angles (PLS→[-pi/3,pi/3]) ----------\n",
        "def make_angles_pls(X_train, X_val, X_test, y_train, n_qubits):\n",
        "    imp = SimpleImputer(strategy=\"median\")\n",
        "    rb  = RobustScaler(quantile_range=(10,90))\n",
        "    pls = PLSRegression(n_components=n_qubits, scale=False)\n",
        "\n",
        "    Xtr_s = rb.fit_transform(imp.fit_transform(X_train)).astype(np.float64)\n",
        "    Xva_s = rb.transform(imp.transform(X_val)).astype(np.float64)\n",
        "    Xte_s = rb.transform(imp.transform(X_test)).astype(np.float64)\n",
        "\n",
        "    pls.fit(Xtr_s, y_train.reshape(-1,1))\n",
        "    Xtr_pls = pls.transform(Xtr_s).astype(np.float64)\n",
        "    Xva_pls = pls.transform(Xva_s).astype(np.float64)\n",
        "    Xte_pls = pls.transform(Xte_s).astype(np.float64)\n",
        "\n",
        "    mm = MinMaxScaler(feature_range=(-np.pi/3, np.pi/3))\n",
        "    Xtr_q = mm.fit_transform(Xtr_pls).astype(np.float64)\n",
        "    Xva_q = mm.transform(Xva_pls).astype(np.float64)\n",
        "    Xte_q = mm.transform(Xte_pls).astype(np.float64)\n",
        "    return Xtr_q, Xva_q, Xte_q\n",
        "\n",
        "# ---------- pure QNN with multi-observable readout ----------\n",
        "def build_qnode(n_qubits=6, n_layers=8, device_name=\"lightning.qubit\", use_xx=True):\n",
        "    try:\n",
        "        dev = qml.device(device_name, wires=n_qubits)\n",
        "    except Exception:\n",
        "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "    obs_Z  = [qml.PauliZ(i) for i in range(n_qubits)]\n",
        "    obs_ZZ = [qml.PauliZ(i) @ qml.PauliZ((i+1)%n_qubits) for i in range(n_qubits)]\n",
        "    obs_XX = [qml.PauliX(i) @ qml.PauliX((i+1)%n_qubits) for i in range(n_qubits)] if use_xx else []\n",
        "    observables = obs_Z + obs_ZZ + obs_XX\n",
        "    n_obs = len(observables)\n",
        "\n",
        "    def entangle_ring():\n",
        "        for i in range(n_qubits-1): qml.CNOT(wires=[i, i+1])\n",
        "        qml.CNOT(wires=[n_qubits-1, 0])\n",
        "\n",
        "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"adjoint\")\n",
        "    def circuit(x, W, alpha, beta, s, layer_mask):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(alpha[i]*x[i] + beta[i], wires=i)\n",
        "        for l in range(n_layers):\n",
        "            m = layer_mask[l]\n",
        "            for i in range(n_qubits):\n",
        "                qml.RX(m*W[l,i,0], wires=i)\n",
        "                qml.RY(m*W[l,i,1], wires=i)\n",
        "                qml.RZ(m*W[l,i,2], wires=i)\n",
        "            entangle_ring()\n",
        "            for i in range(n_qubits):\n",
        "                qml.RY(m*s*(alpha[i]*x[i] + beta[i]), wires=i)\n",
        "        return [qml.expval(o) for o in observables]\n",
        "\n",
        "    return circuit, n_obs\n",
        "\n",
        "# ---------- training ----------\n",
        "def train_pure_qnn(\n",
        "    X_raw, y,\n",
        "    topk=12,\n",
        "    n_qubits=6,\n",
        "    n_layers=8,\n",
        "    steps=140,\n",
        "    batch_size=48,\n",
        "    stepsize=0.012,\n",
        "    early_patience=18,\n",
        "    n_restarts=3,\n",
        "    bag_top=2,\n",
        "    seed=42,\n",
        "    use_pls=True,\n",
        "    eval_every=5,\n",
        "    max_batches_per_epoch=10,   # <-- cap work per epoch to avoid \"silent hours\"\n",
        "    use_xx=True\n",
        "):\n",
        "    set_seeds(seed)\n",
        "    X_tr_full, X_te_full, y_tr_full, y_te = train_test_split(X_raw, y, test_size=0.20, random_state=42)\n",
        "    print(f\"\\n✓ Split: train={len(y_tr_full)}, test={len(y_te)}\")\n",
        "\n",
        "    X_tr_raw, X_va_raw, y_tr, y_va = train_test_split(X_tr_full, y_tr_full, test_size=0.15, random_state=123)\n",
        "    print(f\"✓ Train/Val split: train={len(y_tr)}, val={len(y_va)}\")\n",
        "\n",
        "    feat_idx, feat_cols = improved_feature_selection(pd.DataFrame(X_tr_raw, columns=X_raw.columns), y_tr, k=topk, seed=seed)\n",
        "    def sel(df_like):\n",
        "        df_like = pd.DataFrame(df_like, columns=X_raw.columns)\n",
        "        return df_like.iloc[:, feat_idx].values\n",
        "    Xtr_sel, Xva_sel, Xte_sel = sel(X_tr_raw), sel(X_va_raw), sel(X_te_full)\n",
        "\n",
        "    print(\"\\n✓ Preparing quantum angles...\")\n",
        "    if use_pls:\n",
        "        Xtr_q, Xva_q, Xte_q = make_angles_pls(Xtr_sel, Xva_sel, Xte_sel, y_tr, n_qubits)\n",
        "        print(f\"  PLS→{n_qubits} angles from top-{topk} features.\")\n",
        "    else:\n",
        "        imp = SimpleImputer(strategy=\"median\"); rb=RobustScaler((10,90)); mm=MinMaxScaler((-np.pi/3, np.pi/3))\n",
        "        Xtr_q = mm.fit_transform(rb.fit_transform(imp.fit_transform(Xtr_sel))).astype(np.float64)\n",
        "        Xva_q = mm.transform(rb.transform(imp.transform(Xva_sel))).astype(np.float64)\n",
        "        Xte_q = mm.transform(rb.transform(imp.transform(Xte_sel))).astype(np.float64)\n",
        "\n",
        "    y_min, y_max = y_tr.min(), y_tr.max()\n",
        "    def y_to_norm(v): return 2.0*(v - y_min)/(y_max - y_min + 1e-12) - 1.0\n",
        "    def y_from_norm(vn): return 0.5*(vn + 1.0)*(y_max - y_min) + y_min\n",
        "    y_tr_n, y_va_n = y_to_norm(y_tr), y_to_norm(y_va)\n",
        "\n",
        "    circuit, n_obs = build_qnode(n_qubits=n_qubits, n_layers=n_layers, use_xx=use_xx)\n",
        "\n",
        "    Xtr_arr = pnp.array(Xtr_q, requires_grad=False)\n",
        "    Xva_arr = pnp.array(Xva_q, requires_grad=False)\n",
        "    Xte_arr = pnp.array(Xte_q, requires_grad=False)\n",
        "    ytr_arr = pnp.array(y_tr_n, requires_grad=False)\n",
        "    yva_arr = pnp.array(y_va_n, requires_grad=False)\n",
        "\n",
        "    def batch_iter(XA, yA, bs, max_batches=None):\n",
        "        n = len(yA); idx = np.arange(n); pnp.random.shuffle(idx)\n",
        "        count = 0\n",
        "        for i in range(0, n, bs):\n",
        "            if max_batches is not None and count >= max_batches: break\n",
        "            sel = idx[i:i+bs]\n",
        "            count += 1\n",
        "            yield XA[sel], yA[sel]\n",
        "\n",
        "    def layer_mask_schedule(ep, total):\n",
        "        t = ep/float(total)\n",
        "        base = 0.15 + 0.85*0.5*(1.0 - np.cos(np.pi*t))\n",
        "        m = np.linspace(0.25, 1.0, n_layers) * base\n",
        "        return pnp.array(np.clip(m,0.0,1.0), requires_grad=False)\n",
        "\n",
        "    restart_stats, restart_params = [], []\n",
        "    print(f\"\\n  Training Pure QNN with {n_restarts} restarts...\")\n",
        "    for r in range(n_restarts):\n",
        "        set_seeds(seed + 100*r)\n",
        "        W     = 0.05 * pnp.array(np.random.randn(n_layers, n_qubits, 3), requires_grad=True)\n",
        "        alpha = 1.00 * pnp.ones((n_qubits,), requires_grad=True)\n",
        "        beta  = pnp.zeros((n_qubits,), requires_grad=True)\n",
        "        s     = pnp.array(0.30, requires_grad=True)\n",
        "        rw    = 0.05 * pnp.array(np.random.randn(n_obs), requires_grad=True)\n",
        "        rb    = pnp.array(0.0, requires_grad=True)\n",
        "\n",
        "        opt = qml.AdamOptimizer(stepsize=stepsize)\n",
        "\n",
        "        def pred_batch(XB, W, alpha, beta, s, rw, rb, lmask):\n",
        "            preds = []\n",
        "            for x in XB:\n",
        "                o = pnp.stack(circuit(x, W, alpha, beta, s, lmask))\n",
        "                preds.append(pnp.dot(rw, o) + rb)\n",
        "            return pnp.stack(preds)\n",
        "\n",
        "        def batch_loss(W, alpha, beta, s, rw, rb, XB, yB, lmask, l2=8e-5):\n",
        "            pr = pred_batch(XB, W, alpha, beta, s, rw, rb, lmask)\n",
        "            return pnp.mean((pr - yB)**2) + l2*(pnp.sum(W**2) + pnp.sum(rw**2))\n",
        "\n",
        "        def val_mse(W, alpha, beta, s, rw, rb, lmask):\n",
        "            pv = pred_batch(Xva_arr, W, alpha, beta, s, rw, rb, lmask)\n",
        "            return pnp.mean((pv - yva_arr)**2)\n",
        "\n",
        "        best_val, bad, best_pack = 1e9, 0, None\n",
        "        t_start = time.time()\n",
        "        for ep in range(1, steps+1):\n",
        "            lmask = layer_mask_schedule(ep, steps)\n",
        "            for XB, yB in batch_iter(Xtr_arr, ytr_arr, batch_size, max_batches=max_batches_per_epoch):\n",
        "                W, alpha, beta, s, rw, rb = opt.step(\n",
        "                    lambda W_,a_,b_,s_,rw_,rb_: batch_loss(W_,a_,b_,s_,rw_,rb_, XB, yB, lmask),\n",
        "                    W, alpha, beta, s, rw, rb\n",
        "                )\n",
        "            # cosine LR\n",
        "            opt.stepsize = float(max(0.25, 0.5*(1+np.cos(ep*np.pi/max(1,steps)))) * stepsize)\n",
        "\n",
        "            if ep % eval_every == 0 or ep == 1:\n",
        "                mse_v = float(val_mse(W, alpha, beta, s, rw, rb, lmask))\n",
        "                elapsed = time.time() - t_start\n",
        "                print(f\"[restart {r+1}/{n_restarts} | ep {ep:03d}] val_MSE={mse_v:.5f}  lr={opt.stepsize:.4f}  t={elapsed/60:.1f}m\")\n",
        "                t_start = time.time()\n",
        "                if mse_v + 1e-9 < best_val:\n",
        "                    best_val, bad = mse_v, 0\n",
        "                    best_pack = (pnp.array(W), pnp.array(alpha), pnp.array(beta),\n",
        "                                 pnp.array(s), pnp.array(rw), pnp.array(rb))\n",
        "                else:\n",
        "                    bad += 1\n",
        "                    if bad >= early_patience:\n",
        "                        print(\"  early stop (patience)\\n\")\n",
        "                        break\n",
        "\n",
        "        restart_stats.append(best_val); restart_params.append(best_pack)\n",
        "\n",
        "    order = np.argsort(restart_stats); keep = order[:max(1, min(bag_top, len(order)))]\n",
        "    print(f\"Selected restarts for bagging: {list((keep+1).tolist())}\")\n",
        "\n",
        "    def y_from_norm(vn): return 0.5*(vn + 1.0)*(y_tr.max() - y_tr.min()) + y_tr.min()\n",
        "    preds_all = []\n",
        "    for idx in keep:\n",
        "        W, alpha, beta, s, rw, rb = restart_params[idx]\n",
        "        lmask = layer_mask_schedule(steps, steps)\n",
        "        pr_n = np.array(pred_batch(Xte_arr, W, alpha, beta, s, rw, rb, lmask))\n",
        "        preds_all.append(y_from_norm(pr_n))\n",
        "    y_te_pred = np.mean(preds_all, axis=0)\n",
        "\n",
        "    te_rmse = rmse(y_te, y_te_pred); te_pcc = safe_pcc(y_te, y_te_pred); te_r2 = r2_score(y_te, y_te_pred)\n",
        "    print(\"\\n===== FINAL TEST (Pure QNN) =====\")\n",
        "    print(f\"RMSE={te_rmse:.4f}  PCC={te_pcc:.4f}  R²={te_r2:.4f}\")\n",
        "    return dict(rmse=te_rmse, pcc=te_pcc, r2=te_r2)\n",
        "\n",
        "# ----- runner -----\n",
        "if __name__ == \"__main__\":\n",
        "    t0 = time.time()\n",
        "    X_raw, y = load_als(\"final_processed_als_data.csv\")\n",
        "    X_raw = sanitize_features(X_raw)\n",
        "    out = train_pure_qnn(\n",
        "        X_raw, y,\n",
        "        topk=12,\n",
        "        n_qubits=6,        # 6–7 works well on CPU\n",
        "        n_layers=8,        # slightly shallower = faster, still expressive\n",
        "        steps=140,\n",
        "        batch_size=48,\n",
        "        stepsize=0.012,\n",
        "        early_patience=18,\n",
        "        n_restarts=3,\n",
        "        bag_top=2,\n",
        "        seed=42,\n",
        "        use_pls=True,\n",
        "        eval_every=5,        # more frequent feedback so it never feels “stuck”\n",
        "        max_batches_per_epoch=10,  # cap work per epoch\n",
        "        use_xx=True\n",
        "    )\n",
        "    print(f\"\\n✓ Total time: {time.time()-t0:.1f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjVufQTWw2Np",
        "outputId": "4144897b-7803-40da-c638-947d5225b778"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded: X=(1897, 346), y=(1897,)\n",
            "  Target: mean=-0.667, std=0.572, range=[-3.628,1.208]\n",
            "\n",
            "✓ Split: train=1517, test=380\n",
            "✓ Train/Val split: train=1289, val=228\n",
            "✓ Selected top-12: ['alsfrs_ALSFRS_Total_std', 'fvc_FVC_Liters_slope', 'alsfrs_ALSFRS_Total_slope', 'alsfrs_Q1_Speech_min', 'alsfrs_Q1_Speech_last', 'fvc_FVC_Liters_std', 'alsfrs_Q1_Speech_median', 'alsfrs_Q3_Swallowing_min', 'alsfrs_Q1_Speech_max', 'alsfrs_Q3_Swallowing_std', 'vitals_Pulse_median', 'labs_Phosphorus_median']\n",
            "\n",
            "✓ Preparing quantum angles...\n",
            "  PLS→6 angles from top-12 features.\n",
            "\n",
            "  Training Pure QNN with 3 restarts...\n",
            "[restart 1/3 | ep 001] val_MSE=0.09790  lr=0.0120  t=1.2m\n",
            "[restart 1/3 | ep 005] val_MSE=0.09326  lr=0.0120  t=3.3m\n",
            "[restart 1/3 | ep 010] val_MSE=0.09031  lr=0.0118  t=4.1m\n",
            "[restart 1/3 | ep 015] val_MSE=0.08945  lr=0.0117  t=4.0m\n",
            "[restart 1/3 | ep 020] val_MSE=0.09065  lr=0.0114  t=4.1m\n",
            "[restart 1/3 | ep 025] val_MSE=0.09234  lr=0.0111  t=4.3m\n",
            "[restart 1/3 | ep 030] val_MSE=0.09150  lr=0.0107  t=4.1m\n",
            "[restart 1/3 | ep 035] val_MSE=0.08845  lr=0.0102  t=4.1m\n",
            "[restart 1/3 | ep 040] val_MSE=0.08889  lr=0.0097  t=4.2m\n",
            "[restart 1/3 | ep 045] val_MSE=0.08949  lr=0.0092  t=4.0m\n",
            "[restart 1/3 | ep 050] val_MSE=0.08756  lr=0.0086  t=4.0m\n",
            "[restart 1/3 | ep 055] val_MSE=0.08742  lr=0.0080  t=4.1m\n",
            "[restart 1/3 | ep 060] val_MSE=0.08843  lr=0.0073  t=4.1m\n",
            "[restart 1/3 | ep 065] val_MSE=0.08876  lr=0.0067  t=4.2m\n",
            "[restart 1/3 | ep 070] val_MSE=0.08851  lr=0.0060  t=4.1m\n",
            "[restart 1/3 | ep 075] val_MSE=0.08838  lr=0.0053  t=4.1m\n",
            "[restart 1/3 | ep 080] val_MSE=0.08864  lr=0.0047  t=4.1m\n",
            "[restart 1/3 | ep 085] val_MSE=0.08909  lr=0.0040  t=4.2m\n",
            "[restart 1/3 | ep 090] val_MSE=0.08938  lr=0.0034  t=4.1m\n",
            "[restart 1/3 | ep 095] val_MSE=0.08936  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 100] val_MSE=0.08961  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 105] val_MSE=0.08892  lr=0.0030  t=4.0m\n",
            "[restart 1/3 | ep 110] val_MSE=0.08788  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 115] val_MSE=0.08856  lr=0.0030  t=4.0m\n",
            "[restart 1/3 | ep 120] val_MSE=0.08814  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 125] val_MSE=0.08843  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 130] val_MSE=0.08828  lr=0.0030  t=4.2m\n",
            "[restart 1/3 | ep 135] val_MSE=0.08877  lr=0.0030  t=4.1m\n",
            "[restart 1/3 | ep 140] val_MSE=0.08835  lr=0.0030  t=4.0m\n",
            "[restart 2/3 | ep 001] val_MSE=0.09733  lr=0.0120  t=1.0m\n",
            "[restart 2/3 | ep 005] val_MSE=0.09459  lr=0.0120  t=3.3m\n",
            "[restart 2/3 | ep 010] val_MSE=0.09149  lr=0.0118  t=4.0m\n",
            "[restart 2/3 | ep 015] val_MSE=0.09095  lr=0.0117  t=4.2m\n",
            "[restart 2/3 | ep 020] val_MSE=0.10035  lr=0.0114  t=4.1m\n",
            "[restart 2/3 | ep 025] val_MSE=0.09358  lr=0.0111  t=4.4m\n",
            "[restart 2/3 | ep 030] val_MSE=0.09383  lr=0.0107  t=4.1m\n",
            "[restart 2/3 | ep 035] val_MSE=0.09093  lr=0.0102  t=4.1m\n",
            "[restart 2/3 | ep 040] val_MSE=0.09081  lr=0.0097  t=4.2m\n",
            "[restart 2/3 | ep 045] val_MSE=0.09444  lr=0.0092  t=4.2m\n",
            "[restart 2/3 | ep 050] val_MSE=0.09221  lr=0.0086  t=4.3m\n",
            "[restart 2/3 | ep 055] val_MSE=0.09203  lr=0.0080  t=4.2m\n",
            "[restart 2/3 | ep 060] val_MSE=0.09022  lr=0.0073  t=4.1m\n",
            "[restart 2/3 | ep 065] val_MSE=0.09076  lr=0.0067  t=4.2m\n",
            "[restart 2/3 | ep 070] val_MSE=0.09133  lr=0.0060  t=4.3m\n",
            "[restart 2/3 | ep 075] val_MSE=0.09065  lr=0.0053  t=4.0m\n",
            "[restart 2/3 | ep 080] val_MSE=0.09235  lr=0.0047  t=4.0m\n",
            "[restart 2/3 | ep 085] val_MSE=0.09133  lr=0.0040  t=4.2m\n",
            "[restart 2/3 | ep 090] val_MSE=0.09113  lr=0.0034  t=4.3m\n",
            "[restart 2/3 | ep 095] val_MSE=0.09072  lr=0.0030  t=4.1m\n",
            "[restart 2/3 | ep 100] val_MSE=0.09076  lr=0.0030  t=4.1m\n",
            "[restart 2/3 | ep 105] val_MSE=0.09101  lr=0.0030  t=4.0m\n",
            "[restart 2/3 | ep 110] val_MSE=0.09158  lr=0.0030  t=4.3m\n",
            "[restart 2/3 | ep 115] val_MSE=0.09159  lr=0.0030  t=4.2m\n",
            "[restart 2/3 | ep 120] val_MSE=0.09149  lr=0.0030  t=4.1m\n",
            "[restart 2/3 | ep 125] val_MSE=0.09090  lr=0.0030  t=4.3m\n",
            "[restart 2/3 | ep 130] val_MSE=0.09218  lr=0.0030  t=4.3m\n",
            "[restart 2/3 | ep 135] val_MSE=0.09133  lr=0.0030  t=4.0m\n",
            "[restart 2/3 | ep 140] val_MSE=0.09155  lr=0.0030  t=4.1m\n",
            "[restart 3/3 | ep 001] val_MSE=0.09428  lr=0.0120  t=0.9m\n",
            "[restart 3/3 | ep 005] val_MSE=0.09604  lr=0.0120  t=3.3m\n",
            "[restart 3/3 | ep 010] val_MSE=0.08892  lr=0.0118  t=4.1m\n",
            "[restart 3/3 | ep 015] val_MSE=0.09087  lr=0.0117  t=4.1m\n",
            "[restart 3/3 | ep 020] val_MSE=0.08945  lr=0.0114  t=4.1m\n",
            "[restart 3/3 | ep 025] val_MSE=0.09022  lr=0.0111  t=4.0m\n",
            "[restart 3/3 | ep 030] val_MSE=0.09001  lr=0.0107  t=4.0m\n",
            "[restart 3/3 | ep 035] val_MSE=0.08919  lr=0.0102  t=4.1m\n",
            "[restart 3/3 | ep 040] val_MSE=0.08908  lr=0.0097  t=4.1m\n",
            "[restart 3/3 | ep 045] val_MSE=0.09177  lr=0.0092  t=4.2m\n",
            "[restart 3/3 | ep 050] val_MSE=0.09078  lr=0.0086  t=4.1m\n",
            "[restart 3/3 | ep 055] val_MSE=0.08927  lr=0.0080  t=4.1m\n",
            "[restart 3/3 | ep 060] val_MSE=0.09282  lr=0.0073  t=4.1m\n",
            "[restart 3/3 | ep 065] val_MSE=0.09071  lr=0.0067  t=4.1m\n",
            "[restart 3/3 | ep 070] val_MSE=0.09033  lr=0.0060  t=4.1m\n",
            "[restart 3/3 | ep 075] val_MSE=0.09056  lr=0.0053  t=4.0m\n",
            "[restart 3/3 | ep 080] val_MSE=0.08939  lr=0.0047  t=4.0m\n",
            "[restart 3/3 | ep 085] val_MSE=0.09219  lr=0.0040  t=4.1m\n",
            "[restart 3/3 | ep 090] val_MSE=0.09098  lr=0.0034  t=4.0m\n",
            "[restart 3/3 | ep 095] val_MSE=0.09155  lr=0.0030  t=4.1m\n",
            "[restart 3/3 | ep 100] val_MSE=0.09159  lr=0.0030  t=4.0m\n",
            "  early stop (patience)\n",
            "\n",
            "Selected restarts for bagging: [1, 3]\n",
            "\n",
            "===== FINAL TEST (Pure QNN) =====\n",
            "RMSE=0.6137  PCC=0.1884  R²=0.0154\n",
            "\n",
            "✓ Total time: 18843.9s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSnHAM+Qlmpv0H8g9dBxjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}