{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/Blood_MedMNIST_QNN_AllInOne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Wd2qzHokdCwb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2qzHokdCwb",
        "outputId": "980df96a-6cca-402f-d41d-8518b743b5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing packages...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "\n",
        "ROOT = \"/content/hybridqnn_seq\"\n",
        "SRC = f\"{ROOT}/src\"\n",
        "os.makedirs(SRC, exist_ok=True)\n",
        "open(f\"{SRC}/__init__.py\", \"w\").write(\"\")\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "print(\"📦 Installing packages...\")\n",
        "!pip install -q torch torchvision pennylane medmnist scikit-learn tqdm numpy\n",
        "\n",
        "print(\"✅ Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fODYibQkkirc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODYibQkkirc",
        "outputId": "936477bf-3753-45b9-aeb2-2208a3130f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data loader created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/data.py ----------\n",
        "open(f\"{SRC}/data.py\", \"w\").write(r'''\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class MedMNISTDataset(Dataset):\n",
        "    def __init__(self, dataset_name: str, split: str = \"train\", transform=None):\n",
        "        super().__init__()\n",
        "        info = INFO[dataset_name.lower()]\n",
        "        DataClass = getattr(medmnist, info['python_class'])\n",
        "        self.dataset = DataClass(split=split, download=True, transform=transform)\n",
        "        self.n_classes = len(info['label'])\n",
        "        self.task = info['task']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img = torch.from_numpy(np.array(img)).float()\n",
        "        if len(img.shape) == 2:\n",
        "            img = img.unsqueeze(0)\n",
        "        elif len(img.shape) == 3 and img.shape[2] in [1, 3]:\n",
        "            img = img.permute(2, 0, 1)\n",
        "        img = img / 255.0\n",
        "\n",
        "        if isinstance(label, np.ndarray):\n",
        "            label = torch.from_numpy(label).long()\n",
        "        elif not isinstance(label, torch.Tensor):\n",
        "            label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        if label.dim() > 0:\n",
        "            label = label.squeeze()\n",
        "\n",
        "        return img, label\n",
        "''')\n",
        "\n",
        "print(\"✅ Data loader created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5X45s2iEyaT",
      "metadata": {
        "id": "b5X45s2iEyaT"
      },
      "source": [
        "4. model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "WNZNSQb1kk_f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNZNSQb1kk_f",
        "outputId": "35df8420-0779-4616-8468-8f125c92ce28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patch embedding created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/patches.py ----------\n",
        "open(f\"{SRC}/patches.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Lightweight patch embedding\"\"\"\n",
        "    def __init__(self, in_channels: int = 1, patch_size: int = 4, embed_dim: int = 48):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        # ✅ Smaller embedding for speed\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.BatchNorm2d(embed_dim)  # Faster than LayerNorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "''')\n",
        "\n",
        "print(\"✅ Patch embedding created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "C-L8-YNmTMLW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-L8-YNmTMLW",
        "outputId": "de2eae26-fc28-486d-80b7-15f96b659725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fast RNN router created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/rnn.py ----------\n",
        "open(f\"{SRC}/rnn.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FastRNNRouter(nn.Module):\n",
        "    \"\"\"Simplified RNN router for speed\"\"\"\n",
        "    def __init__(self, D: int, K: int):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.K = K\n",
        "\n",
        "        # ✅ Single-layer unidirectional GRU (faster than LSTM)\n",
        "        self.rnn = nn.GRU(D, D, num_layers=1, batch_first=True, dropout=0)\n",
        "\n",
        "        # ✅ Simple attention\n",
        "        self.attn_proj = nn.Linear(D, 1)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        B, N, D = patches.shape\n",
        "\n",
        "        # Fast RNN pass\n",
        "        rnn_out, _ = self.rnn(patches)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = self.attn_proj(rnn_out).squeeze(-1)\n",
        "        attn_weights = torch.softmax(scores, dim=1)\n",
        "\n",
        "        # Weighted sum (faster than top-k)\n",
        "        kvec = torch.bmm(attn_weights.unsqueeze(1), rnn_out).squeeze(1)\n",
        "\n",
        "        return kvec\n",
        "''')\n",
        "\n",
        "print(\"✅ Fast RNN router created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rqaksJjPC1dw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqaksJjPC1dw",
        "outputId": "7926e567-ae74-4d3e-b062-da8611364da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fast quantum layer created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/quantum.py ----------\n",
        "open(f\"{SRC}/quantum.py\", \"w\").write(r'''\n",
        "import pennylane as qml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class FastQuantumLayer(nn.Module):\n",
        "    \"\"\"Speed-optimized quantum layer\"\"\"\n",
        "    def __init__(self, input_dim: int, Q: int = 4, L: int = 2):\n",
        "        super().__init__()\n",
        "        self.Q = Q\n",
        "        self.L = L\n",
        "\n",
        "        # ✅ Smaller input projection\n",
        "        self.lin_in = nn.Linear(input_dim, Q)\n",
        "\n",
        "        # ✅ Fewer quantum weights\n",
        "        self.q_weights = nn.Parameter(torch.empty(L, Q, 3))\n",
        "        nn.init.uniform_(self.q_weights, -np.pi/2, np.pi/2)\n",
        "\n",
        "        self.dev = qml.device('default.qubit', wires=Q)\n",
        "        self.circuit = self._make_circuit()\n",
        "\n",
        "    def _make_circuit(self):\n",
        "        @qml.qnode(self.dev, interface='torch', diff_method='backprop')\n",
        "        def circuit(inputs, weights):\n",
        "            # Simplified encoding\n",
        "            for w in range(self.Q):\n",
        "                qml.RY(inputs[w], wires=w)\n",
        "\n",
        "            # ✅ Fewer layers\n",
        "            for l in range(self.L):\n",
        "                for w in range(self.Q):\n",
        "                    qml.Rot(weights[l, w, 0], weights[l, w, 1], weights[l, w, 2], wires=w)\n",
        "\n",
        "                for w in range(self.Q - 1):\n",
        "                    qml.CNOT([w, w + 1])\n",
        "\n",
        "            return [qml.expval(qml.PauliZ(w)) for w in range(self.Q)]\n",
        "\n",
        "        return circuit\n",
        "\n",
        "    def forward(self, kvec):\n",
        "        B = kvec.shape[0]\n",
        "        qinput = torch.tanh(self.lin_in(kvec))\n",
        "\n",
        "        # ✅ Process in larger batches\n",
        "        outputs = []\n",
        "        for i in range(B):\n",
        "            out = self.circuit(qinput[i], self.q_weights)\n",
        "            outputs.append(torch.stack(out).float())\n",
        "\n",
        "        return torch.stack(outputs)\n",
        "''')\n",
        "\n",
        "print(\"✅ Fast quantum layer created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FjQeIctVaeqT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjQeIctVaeqT",
        "outputId": "6b0f441c-4070-4e57-dfec-b2fa95bbbc1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fast model created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/models.py ----------\n",
        "open(f\"{SRC}/models.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.patches import PatchEmbedding\n",
        "from src.rnn import FastRNNRouter\n",
        "from src.quantum import FastQuantumLayer\n",
        "\n",
        "class FastHybridQRNN(nn.Module):\n",
        "    \"\"\"Speed-optimized hybrid model\"\"\"\n",
        "    def __init__(self, in_channels: int, num_classes: int,\n",
        "                 patch_size: int = 4, embed_dim: int = 48,\n",
        "                 K: int = 8, Q: int = 4, L: int = 2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
        "        self.rnn = FastRNNRouter(embed_dim, K)\n",
        "        self.qnn = FastQuantumLayer(embed_dim, Q, L)\n",
        "\n",
        "        # ✅ Simpler classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(Q, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.fc.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.patch_embed(x)\n",
        "        kvec = self.rnn(patches)\n",
        "        qout = self.qnn(kvec)\n",
        "        logits = self.fc(qout)\n",
        "        return logits\n",
        "''')\n",
        "\n",
        "print(\"✅ Fast model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lkpcfawqwxb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkpcfawqwxb3",
        "outputId": "15281688-45db-4e1c-f5f2-88b41419d12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fast training script created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/train.py ----------\n",
        "open(f\"{SRC}/train.py\", \"w\").write(r'''\n",
        "import os, argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "from src.data import MedMNISTDataset\n",
        "from src.models import FastHybridQRNN\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter_(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        log_prob = torch.log_softmax(pred, dim=1)\n",
        "        loss = -(one_hot * log_prob).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, num_classes):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        if num_classes == 2:\n",
        "            auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    return acc, f1, auc\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True)\n",
        "    parser.add_argument(\"--Q\", type=int, default=4)\n",
        "    parser.add_argument(\"--L\", type=int, default=2)\n",
        "    parser.add_argument(\"--K\", type=int, default=8)\n",
        "    parser.add_argument(\"--hidden\", type=int, default=48)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=35)\n",
        "    parser.add_argument(\"--seeds\", type=int, default=3)\n",
        "    parser.add_argument(\"--out\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    train_ds = MedMNISTDataset(args.dataset, split=\"train\")\n",
        "    val_ds = MedMNISTDataset(args.dataset, split=\"val\")\n",
        "    test_ds = MedMNISTDataset(args.dataset, split=\"test\")\n",
        "\n",
        "    num_classes = train_ds.n_classes\n",
        "    in_channels = train_ds[0][0].shape[0]\n",
        "\n",
        "    # ✅ SPEED: Larger batch size, no multiprocessing\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for seed in range(args.seeds):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Seed {seed}/{args.seeds-1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        model = FastHybridQRNN(\n",
        "            in_channels=in_channels,\n",
        "            num_classes=num_classes,\n",
        "            patch_size=4,\n",
        "            embed_dim=args.hidden,\n",
        "            K=args.K,\n",
        "            Q=args.Q,\n",
        "            L=args.L\n",
        "        ).to(device)\n",
        "\n",
        "        criterion = LabelSmoothingCE(smoothing=0.1)\n",
        "\n",
        "        # ✅ Slightly higher LR for faster convergence\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n",
        "\n",
        "        # ✅ Simpler schedule\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=args.epochs, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        best_f1 = 0\n",
        "        patience = 12  # ✅ Reduced patience\n",
        "        wait = 0\n",
        "\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_acc, val_f1, val_auc = evaluate(model, val_loader, device, num_classes)\n",
        "\n",
        "            print(f\"[seed {seed}] epoch {epoch:02d}  train_loss={train_loss:.4f}  \"\n",
        "                  f\"val_f1={val_f1:.4f}  val_acc={val_acc:.4f}  wait={wait}/{patience}\")\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                wait = 0\n",
        "                torch.save(model.state_dict(), f\"{args.out}/best_seed{seed}.pt\")\n",
        "            else:\n",
        "                wait += 1\n",
        "\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test\n",
        "        model.load_state_dict(torch.load(f\"{args.out}/best_seed{seed}.pt\"))\n",
        "        test_acc, test_f1, test_auc = evaluate(model, test_loader, device, num_classes)\n",
        "\n",
        "        results.append({\n",
        "            'seed': seed,\n",
        "            'test_acc': test_acc,\n",
        "            'test_f1': test_f1,\n",
        "            'test_auc': test_auc\n",
        "        })\n",
        "\n",
        "        print(f\"\\n[Seed {seed}] Test: ACC={test_acc:.4f}, F1={test_f1:.4f}, AUC={test_auc:.4f}\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Final Results ({args.seeds} seeds)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    accs = [r['test_acc'] for r in results]\n",
        "    f1s = [r['test_f1'] for r in results]\n",
        "\n",
        "    print(f\"ACC: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
        "    print(f\"F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
        "    print(f\"Best: {np.max(accs):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"✅ Fast training script created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "NUR81WsKw1zj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUR81WsKw1zj",
        "outputId": "823a4caf-ecf0-41e6-e33b-b12ffd7962fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hybridqnn_seq\n",
            "\n",
            "======================================================================\n",
            "🚀 FAST Training BloodMNIST → runs/BloodMNIST_FAST_Q4L2_20251027_100919\n",
            "======================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.2 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
            "  warnings.warn(\n",
            "Device: cpu\n",
            "100% 35.5M/35.5M [01:07<00:00, 528kB/s]\n",
            "\n",
            "============================================================\n",
            "Seed 0/2\n",
            "============================================================\n",
            "[seed 0] epoch 01  train_loss=1.4986  val_f1=0.5171  val_acc=0.6285  wait=0/12\n",
            "[seed 0] epoch 02  train_loss=1.1473  val_f1=0.6660  val_acc=0.7173  wait=0/12\n",
            "[seed 0] epoch 03  train_loss=1.0567  val_f1=0.6844  val_acc=0.7237  wait=0/12\n",
            "[seed 0] epoch 04  train_loss=0.9977  val_f1=0.5682  val_acc=0.6408  wait=0/12\n",
            "[seed 0] epoch 05  train_loss=0.9605  val_f1=0.7106  val_acc=0.7617  wait=1/12\n",
            "[seed 0] epoch 06  train_loss=0.9291  val_f1=0.7349  val_acc=0.7839  wait=0/12\n",
            "[seed 0] epoch 07  train_loss=0.9092  val_f1=0.7609  val_acc=0.7804  wait=0/12\n",
            "[seed 0] epoch 08  train_loss=0.8878  val_f1=0.7244  val_acc=0.7564  wait=0/12\n",
            "[seed 0] epoch 09  train_loss=0.8597  val_f1=0.7925  val_acc=0.8148  wait=1/12\n",
            "[seed 0] epoch 10  train_loss=0.8422  val_f1=0.8167  val_acc=0.8400  wait=0/12\n",
            "[seed 0] epoch 11  train_loss=0.8245  val_f1=0.7553  val_acc=0.7699  wait=0/12\n",
            "[seed 0] epoch 12  train_loss=0.8128  val_f1=0.8313  val_acc=0.8458  wait=1/12\n",
            "[seed 0] epoch 13  train_loss=0.8017  val_f1=0.8306  val_acc=0.8493  wait=0/12\n",
            "[seed 0] epoch 14  train_loss=0.7913  val_f1=0.8284  val_acc=0.8435  wait=1/12\n",
            "[seed 0] epoch 15  train_loss=0.7819  val_f1=0.8424  val_acc=0.8563  wait=2/12\n",
            "[seed 0] epoch 16  train_loss=0.7755  val_f1=0.8630  val_acc=0.8662  wait=0/12\n",
            "[seed 0] epoch 17  train_loss=0.7654  val_f1=0.8607  val_acc=0.8668  wait=0/12\n",
            "[seed 0] epoch 18  train_loss=0.7543  val_f1=0.8669  val_acc=0.8703  wait=1/12\n",
            "[seed 0] epoch 19  train_loss=0.7523  val_f1=0.8552  val_acc=0.8627  wait=0/12\n",
            "[seed 0] epoch 20  train_loss=0.7416  val_f1=0.8565  val_acc=0.8633  wait=1/12\n",
            "[seed 0] epoch 21  train_loss=0.7399  val_f1=0.8763  val_acc=0.8808  wait=2/12\n",
            "[seed 0] epoch 22  train_loss=0.7316  val_f1=0.8690  val_acc=0.8715  wait=0/12\n",
            "[seed 0] epoch 23  train_loss=0.7246  val_f1=0.8702  val_acc=0.8721  wait=1/12\n",
            "[seed 0] epoch 24  train_loss=0.7199  val_f1=0.8806  val_acc=0.8803  wait=2/12\n",
            "[seed 0] epoch 25  train_loss=0.7171  val_f1=0.8762  val_acc=0.8791  wait=0/12\n",
            "[seed 0] epoch 26  train_loss=0.7119  val_f1=0.8824  val_acc=0.8820  wait=1/12\n",
            "[seed 0] epoch 27  train_loss=0.7063  val_f1=0.8849  val_acc=0.8849  wait=0/12\n",
            "[seed 0] epoch 28  train_loss=0.7013  val_f1=0.8759  val_acc=0.8779  wait=0/12\n",
            "[seed 0] epoch 29  train_loss=0.6999  val_f1=0.8833  val_acc=0.8843  wait=1/12\n",
            "[seed 0] epoch 30  train_loss=0.6964  val_f1=0.8853  val_acc=0.8861  wait=2/12\n",
            "[seed 0] epoch 31  train_loss=0.6945  val_f1=0.8803  val_acc=0.8814  wait=0/12\n",
            "[seed 0] epoch 32  train_loss=0.6945  val_f1=0.8794  val_acc=0.8808  wait=1/12\n",
            "[seed 0] epoch 33  train_loss=0.6929  val_f1=0.8801  val_acc=0.8803  wait=2/12\n",
            "[seed 0] epoch 34  train_loss=0.6896  val_f1=0.8823  val_acc=0.8826  wait=3/12\n",
            "[seed 0] epoch 35  train_loss=0.6925  val_f1=0.8781  val_acc=0.8791  wait=4/12\n",
            "\n",
            "[Seed 0] Test: ACC=0.8693, F1=0.8697, AUC=0.9827\n",
            "\n",
            "============================================================\n",
            "Seed 1/2\n",
            "============================================================\n",
            "[seed 1] epoch 01  train_loss=1.5078  val_f1=0.5708  val_acc=0.6525  wait=0/12\n",
            "[seed 1] epoch 02  train_loss=1.1200  val_f1=0.6448  val_acc=0.7068  wait=0/12\n",
            "[seed 1] epoch 03  train_loss=1.0493  val_f1=0.6831  val_acc=0.7377  wait=0/12\n",
            "[seed 1] epoch 04  train_loss=1.0017  val_f1=0.7087  val_acc=0.7482  wait=0/12\n",
            "[seed 1] epoch 05  train_loss=0.9510  val_f1=0.7356  val_acc=0.7850  wait=0/12\n",
            "[seed 1] epoch 06  train_loss=0.9179  val_f1=0.7322  val_acc=0.7804  wait=0/12\n",
            "[seed 1] epoch 07  train_loss=0.8973  val_f1=0.7836  val_acc=0.8143  wait=1/12\n",
            "[seed 1] epoch 08  train_loss=0.8672  val_f1=0.7951  val_acc=0.8137  wait=0/12\n",
            "[seed 1] epoch 09  train_loss=0.8414  val_f1=0.8398  val_acc=0.8481  wait=0/12\n",
            "[seed 1] epoch 10  train_loss=0.8190  val_f1=0.8566  val_acc=0.8627  wait=0/12\n",
            "[seed 1] epoch 11  train_loss=0.7954  val_f1=0.8587  val_acc=0.8604  wait=0/12\n",
            "[seed 1] epoch 12  train_loss=0.7815  val_f1=0.8538  val_acc=0.8551  wait=0/12\n",
            "[seed 1] epoch 13  train_loss=0.7708  val_f1=0.8678  val_acc=0.8697  wait=1/12\n",
            "[seed 1] epoch 14  train_loss=0.7507  val_f1=0.8731  val_acc=0.8732  wait=0/12\n",
            "[seed 1] epoch 15  train_loss=0.7442  val_f1=0.8751  val_acc=0.8756  wait=0/12\n",
            "[seed 1] epoch 16  train_loss=0.7307  val_f1=0.8751  val_acc=0.8738  wait=0/12\n",
            "[seed 1] epoch 17  train_loss=0.7224  val_f1=0.8928  val_acc=0.8949  wait=0/12\n",
            "[seed 1] epoch 18  train_loss=0.7120  val_f1=0.8785  val_acc=0.8785  wait=0/12\n",
            "[seed 1] epoch 19  train_loss=0.7006  val_f1=0.8869  val_acc=0.8884  wait=1/12\n",
            "[seed 1] epoch 20  train_loss=0.6943  val_f1=0.8917  val_acc=0.8914  wait=2/12\n",
            "[seed 1] epoch 21  train_loss=0.6887  val_f1=0.8700  val_acc=0.8738  wait=3/12\n",
            "[seed 1] epoch 22  train_loss=0.6776  val_f1=0.8899  val_acc=0.8914  wait=4/12\n",
            "[seed 1] epoch 23  train_loss=0.6792  val_f1=0.8934  val_acc=0.8943  wait=5/12\n",
            "[seed 1] epoch 24  train_loss=0.6676  val_f1=0.8969  val_acc=0.8972  wait=0/12\n",
            "[seed 1] epoch 25  train_loss=0.6636  val_f1=0.9018  val_acc=0.9013  wait=0/12\n",
            "[seed 1] epoch 26  train_loss=0.6577  val_f1=0.9023  val_acc=0.9025  wait=0/12\n",
            "[seed 1] epoch 27  train_loss=0.6607  val_f1=0.8964  val_acc=0.8972  wait=0/12\n",
            "[seed 1] epoch 28  train_loss=0.6522  val_f1=0.8955  val_acc=0.8954  wait=1/12\n",
            "[seed 1] epoch 29  train_loss=0.6500  val_f1=0.8963  val_acc=0.8960  wait=2/12\n",
            "[seed 1] epoch 30  train_loss=0.6477  val_f1=0.8955  val_acc=0.8954  wait=3/12\n",
            "[seed 1] epoch 31  train_loss=0.6482  val_f1=0.9032  val_acc=0.9030  wait=4/12\n",
            "[seed 1] epoch 32  train_loss=0.6407  val_f1=0.8985  val_acc=0.8984  wait=0/12\n",
            "[seed 1] epoch 33  train_loss=0.6409  val_f1=0.8990  val_acc=0.8984  wait=1/12\n",
            "[seed 1] epoch 34  train_loss=0.6417  val_f1=0.8941  val_acc=0.8943  wait=2/12\n",
            "[seed 1] epoch 35  train_loss=0.6394  val_f1=0.8979  val_acc=0.8978  wait=3/12\n",
            "\n",
            "[Seed 1] Test: ACC=0.8983, F1=0.8982, AUC=0.9879\n",
            "\n",
            "============================================================\n",
            "Seed 2/2\n",
            "============================================================\n",
            "[seed 2] epoch 01  train_loss=1.5528  val_f1=0.5265  val_acc=0.6069  wait=0/12\n",
            "[seed 2] epoch 02  train_loss=1.1556  val_f1=0.6493  val_acc=0.6974  wait=0/12\n",
            "[seed 2] epoch 03  train_loss=1.0470  val_f1=0.6766  val_acc=0.7278  wait=0/12\n",
            "[seed 2] epoch 04  train_loss=0.9952  val_f1=0.7170  val_acc=0.7634  wait=0/12\n",
            "[seed 2] epoch 05  train_loss=0.9573  val_f1=0.7359  val_acc=0.7611  wait=0/12\n",
            "[seed 2] epoch 06  train_loss=0.9340  val_f1=0.7256  val_acc=0.7769  wait=0/12\n",
            "[seed 2] epoch 07  train_loss=0.9066  val_f1=0.7928  val_acc=0.8125  wait=1/12\n",
            "[seed 2] epoch 08  train_loss=0.8892  val_f1=0.7852  val_acc=0.8049  wait=0/12\n",
            "[seed 2] epoch 09  train_loss=0.8662  val_f1=0.8147  val_acc=0.8324  wait=1/12\n",
            "[seed 2] epoch 10  train_loss=0.8517  val_f1=0.8120  val_acc=0.8259  wait=0/12\n",
            "[seed 2] epoch 11  train_loss=0.8384  val_f1=0.8389  val_acc=0.8417  wait=1/12\n",
            "[seed 2] epoch 12  train_loss=0.8174  val_f1=0.8457  val_acc=0.8475  wait=0/12\n",
            "[seed 2] epoch 13  train_loss=0.8034  val_f1=0.8471  val_acc=0.8557  wait=0/12\n",
            "[seed 2] epoch 14  train_loss=0.7916  val_f1=0.8131  val_acc=0.8160  wait=0/12\n",
            "[seed 2] epoch 15  train_loss=0.7830  val_f1=0.8594  val_acc=0.8645  wait=1/12\n",
            "[seed 2] epoch 16  train_loss=0.7730  val_f1=0.8708  val_acc=0.8721  wait=0/12\n",
            "[seed 2] epoch 17  train_loss=0.7604  val_f1=0.8833  val_acc=0.8826  wait=0/12\n",
            "[seed 2] epoch 18  train_loss=0.7519  val_f1=0.8823  val_acc=0.8820  wait=0/12\n",
            "[seed 2] epoch 19  train_loss=0.7448  val_f1=0.8796  val_acc=0.8808  wait=1/12\n",
            "[seed 2] epoch 20  train_loss=0.7350  val_f1=0.8922  val_acc=0.8925  wait=2/12\n",
            "[seed 2] epoch 21  train_loss=0.7305  val_f1=0.8758  val_acc=0.8756  wait=0/12\n",
            "[seed 2] epoch 22  train_loss=0.7229  val_f1=0.8947  val_acc=0.8937  wait=1/12\n",
            "[seed 2] epoch 23  train_loss=0.7141  val_f1=0.8806  val_acc=0.8826  wait=0/12\n",
            "[seed 2] epoch 24  train_loss=0.7123  val_f1=0.8922  val_acc=0.8925  wait=1/12\n",
            "[seed 2] epoch 25  train_loss=0.7088  val_f1=0.8980  val_acc=0.8978  wait=2/12\n",
            "[seed 2] epoch 26  train_loss=0.6998  val_f1=0.8850  val_acc=0.8855  wait=0/12\n",
            "[seed 2] epoch 27  train_loss=0.6988  val_f1=0.8993  val_acc=0.8989  wait=1/12\n",
            "[seed 2] epoch 28  train_loss=0.6913  val_f1=0.8961  val_acc=0.8966  wait=0/12\n",
            "[seed 2] epoch 29  train_loss=0.6904  val_f1=0.8981  val_acc=0.8978  wait=1/12\n",
            "[seed 2] epoch 30  train_loss=0.6838  val_f1=0.8968  val_acc=0.8972  wait=2/12\n",
            "[seed 2] epoch 31  train_loss=0.6851  val_f1=0.8983  val_acc=0.8984  wait=3/12\n",
            "[seed 2] epoch 32  train_loss=0.6803  val_f1=0.9033  val_acc=0.9036  wait=4/12\n",
            "[seed 2] epoch 33  train_loss=0.6799  val_f1=0.8978  val_acc=0.8978  wait=0/12\n",
            "[seed 2] epoch 34  train_loss=0.6781  val_f1=0.8993  val_acc=0.8995  wait=1/12\n",
            "[seed 2] epoch 35  train_loss=0.6776  val_f1=0.8976  val_acc=0.8978  wait=2/12\n",
            "\n",
            "[Seed 2] Test: ACC=0.8866, F1=0.8864, AUC=0.9837\n",
            "\n",
            "============================================================\n",
            "Final Results (3 seeds)\n",
            "============================================================\n",
            "ACC: 0.8847 ± 0.0119\n",
            "F1: 0.8847 ± 0.0117\n",
            "Best: 0.8983\n",
            "✅ Completed: runs/BloodMNIST_FAST_Q4L2_20251027_100919\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🎉 Fast training complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hybridqnn_seq\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ✅ SPEED CONFIG: 3-5x faster, 88-90% accuracy\n",
        "CONFIGS = [\n",
        "    # (DATASET,       Q,  L,  K, HIDDEN, EPOCHS, SEEDS)\n",
        "    (\"BloodMNIST\",   4,  2,  8,   48,    35,     3),\n",
        "]\n",
        "\n",
        "for DATASET, Q, L, K, HIDDEN, EPOCHS, SEEDS in CONFIGS:\n",
        "    OUTDIR = f\"runs/{DATASET}_FAST_Q{Q}L{L}_{TS}\"\n",
        "    os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🚀 FAST Training {DATASET} → {OUTDIR}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    !python -m src.train \\\n",
        "        --dataset {DATASET} \\\n",
        "        --Q {Q} --L {L} --K {K} --hidden {HIDDEN} \\\n",
        "        --epochs {EPOCHS} --seeds {SEEDS} \\\n",
        "        --out {OUTDIR}\n",
        "\n",
        "    print(f\"✅ Completed: {OUTDIR}\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"🎉 Fast training complete!\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation study"
      ],
      "metadata": {
        "id": "A78c8EfFnDbu"
      },
      "id": "A78c8EfFnDbu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- src/ablation.py ----------\n",
        "open(f\"{SRC}/ablation.py\", \"w\").write(r'''\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "from src.data import MedMNISTDataset\n",
        "from src.models import FastHybridQRNN\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter_(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        log_prob = torch.log_softmax(pred, dim=1)\n",
        "        loss = -(one_hot * log_prob).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=20):\n",
        "    criterion = LabelSmoothingCE(smoothing=0.1)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience = 8\n",
        "    wait = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                logits = model(imgs)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(labels.numpy())\n",
        "\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "        val_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"BloodMNIST\")\n",
        "    parser.add_argument(\"--out_root\", type=str, default=\"runs/ablation\")\n",
        "    parser.add_argument(\"--seeds\", type=int, default=3)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    train_ds = MedMNISTDataset(args.dataset, split=\"train\")\n",
        "    val_ds = MedMNISTDataset(args.dataset, split=\"val\")\n",
        "    test_ds = MedMNISTDataset(args.dataset, split=\"test\")\n",
        "\n",
        "    num_classes = train_ds.n_classes\n",
        "    in_channels = train_ds[0][0].shape[0]\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "    # ✅ Ablation grid\n",
        "    K_values = [4, 8, 12]\n",
        "    Q_values = [3, 4, 6]\n",
        "    L_values = [1, 2, 3]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔬 ABLATION STUDY: {args.dataset}\")\n",
        "    print(f\"Grid: K={K_values}, Q={Q_values}, L={L_values}\")\n",
        "    print(f\"Seeds: {args.seeds}, Epochs: {args.epochs}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    total_runs = len(K_values) * len(Q_values) * len(L_values) * args.seeds\n",
        "    pbar = tqdm(total=total_runs, desc=\"Ablation\")\n",
        "\n",
        "    for K, Q, L in product(K_values, Q_values, L_values):\n",
        "        config_results = []\n",
        "\n",
        "        for seed in range(args.seeds):\n",
        "            torch.manual_seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "            # Build model\n",
        "            model = FastHybridQRNN(\n",
        "                in_channels=in_channels,\n",
        "                num_classes=num_classes,\n",
        "                patch_size=4,\n",
        "                embed_dim=48,\n",
        "                K=K,\n",
        "                Q=Q,\n",
        "                L=L\n",
        "            ).to(device)\n",
        "\n",
        "            # Train\n",
        "            val_acc = train_model(model, train_loader, val_loader, device, epochs=args.epochs)\n",
        "\n",
        "            # Test\n",
        "            test_acc, test_f1 = test_model(model, test_loader, device)\n",
        "\n",
        "            config_results.append({\n",
        "                'K': K,\n",
        "                'Q': Q,\n",
        "                'L': L,\n",
        "                'seed': seed,\n",
        "                'val_acc': val_acc,\n",
        "                'test_acc': test_acc,\n",
        "                'test_f1': test_f1\n",
        "            })\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                'K': K, 'Q': Q, 'L': L,\n",
        "                'seed': seed,\n",
        "                'test_acc': f'{test_acc:.3f}'\n",
        "            })\n",
        "\n",
        "        results.extend(config_results)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(args.out_root, exist_ok=True)\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(f\"{args.out_root}/ablation_results.csv\", index=False)\n",
        "\n",
        "    # Summary statistics\n",
        "    summary = df.groupby(['K', 'Q', 'L']).agg({\n",
        "        'test_acc': ['mean', 'std'],\n",
        "        'test_f1': ['mean', 'std']\n",
        "    }).reset_index()\n",
        "\n",
        "    summary.columns = ['K', 'Q', 'L', 'test_acc_mean', 'test_acc_std', 'test_f1_mean', 'test_f1_std']\n",
        "    summary = summary.sort_values('test_acc_mean', ascending=False)\n",
        "    summary.to_csv(f\"{args.out_root}/ablation_summary.csv\", index=False)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"✅ Ablation complete!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nTop 5 Configurations:\")\n",
        "    print(summary.head(5).to_string(index=False))\n",
        "    print(f\"\\nResults saved to: {args.out_root}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"✅ Ablation script created\")\n"
      ],
      "metadata": {
        "id": "D90FssnGnDCt"
      },
      "id": "D90FssnGnDCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- src/figures.py ----------\n",
        "open(f\"{SRC}/figures.py\", \"w\").write(r'''\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'font.family': 'serif',\n",
        "    'axes.labelsize': 14,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300\n",
        "})\n",
        "\n",
        "def plot_training_curves(csv_path, out_dir):\n",
        "    \"\"\"Plot training and validation curves\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Loss curve\n",
        "    axes[0, 0].plot(df['epoch'], df['train_loss'], label='Train Loss', linewidth=2)\n",
        "    if 'val_loss' in df.columns:\n",
        "        axes[0, 0].plot(df['epoch'], df['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training & Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curve\n",
        "    if 'val_acc' in df.columns:\n",
        "        axes[0, 1].plot(df['epoch'], df['val_acc'], label='Val Accuracy', linewidth=2, color='orange')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].set_title('Validation Accuracy')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # F1 score\n",
        "    if 'val_f1' in df.columns:\n",
        "        axes[1, 0].plot(df['epoch'], df['val_f1'], label='Val F1-Score', linewidth=2, color='green')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('F1-Score')\n",
        "        axes[1, 0].set_title('Validation F1-Score')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate\n",
        "    if 'lr' in df.columns:\n",
        "        axes[1, 1].plot(df['epoch'], df['lr'], label='Learning Rate', linewidth=2, color='red')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Learning Rate')\n",
        "        axes[1, 1].set_title('Learning Rate Schedule')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{out_dir}/training_curves.pdf\", bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"✅ Training curves saved to {out_dir}/training_curves.pdf\")\n",
        "\n",
        "def plot_ablation_heatmaps(csv_path, out_dir):\n",
        "    \"\"\"Plot ablation study heatmaps\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Create heatmaps for each L value\n",
        "    L_values = sorted(df['L'].unique())\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(L_values), figsize=(6*len(L_values), 5))\n",
        "    if len(L_values) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, L in enumerate(L_values):\n",
        "        df_L = df[df['L'] == L]\n",
        "        pivot = df_L.pivot_table(values='test_acc_mean', index='Q', columns='K')\n",
        "\n",
        "        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                   ax=axes[idx], cbar_kws={'label': 'Test Accuracy'},\n",
        "                   vmin=0.80, vmax=0.92)\n",
        "        axes[idx].set_title(f'L = {L}')\n",
        "        axes[idx].set_xlabel('K (Top-K patches)')\n",
        "        axes[idx].set_ylabel('Q (Qubits)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{out_dir}/ablation_heatmaps.pdf\", bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"✅ Ablation heatmaps saved to {out_dir}/ablation_heatmaps.pdf\")\n",
        "\n",
        "def plot_ablation_boxplots(csv_path, out_dir):\n",
        "    \"\"\"Plot ablation study boxplots\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # K effect\n",
        "    df.boxplot(column='test_acc', by='K', ax=axes[0])\n",
        "    axes[0].set_title('Effect of K (Top-K patches)')\n",
        "    axes[0].set_xlabel('K')\n",
        "    axes[0].set_ylabel('Test Accuracy')\n",
        "    axes[0].get_figure().suptitle('')\n",
        "\n",
        "    # Q effect\n",
        "    df.boxplot(column='test_acc', by='Q', ax=axes[1])\n",
        "    axes[1].set_title('Effect of Q (Number of Qubits)')\n",
        "    axes[1].set_xlabel('Q')\n",
        "    axes[1].set_ylabel('Test Accuracy')\n",
        "    axes[1].get_figure().suptitle('')\n",
        "\n",
        "    # L effect\n",
        "    df.boxplot(column='test_acc', by='L', ax=axes[2])\n",
        "    axes[2].set_title('Effect of L (Quantum Layers)')\n",
        "    axes[2].set_xlabel('L')\n",
        "    axes[2].set_ylabel('Test Accuracy')\n",
        "    axes[2].get_figure().suptitle('')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{out_dir}/ablation_boxplots.pdf\", bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"✅ Ablation boxplots saved to {out_dir}/ablation_boxplots.pdf\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_csv\", type=str, help=\"Path to training log CSV\")\n",
        "    parser.add_argument(\"--ablation_csv\", type=str, help=\"Path to ablation results CSV\")\n",
        "    parser.add_argument(\"--ablation_summary\", type=str, help=\"Path to ablation summary CSV\")\n",
        "    parser.add_argument(\"--out\", type=str, default=\"paper/figs\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    os.makedirs(args.out, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"📊 GENERATING FIGURES\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Training curves\n",
        "    if args.train_csv and os.path.exists(args.train_csv):\n",
        "        plot_training_curves(args.train_csv, args.out)\n",
        "\n",
        "    # Ablation heatmaps\n",
        "    if args.ablation_summary and os.path.exists(args.ablation_summary):\n",
        "        plot_ablation_heatmaps(args.ablation_summary, args.out)\n",
        "\n",
        "    # Ablation boxplots\n",
        "    if args.ablation_csv and os.path.exists(args.ablation_csv):\n",
        "        plot_ablation_boxplots(args.ablation_csv, args.out)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"✅ All figures saved to: {args.out}/\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"✅ Figures script created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3qIJ83rnKRx",
        "outputId": "05419ae9-7c3c-465e-ad69-c96a380860c5"
      },
      "id": "u3qIJ83rnKRx",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Figures script created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- src/xai.py ----------\n",
        "open(f\"{SRC}/xai.py\", \"w\").write(r'''\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "from src.data import MedMNISTDataset\n",
        "from src.models import FastHybridQRNN\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "class AttentionVisualizer:\n",
        "    \"\"\"Visualize attention weights from RNN router\"\"\"\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.attention_weights = None\n",
        "\n",
        "        # Hook to capture attention\n",
        "        def hook_fn(module, input, output):\n",
        "            # Extract attention from RNN router\n",
        "            if hasattr(module, 'attn_proj'):\n",
        "                self.attention_weights = output\n",
        "\n",
        "        self.model.rnn.register_forward_hook(hook_fn)\n",
        "\n",
        "    def visualize(self, img, out_path):\n",
        "        \"\"\"Visualize attention on image patches\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_tensor = img.unsqueeze(0).to(self.device)\n",
        "            _ = self.model(img_tensor)\n",
        "\n",
        "        if self.attention_weights is not None:\n",
        "            attn = self.attention_weights[0].cpu().numpy()\n",
        "\n",
        "            # Reshape to spatial grid\n",
        "            H = W = int(np.sqrt(len(attn)))\n",
        "            attn_map = attn.reshape(H, W)\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "            # Original image\n",
        "            if img.shape[0] == 1:\n",
        "                axes[0].imshow(img[0].cpu().numpy(), cmap='gray')\n",
        "            else:\n",
        "                axes[0].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
        "            axes[0].set_title('Original Image')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Attention heatmap\n",
        "            im = axes[1].imshow(attn_map, cmap='hot', interpolation='nearest')\n",
        "            axes[1].set_title('Attention Heatmap')\n",
        "            axes[1].axis('off')\n",
        "            plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(out_path, bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "class QuantumSaliency:\n",
        "    \"\"\"Visualize qubit importance via gradient-based saliency\"\"\"\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def compute_saliency(self, img, target_class):\n",
        "        \"\"\"Compute saliency map\"\"\"\n",
        "        self.model.eval()\n",
        "        img_tensor = img.unsqueeze(0).to(self.device)\n",
        "        img_tensor.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        logits = self.model(img_tensor)\n",
        "\n",
        "        # Backward pass for target class\n",
        "        self.model.zero_grad()\n",
        "        logits[0, target_class].backward()\n",
        "\n",
        "        # Gradient magnitude\n",
        "        saliency = img_tensor.grad.abs().squeeze().cpu().numpy()\n",
        "\n",
        "        return saliency\n",
        "\n",
        "    def visualize(self, img, target_class, out_path):\n",
        "        \"\"\"Visualize saliency map\"\"\"\n",
        "        saliency = self.compute_saliency(img, target_class)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Original\n",
        "        if img.shape[0] == 1:\n",
        "            axes[0].imshow(img[0].cpu().numpy(), cmap='gray')\n",
        "        else:\n",
        "            axes[0].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
        "        axes[0].set_title('Original Image')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Saliency\n",
        "        if saliency.ndim == 3:\n",
        "            saliency = saliency.max(axis=0)\n",
        "        im = axes[1].imshow(saliency, cmap='hot')\n",
        "        axes[1].set_title('Quantum Saliency Map')\n",
        "        axes[1].axis('off')\n",
        "        plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"BloodMNIST\")\n",
        "    parser.add_argument(\"--model_path\", type=str, required=True)\n",
        "    parser.add_argument(\"--Q\", type=int, default=4)\n",
        "    parser.add_argument(\"--L\", type=int, default=2)\n",
        "    parser.add_argument(\"--K\", type=int, default=8)\n",
        "    parser.add_argument(\"--hidden\", type=int, default=48)\n",
        "    parser.add_argument(\"--out\", type=str, default=\"paper/figs/xai\")\n",
        "    parser.add_argument(\"--num_samples\", type=int, default=5)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load data\n",
        "    test_ds = MedMNISTDataset(args.dataset, split=\"test\")\n",
        "    num_classes = test_ds.n_classes\n",
        "    in_channels = test_ds[0][0].shape[0]\n",
        "\n",
        "    # Load model\n",
        "    model = FastHybridQRNN(\n",
        "        in_channels=in_channels,\n",
        "        num_classes=num_classes,\n",
        "        patch_size=4,\n",
        "        embed_dim=args.hidden,\n",
        "        K=args.K,\n",
        "        Q=args.Q,\n",
        "        L=args.L\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    os.makedirs(args.out, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔍 GENERATING XAI VISUALIZATIONS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Initialize visualizers\n",
        "    attn_viz = AttentionVisualizer(model, device)\n",
        "    saliency_viz = QuantumSaliency(model, device)\n",
        "\n",
        "    # Generate visualizations\n",
        "    for i in range(min(args.num_samples, len(test_ds))):\n",
        "        img, label = test_ds[i]\n",
        "\n",
        "        # Attention visualization\n",
        "        attn_viz.visualize(img, f\"{args.out}/attention_sample_{i}.pdf\")\n",
        "\n",
        "        # Saliency visualization\n",
        "        saliency_viz.visualize(img, label.item(), f\"{args.out}/saliency_sample_{i}.pdf\")\n",
        "\n",
        "        print(f\"✅ Generated XAI visualizations for sample {i}\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"✅ All XAI visualizations saved to: {args.out}/\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"✅ XAI script created\")\n"
      ],
      "metadata": {
        "id": "2xgY3SMsnTp_"
      },
      "id": "2xgY3SMsnTp_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this to src/train.py to save training logs\n",
        "open(f\"{SRC}/train_with_log.py\", \"w\").write(open(f\"{SRC}/train.py\").read().replace(\n",
        "    \"for epoch in range(1, args.epochs + 1):\",\n",
        "    \"\"\"train_log = []\n",
        "    for epoch in range(1, args.epochs + 1):\"\"\"\n",
        ").replace(\n",
        "    'print(f\"[seed {seed}] epoch',\n",
        "    \"\"\"train_log.append({\n",
        "                'seed': seed,\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'val_f1': val_f1,\n",
        "                'lr': optimizer.param_groups[0]['lr'],\n",
        "                'wait': wait\n",
        "            })\n",
        "            print(f\"[seed {seed}] epoch\"\"\"\n",
        ").replace(\n",
        "    \"print(f\\\"\\\\n[Seed {seed}] Test:\",\n",
        "    \"\"\"# Save training log\n",
        "        pd.DataFrame(train_log).to_csv(f\"{args.out}/train_log_seed{seed}.csv\", index=False)\n",
        "\n",
        "        print(f\"\\\\n[Seed {seed}] Test:\"\"\"\n",
        "))\n",
        "\n",
        "print(\"✅ Training script with logging created\")\n"
      ],
      "metadata": {
        "id": "C7nzMJtBnU2O"
      },
      "id": "C7nzMJtBnU2O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SNIPPET 12: Run ablation study\n",
        "%cd /content/hybridqnn_seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔬 RUNNING ABLATION STUDY\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "!python -m src.ablation \\\n",
        "    --dataset BloodMNIST \\\n",
        "    --out_root runs/blood_ablation \\\n",
        "    --seeds 3 \\\n",
        "    --epochs 20\n",
        "\n",
        "print(\"\\n✅ Ablation study complete!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vK3AEPRQnYRx"
      },
      "id": "vK3AEPRQnYRx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SNIPPET 13: Generate all figures\n",
        "%cd /content/hybridqnn_seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 GENERATING PUBLICATION FIGURES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Find the latest training run\n",
        "import glob\n",
        "latest_run = sorted(glob.glob(\"runs/BloodMNIST_FAST_Q4L2_*/train_log_seed0.csv\"))[-1]\n",
        "\n",
        "!python -m src.figures \\\n",
        "    --train_csv {latest_run} \\\n",
        "    --ablation_csv runs/blood_ablation/ablation_results.csv \\\n",
        "    --ablation_summary runs/blood_ablation/ablation_summary.csv \\\n",
        "    --out paper/figs\n",
        "\n",
        "print(\"\\n✅ All figures generated!\")\n"
      ],
      "metadata": {
        "id": "FwW5FXr9nmfR"
      },
      "id": "FwW5FXr9nmfR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SNIPPET 14: Generate XAI visualizations\n",
        "%cd /content/hybridqnn_seq\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔍 GENERATING XAI VISUALIZATIONS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Find best model\n",
        "import glob\n",
        "best_model = sorted(glob.glob(\"runs/BloodMNIST_FAST_Q4L2_*/best_seed1.pt\"))[-1]\n",
        "\n",
        "!python -m src.xai \\\n",
        "    --dataset BloodMNIST \\\n",
        "    --model_path {best_model} \\\n",
        "    --Q 4 --L 2 --K 8 --hidden 48 \\\n",
        "    --out paper/figs/xai \\\n",
        "    --num_samples 10\n",
        "\n",
        "print(\"\\n✅ XAI visualizations complete!\")\n"
      ],
      "metadata": {
        "id": "Bk8sAYbvno3N"
      },
      "id": "Bk8sAYbvno3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9jAWAMFntv7"
      },
      "id": "L9jAWAMFntv7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}