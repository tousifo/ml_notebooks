{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/Blood_MedMNIST_QNN_AllInOne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7a964ac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a964ac5",
        "outputId": "591b0f7d-0404-4b7f-f5e0-293413d74f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Installing dependencies...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "âœ… All packages installed successfully!\n",
            "   - PyTorch\n",
            "   - PennyLane + Lightning\n",
            "   - MedMNIST\n",
            "   - Scikit-learn\n",
            "   - tqdm\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ========================================\n",
        "\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q pennylane pennylane-lightning\n",
        "!pip install -q medmnist\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"\\nâœ… All packages installed successfully!\")\n",
        "print(\"   - PyTorch\")\n",
        "print(\"   - PennyLane + Lightning\")\n",
        "print(\"   - MedMNIST\")\n",
        "print(\"   - Scikit-learn\")\n",
        "print(\"   - tqdm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Wd2qzHokdCwb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2qzHokdCwb",
        "outputId": "e4d72424-5594-4601-b7ed-74612f6fee87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Installing packages...\n",
            "âœ… Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# HYBRID QUANTUM-RNN ARCHITECTURE\n",
        "# Original design with stability fixes\n",
        "# ========================================\n",
        "\n",
        "import os, sys\n",
        "\n",
        "ROOT = \"/content/hybridqnn_seq\"\n",
        "SRC = f\"{ROOT}/src\"\n",
        "os.makedirs(SRC, exist_ok=True)\n",
        "open(f\"{SRC}/__init__.py\", \"w\").write(\"\")\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "# Install dependencies\n",
        "print(\"ðŸ“¦ Installing packages...\")\n",
        "!pip install -q torch torchvision pennylane pennylane-lightning medmnist scikit-learn tqdm numpy\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fODYibQkkirc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODYibQkkirc",
        "outputId": "0db2407d-f9d6-4de1-a5a9-b606686635b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data loader created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/data.py ----------\n",
        "open(f\"{SRC}/data.py\", \"w\").write(r'''\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class MedMNISTDataset(Dataset):\n",
        "    def __init__(self, dataset_name: str, split: str = \"train\", transform=None):\n",
        "        super().__init__()\n",
        "        info = INFO[dataset_name.lower()]\n",
        "        DataClass = getattr(medmnist, info['python_class'])\n",
        "        self.dataset = DataClass(split=split, download=True, transform=transform)\n",
        "        self.n_classes = len(info['label'])\n",
        "        self.task = info['task']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img = torch.from_numpy(np.array(img)).float()\n",
        "        if len(img.shape) == 2:\n",
        "            img = img.unsqueeze(0)\n",
        "        elif len(img.shape) == 3 and img.shape[2] in [1, 3]:\n",
        "            img = img.permute(2, 0, 1)\n",
        "        img = img / 255.0\n",
        "\n",
        "        if isinstance(label, np.ndarray):\n",
        "            label = torch.from_numpy(label).long()\n",
        "        elif not isinstance(label, torch.Tensor):\n",
        "            label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        if label.dim() > 0:\n",
        "            label = label.squeeze()\n",
        "\n",
        "        return img, label\n",
        "''')\n",
        "\n",
        "print(\"âœ… Data loader created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5X45s2iEyaT",
      "metadata": {
        "id": "b5X45s2iEyaT"
      },
      "source": [
        "4. model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "WNZNSQb1kk_f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNZNSQb1kk_f",
        "outputId": "6d5b2125-8cf6-40c3-b932-1b98184e35b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Patch embedding created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/patches.py ----------\n",
        "open(f\"{SRC}/patches.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Convert image to patches and embed them\"\"\"\n",
        "    def __init__(self, in_channels: int = 1, patch_size: int = 4, embed_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        # âœ… FIX: Add normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W) -> (B, D, H', W') -> (B, N, D)\n",
        "        x = self.proj(x)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n",
        "        x = self.norm(x)  # âœ… Normalize patches\n",
        "        return x\n",
        "''')\n",
        "\n",
        "print(\"âœ… Patch embedding created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "C-L8-YNmTMLW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-L8-YNmTMLW",
        "outputId": "5bb802f0-269c-4011-e350-0f434b275397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… RNN router created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/rnn.py ----------\n",
        "open(f\"{SRC}/rnn.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNRouter(nn.Module):\n",
        "    \"\"\"RNN with attention to select top-K important patches\"\"\"\n",
        "    def __init__(self, D: int, K: int):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.K = K\n",
        "\n",
        "        # âœ… FIX: Bidirectional LSTM for better context\n",
        "        self.rnn = nn.LSTM(D, D // 2, num_layers=2, batch_first=True,\n",
        "                          bidirectional=True, dropout=0.2)\n",
        "\n",
        "        # âœ… FIX: Multi-head attention\n",
        "        self.attn = nn.MultiheadAttention(D, num_heads=4, batch_first=True, dropout=0.2)\n",
        "\n",
        "        # Score projection\n",
        "        self.topk_proj = nn.Linear(D, 1)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # patches: (B, N, D)\n",
        "        B, N, D = patches.shape\n",
        "\n",
        "        # RNN processing\n",
        "        rnn_out, _ = self.rnn(patches)  # (B, N, D)\n",
        "\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.attn(rnn_out, rnn_out, rnn_out)  # (B, N, D)\n",
        "\n",
        "        # Compute importance scores\n",
        "        scores = self.topk_proj(attn_out).squeeze(-1)  # (B, N)\n",
        "\n",
        "        # Select top-K patches\n",
        "        topk_vals, topk_idx = torch.topk(scores, min(self.K, N), dim=1)\n",
        "\n",
        "        # Gather selected patches\n",
        "        selected = []\n",
        "        for i in range(B):\n",
        "            selected.append(attn_out[i, topk_idx[i], :])\n",
        "        selected = torch.stack(selected, dim=0)  # (B, K, D)\n",
        "\n",
        "        # Aggregate to fixed-size vector\n",
        "        kvec = selected.mean(dim=1)  # (B, D)\n",
        "\n",
        "        return kvec\n",
        "''')\n",
        "\n",
        "print(\"âœ… RNN router created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "rqaksJjPC1dw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqaksJjPC1dw",
        "outputId": "ad58042f-4cc2-4beb-f967-c9344267acb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Quantum layer created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/quantum.py ----------\n",
        "open(f\"{SRC}/quantum.py\", \"w\").write(r'''\n",
        "import pennylane as qml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class TorchQNN(nn.Module):\n",
        "    \"\"\"Optimized Quantum Neural Network\"\"\"\n",
        "    def __init__(self, K: int, Q: int = 6, L: int = 3):\n",
        "        super().__init__()\n",
        "        self.Q = Q\n",
        "        self.L = L\n",
        "\n",
        "        # Input projection\n",
        "        self.lin_in = nn.Linear(K, Q)\n",
        "\n",
        "        # âœ… FIX: Better weight initialization\n",
        "        self.q_weights = nn.Parameter(torch.empty(L, Q, 3))\n",
        "        nn.init.uniform_(self.q_weights, -np.pi/2, np.pi/2)  # Smaller range\n",
        "\n",
        "        # Use default.qubit for stability\n",
        "        self.dev = qml.device('default.qubit', wires=Q)\n",
        "\n",
        "        self.circuit = self._make_circuit()\n",
        "\n",
        "    def _make_circuit(self):\n",
        "        @qml.qnode(self.dev, interface='torch', diff_method='backprop')\n",
        "        def circuit(inputs, weights):\n",
        "            # âœ… Data encoding\n",
        "            for l in range(self.L):\n",
        "                for w in range(self.Q):\n",
        "                    qml.RX(inputs[w], wires=w)\n",
        "                    qml.RZ(0.5 * inputs[w], wires=w)\n",
        "\n",
        "                # âœ… Variational layer\n",
        "                qml.StronglyEntanglingLayers(weights[l:l+1], wires=range(self.Q))\n",
        "\n",
        "                # Entanglement\n",
        "                for w in range(self.Q - 1):\n",
        "                    qml.CNOT([w, w + 1])\n",
        "                if self.Q > 1:\n",
        "                    qml.CNOT([self.Q - 1, 0])\n",
        "\n",
        "            return [qml.expval(qml.PauliZ(w)) for w in range(self.Q)]\n",
        "\n",
        "        return circuit\n",
        "\n",
        "    def forward(self, kvec):\n",
        "        B = kvec.shape[0]\n",
        "        qinput = torch.tanh(self.lin_in(kvec))\n",
        "\n",
        "        # âœ… FIX: Process in smaller chunks\n",
        "        chunk_size = 8\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(0, B, chunk_size):\n",
        "            batch = qinput[i:i+chunk_size]\n",
        "            batch_outputs = []\n",
        "\n",
        "            for j in range(batch.shape[0]):\n",
        "                out = self.circuit(batch[j], self.q_weights)\n",
        "                out_tensor = torch.stack(out).float()\n",
        "                batch_outputs.append(out_tensor)\n",
        "\n",
        "            outputs.append(torch.stack(batch_outputs))\n",
        "\n",
        "        return torch.cat(outputs, dim=0)\n",
        "''')\n",
        "\n",
        "print(\"âœ… Quantum layer created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "FjQeIctVaeqT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjQeIctVaeqT",
        "outputId": "f6851a5d-5a12-44d3-d6ae-fd972febd5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Hybrid model created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/models.py ----------\n",
        "open(f\"{SRC}/models.py\", \"w\").write(r'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from src.patches import PatchEmbedding\n",
        "from src.rnn import RNNRouter\n",
        "from src.quantum import TorchQNN\n",
        "\n",
        "class HybridQRNN(nn.Module):\n",
        "    \"\"\"Complete Hybrid Quantum-RNN Architecture\"\"\"\n",
        "    def __init__(self, in_channels: int, num_classes: int, patch_size: int = 4,\n",
        "                 embed_dim: int = 64, K: int = 10, Q: int = 6, L: int = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch extraction\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
        "\n",
        "        # RNN-based routing\n",
        "        self.rnn = RNNRouter(embed_dim, K)\n",
        "\n",
        "        # Quantum layer\n",
        "        self.qnn = TorchQNN(embed_dim, Q, L)\n",
        "\n",
        "        # âœ… FIX: Better classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(Q, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        # âœ… FIX: Initialize classifier\n",
        "        self._init_classifier()\n",
        "\n",
        "    def _init_classifier(self):\n",
        "        for m in self.fc.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.patch_embed(x)  # (B, N, D)\n",
        "        kvec = self.rnn(patches)        # (B, D)\n",
        "        qout = self.qnn(kvec)           # (B, Q)\n",
        "        logits = self.fc(qout)          # (B, C)\n",
        "        return logits\n",
        "''')\n",
        "\n",
        "print(\"âœ… Hybrid model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "lkpcfawqwxb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkpcfawqwxb3",
        "outputId": "6251e431-8ea5-45b3-f684-273a5cd10a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training script created\n"
          ]
        }
      ],
      "source": [
        "# ---------- src/train.py ----------\n",
        "open(f\"{SRC}/train.py\", \"w\").write(r'''\n",
        "import os, argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "from src.data import MedMNISTDataset\n",
        "from src.models import HybridQRNN\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter_(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        log_prob = torch.log_softmax(pred, dim=1)\n",
        "        loss = -(one_hot * log_prob).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # âœ… Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, num_classes):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        if num_classes == 2:\n",
        "            auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    return acc, f1, auc\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True)\n",
        "    parser.add_argument(\"--Q\", type=int, default=6)\n",
        "    parser.add_argument(\"--L\", type=int, default=3)\n",
        "    parser.add_argument(\"--K\", type=int, default=10)\n",
        "    parser.add_argument(\"--hidden\", type=int, default=64)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
        "    parser.add_argument(\"--seeds\", type=int, default=5)\n",
        "    parser.add_argument(\"--out\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Load datasets\n",
        "    train_ds = MedMNISTDataset(args.dataset, split=\"train\")\n",
        "    val_ds = MedMNISTDataset(args.dataset, split=\"val\")\n",
        "    test_ds = MedMNISTDataset(args.dataset, split=\"test\")\n",
        "\n",
        "    num_classes = train_ds.n_classes\n",
        "    in_channels = train_ds[0][0].shape[0]\n",
        "\n",
        "    # âœ… Batch size = 8 for quantum circuits\n",
        "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for seed in range(args.seeds):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Seed {seed}/{args.seeds-1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        model = HybridQRNN(\n",
        "            in_channels=in_channels,\n",
        "            num_classes=num_classes,\n",
        "            patch_size=4,\n",
        "            embed_dim=args.hidden,\n",
        "            K=args.K,\n",
        "            Q=args.Q,\n",
        "            L=args.L\n",
        "        ).to(device)\n",
        "\n",
        "        criterion = LabelSmoothingCE(smoothing=0.1)\n",
        "\n",
        "        # âœ… FIX: Lower LR + AdamW\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "\n",
        "        # âœ… FIX: Warmup + Cosine schedule\n",
        "        warmup_epochs = 5\n",
        "        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=0.1, total_iters=warmup_epochs\n",
        "        )\n",
        "        main_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=args.epochs - warmup_epochs, eta_min=1e-6\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "            optimizer,\n",
        "            schedulers=[warmup_scheduler, main_scheduler],\n",
        "            milestones=[warmup_epochs]\n",
        "        )\n",
        "\n",
        "        best_f1 = 0\n",
        "        patience = 15  # âœ… Increased patience\n",
        "        wait = 0\n",
        "\n",
        "        for epoch in range(1, args.epochs + 1):\n",
        "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_acc, val_f1, val_auc = evaluate(model, val_loader, device, num_classes)\n",
        "\n",
        "            print(f\"[seed {seed}] epoch {epoch:02d}  train_loss={train_loss:.4f}  \"\n",
        "                  f\"val_f1={val_f1:.4f}  val_acc={val_acc:.4f}  wait={wait}/{patience}\")\n",
        "\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                wait = 0\n",
        "                torch.save(model.state_dict(), f\"{args.out}/best_seed{seed}.pt\")\n",
        "            else:\n",
        "                wait += 1\n",
        "\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test\n",
        "        model.load_state_dict(torch.load(f\"{args.out}/best_seed{seed}.pt\"))\n",
        "        test_acc, test_f1, test_auc = evaluate(model, test_loader, device, num_classes)\n",
        "\n",
        "        results.append({\n",
        "            'seed': seed,\n",
        "            'test_acc': test_acc,\n",
        "            'test_f1': test_f1,\n",
        "            'test_auc': test_auc\n",
        "        })\n",
        "\n",
        "        print(f\"\\n[Seed {seed}] Test: ACC={test_acc:.4f}, F1={test_f1:.4f}, AUC={test_auc:.4f}\")\n",
        "\n",
        "    # Summary\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(f\"{args.out}/results.csv\", index=False)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Final Results ({args.seeds} seeds)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ACC: {df['test_acc'].mean():.4f} Â± {df['test_acc'].std():.4f}\")\n",
        "    print(f\"F1: {df['test_f1'].mean():.4f} Â± {df['test_f1'].std():.4f}\")\n",
        "    print(f\"AUC: {df['test_auc'].mean():.4f} Â± {df['test_auc'].std():.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"âœ… Training script created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NUR81WsKw1zj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUR81WsKw1zj",
        "outputId": "686cabb4-0449-4b5a-ada1-9018ca95f94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hybridqnn_seq\n",
            "\n",
            "======================================================================\n",
            "ðŸ”¬ Training BloodMNIST â†’ runs/BloodMNIST_Q6L3_K10_H64_20251026_193607\n",
            "======================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.2 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
            "  warnings.warn(\n",
            "Device: cpu\n",
            "100% 35.5M/35.5M [00:01<00:00, 20.1MB/s]\n",
            "\n",
            "============================================================\n",
            "Seed 0/4\n",
            "============================================================\n",
            "[seed 0] epoch 01  train_loss=1.9454  val_f1=0.3755  val_acc=0.4755  wait=0/15\n",
            "[seed 0] epoch 02  train_loss=1.5271  val_f1=0.3821  val_acc=0.4638  wait=0/15\n",
            "[seed 0] epoch 03  train_loss=1.2164  val_f1=0.6812  val_acc=0.7068  wait=0/15\n",
            "[seed 0] epoch 04  train_loss=1.0737  val_f1=0.7610  val_acc=0.7623  wait=0/15\n",
            "[seed 0] epoch 05  train_loss=1.0026  val_f1=0.7829  val_acc=0.7886  wait=0/15\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "[seed 0] epoch 06  train_loss=0.9602  val_f1=0.7310  val_acc=0.7272  wait=0/15\n",
            "[seed 0] epoch 07  train_loss=0.9149  val_f1=0.7299  val_acc=0.7477  wait=1/15\n",
            "[seed 0] epoch 08  train_loss=0.8810  val_f1=0.8232  val_acc=0.8283  wait=2/15\n",
            "[seed 0] epoch 09  train_loss=0.8511  val_f1=0.8737  val_acc=0.8721  wait=0/15\n",
            "[seed 0] epoch 10  train_loss=0.8239  val_f1=0.8318  val_acc=0.8435  wait=0/15\n",
            "[seed 0] epoch 11  train_loss=0.7874  val_f1=0.8708  val_acc=0.8715  wait=1/15\n",
            "[seed 0] epoch 12  train_loss=0.7769  val_f1=0.8570  val_acc=0.8604  wait=2/15\n",
            "[seed 0] epoch 13  train_loss=0.7638  val_f1=0.8165  val_acc=0.8137  wait=3/15\n",
            "[seed 0] epoch 14  train_loss=0.7418  val_f1=0.8860  val_acc=0.8884  wait=4/15\n",
            "[seed 0] epoch 15  train_loss=0.7355  val_f1=0.8834  val_acc=0.8820  wait=0/15\n",
            "[seed 0] epoch 16  train_loss=0.7237  val_f1=0.8994  val_acc=0.8984  wait=1/15\n",
            "[seed 0] epoch 17  train_loss=0.7119  val_f1=0.9009  val_acc=0.9030  wait=0/15\n",
            "[seed 0] epoch 18  train_loss=0.7009  val_f1=0.8909  val_acc=0.8908  wait=0/15\n",
            "[seed 0] epoch 19  train_loss=0.6884  val_f1=0.9115  val_acc=0.9118  wait=1/15\n",
            "[seed 0] epoch 20  train_loss=0.6823  val_f1=0.9026  val_acc=0.9036  wait=0/15\n",
            "[seed 0] epoch 21  train_loss=0.6700  val_f1=0.9062  val_acc=0.9060  wait=1/15\n",
            "[seed 0] epoch 22  train_loss=0.6698  val_f1=0.9272  val_acc=0.9270  wait=2/15\n",
            "[seed 0] epoch 23  train_loss=0.6537  val_f1=0.9060  val_acc=0.9071  wait=0/15\n",
            "[seed 0] epoch 24  train_loss=0.6467  val_f1=0.9177  val_acc=0.9176  wait=1/15\n",
            "[seed 0] epoch 25  train_loss=0.6413  val_f1=0.9174  val_acc=0.9176  wait=2/15\n",
            "[seed 0] epoch 26  train_loss=0.6304  val_f1=0.9124  val_acc=0.9136  wait=3/15\n",
            "[seed 0] epoch 27  train_loss=0.6243  val_f1=0.9206  val_acc=0.9206  wait=4/15\n",
            "Training:  68% 1014/1495 [14:52<06:41,  1.20it/s]"
          ]
        }
      ],
      "source": [
        "%cd /content/hybridqnn_seq\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# âœ… OPTIMIZED CONFIG: Original architecture with fixes\n",
        "CONFIGS = [\n",
        "    # (DATASET,       Q,  L,  K, HIDDEN, EPOCHS, SEEDS)\n",
        "    (\"BloodMNIST\",   6,  3,  10,   64,    50,     5),\n",
        "]\n",
        "\n",
        "for DATASET, Q, L, K, HIDDEN, EPOCHS, SEEDS in CONFIGS:\n",
        "    OUTDIR = f\"runs/{DATASET}_Q{Q}L{L}_K{K}_H{HIDDEN}_{TS}\"\n",
        "    os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ”¬ Training {DATASET} â†’ {OUTDIR}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    !python -m src.train \\\n",
        "        --dataset {DATASET} \\\n",
        "        --Q {Q} --L {L} --K {K} --hidden {HIDDEN} \\\n",
        "        --epochs {EPOCHS} --seeds {SEEDS} \\\n",
        "        --out {OUTDIR}\n",
        "\n",
        "    print(f\"âœ… Completed: {OUTDIR}\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ðŸŽ‰ Training complete!\")\n",
        "print(f\"{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}