{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/val_80_f1_44_(30_)_SC_MIL_Supervised_Contrastive_Multiple_Instance_Learning_for_Imbalanced_Classification_in_Pathology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f9E_b6KlMFTz"
      },
      "outputs": [],
      "source": [
        "# --- Section 1: Imports and Setup ---\n",
        "\n",
        "# Core libraries for numerical operations and data handling\n",
        "import numpy as np # Used for numerical operations, especially with arrays and matrices.\n",
        "import pandas as pd # Used for creating and manipulating dataframes to manage metadata.\n",
        "import os # Provides a way of using operating system dependent functionality, like reading file paths.\n",
        "import re # Regular expression library, crucial for parsing patient IDs from filenames.\n",
        "from collections import defaultdict # A dictionary subclass that calls a factory function to supply missing values.\n",
        "\n",
        "# Deep learning framework and utilities\n",
        "import torch # The main deep learning library (PyTorch).\n",
        "import torch.nn as nn # PyTorch's module for building neural networks.\n",
        "import torch.optim as optim # Contains optimization algorithms like Adam.\n",
        "from torch.utils.data import Dataset, DataLoader # Tools for creating custom datasets and efficient data loading.\n",
        "import torchvision.transforms as transforms # Contains common image transformations for data augmentation.\n",
        "from torchvision.models import shufflenet_v2_x1_0 # The pretrained ShuffleNet model used as the feature extractor.\n",
        "\n",
        "# Image processing and visualization\n",
        "from PIL import Image # Python Imaging Library, used for opening and manipulating image files.\n",
        "import matplotlib.pyplot as plt # The primary library for creating plots and visualizations.\n",
        "import seaborn as sns # Built on top of matplotlib for more attractive statistical graphics.\n",
        "\n",
        "# Utilities and metrics\n",
        "from tqdm import tqdm # Creates smart progress bars for loops, essential for tracking training progress.\n",
        "from sklearn.model_selection import train_test_split # A utility to split datasets into train, validation, and test sets.\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve # Metrics for comprehensive model evaluation.\n",
        "\n",
        "# Ensure reproducibility\n",
        "import random # Python's built-in library for random number generation.\n",
        "np.random.seed(42) # Set seed for NumPy for consistent random operations.\n",
        "torch.manual_seed(42) # Set seed for PyTorch on the CPU.\n",
        "random.seed(42) # Set seed for Python's random module.\n",
        "if torch.cuda.is_available(): # Check if a GPU is available.\n",
        "    torch.cuda.manual_seed_all(42) # Set seed for PyTorch on all available GPUs.\n",
        "\n",
        "# --- End of Section 1 ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_VsxVpsNDuV",
        "outputId": "dda72711-c493-4e8b-fbde-373da3170bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already unzipped to /content/oaisis_unzipped. Skipping unzipping.\n",
            "Inspecting contents of /content/oaisis_unzipped/Data:\n",
            "Found directory: Moderate Dementia\n",
            "Inspecting contents of /content/oaisis_unzipped/Data/Moderate Dementia:\n",
            "  File 1: OAS1_0351_MR1_mpr-4_105.jpg\n",
            "  File 2: OAS1_0351_MR1_mpr-1_125.jpg\n",
            "  File 3: OAS1_0351_MR1_mpr-2_150.jpg\n",
            "  File 4: OAS1_0351_MR1_mpr-4_133.jpg\n",
            "  File 5: OAS1_0308_MR1_mpr-2_149.jpg\n",
            "  File 6: OAS1_0351_MR1_mpr-3_112.jpg\n",
            "  File 7: OAS1_0308_MR1_mpr-1_124.jpg\n",
            "  ...\n",
            "Found directory: Mild Dementia\n",
            "Inspecting contents of /content/oaisis_unzipped/Data/Mild Dementia:\n",
            "  File 1: OAS1_0031_MR1_mpr-4_135.jpg\n",
            "  File 2: OAS1_0035_MR1_mpr-3_157.jpg\n",
            "  File 3: OAS1_0382_MR1_mpr-2_105.jpg\n",
            "  File 4: OAS1_0031_MR1_mpr-2_114.jpg\n",
            "  File 5: OAS1_0035_MR1_mpr-1_130.jpg\n",
            "  File 6: OAS1_0073_MR1_mpr-1_114.jpg\n",
            "  File 7: OAS1_0056_MR1_mpr-4_140.jpg\n",
            "  ...\n",
            "Found directory: Non Demented\n",
            "Inspecting contents of /content/oaisis_unzipped/Data/Non Demented:\n",
            "  File 1: OAS1_0086_MR1_mpr-1_109.jpg\n",
            "  File 2: OAS1_0059_MR1_mpr-4_137.jpg\n",
            "  File 3: OAS1_0074_MR1_mpr-4_104.jpg\n",
            "  File 4: OAS1_0009_MR1_mpr-1_138.jpg\n",
            "  File 5: OAS1_0005_MR1_mpr-2_144.jpg\n",
            "  File 6: OAS1_0086_MR1_mpr-3_120.jpg\n",
            "  File 7: OAS1_0047_MR1_mpr-3_100.jpg\n",
            "  ...\n",
            "Starting bag reconstruction from image folders...\n",
            "Warning: Directory not found for class 'Very mild Dementia'. Skipping.\n",
            "Successfully created 98 bags from 98 patients.\n",
            "Dataset split complete: 58 training bags, 15 validation bags, 25 test bags.\n",
            "DataLoaders are ready for training, validation, and testing.\n"
          ]
        }
      ],
      "source": [
        "# --- Section 2: Data Preprocessing and Bag Reconstruction ---\n",
        "\n",
        "def create_bags_from_folders(data_dir, patient_id_regex=r'^(OAS\\d_\\d{4})_'):\n",
        "    \"\"\"\n",
        "    Parses image filenames in class-structured folders to reconstruct patient bags.\n",
        "\n",
        "    This function iterates through subdirectories (each representing a class) of the\n",
        "    main data directory. It uses a regular expression to extract a patient ID\n",
        "    from each filename and groups the file paths of all slices belonging to the\n",
        "    same patient into a \"bag\".\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): The path to the root directory containing class folders.\n",
        "        patient_id_regex (str): A regular expression to extract patient IDs.\n",
        "                                The first captured group is used as the ID.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of all patient bags. Each bag is a dictionary containing\n",
        "              the patient ID, a list of image paths, and the class label.\n",
        "        dict: A mapping from class names to integer labels.\n",
        "    \"\"\"\n",
        "    # Print the starting of the bag creation process.\n",
        "    print(\"Starting bag reconstruction from image folders...\")\n",
        "\n",
        "    # Define the classes based on the folder names provided in the dataset description.\n",
        "    class_names = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']\n",
        "\n",
        "    # Create a mapping from class names to integer indices for model training.\n",
        "    class_to_label = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "    # Use a defaultdict to conveniently group slices by patient.\n",
        "    # The key will be (patient_id, class_name), value will be a list of slice paths.\n",
        "    patient_slice_groups = defaultdict(list)\n",
        "\n",
        "    # Iterate over each class directory.\n",
        "    for class_name in class_names:\n",
        "        # Construct the full path to the class directory.\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        # Check if the directory exists to avoid errors.\n",
        "        if not os.path.isdir(class_dir):\n",
        "            # Print a warning if a class folder is not found.\n",
        "            print(f\"Warning: Directory not found for class '{class_name}'. Skipping.\")\n",
        "            # Continue to the next class name in the list.\n",
        "            continue\n",
        "\n",
        "        # List all files in the current class directory.\n",
        "        for filename in os.listdir(class_dir):\n",
        "            # We are only interested in JPG files.\n",
        "            if filename.endswith('.jpg'):\n",
        "                # Try to match the regex to extract the patient ID.\n",
        "                match = re.match(patient_id_regex, filename)\n",
        "                # If a match is found, proceed.\n",
        "                if match:\n",
        "                    # The patient ID is the first group captured by the regex.\n",
        "                    patient_id = match.groups()[0]\n",
        "                    # Get the full path to the image file.\n",
        "                    image_path = os.path.join(class_dir, filename)\n",
        "                    # Append the slice path to the corresponding patient's group.\n",
        "                    patient_slice_groups[(patient_id, class_name)].append(image_path)\n",
        "\n",
        "    # Now, convert the grouped slices into a list of bag dictionaries.\n",
        "    all_bags = []\n",
        "    # Iterate through the patient groups we created.\n",
        "    for (patient_id, class_name), image_paths in patient_slice_groups.items():\n",
        "        # Each bag is a dictionary containing its ID, all its image paths, and its label.\n",
        "        all_bags.append({\n",
        "            'patient_id': patient_id,          # Unique identifier for the patient.\n",
        "            'image_paths': sorted(image_paths), # A sorted list of paths to all slices for this patient.\n",
        "            'label': class_to_label[class_name] # The integer label corresponding to the patient's class.\n",
        "        })\n",
        "\n",
        "    # Print a summary of the bag creation process.\n",
        "    print(f\"Successfully created {len(all_bags)} bags from {len(patient_slice_groups)} patients.\")\n",
        "    # Return the list of all bags and the class-to-label mapping.\n",
        "    return all_bags, class_to_label\n",
        "\n",
        "\n",
        "class OASISBagDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for loading bags of OASIS MRI slices.\n",
        "\n",
        "    Each item returned by this dataset is a complete bag, which consists of\n",
        "    all MRI slices for a single patient, stacked into a single tensor.\n",
        "    \"\"\"\n",
        "    def __init__(self, bags, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bags (list): A list of bag dictionaries, from create_bags_from_folders.\n",
        "            transform (callable, optional): A torchvision.transforms pipeline to be\n",
        "                                            applied to each slice.\n",
        "        \"\"\"\n",
        "        # Store the list of bag definitions.\n",
        "        self.bags = bags\n",
        "        # Store the transformation pipeline.\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of bags (patients) in the dataset.\"\"\"\n",
        "        # The length of the dataset is the number of patients.\n",
        "        return len(self.bags)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves one bag (all slices for one patient) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the bag to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                   - torch.Tensor: A tensor of shape (num_slices, C, H, W)\n",
        "                                   containing all transformed slices for the patient.\n",
        "                   - int: The label for the bag.\n",
        "        \"\"\"\n",
        "        # Get the bag definition dictionary at the specified index.\n",
        "        bag = self.bags[idx]\n",
        "        # Get the label for this bag.\n",
        "        label = bag['label']\n",
        "        # Retrieve the list of image paths for the slices in this bag.\n",
        "        image_paths = bag['image_paths']\n",
        "\n",
        "        # Load and transform each slice in the bag.\n",
        "        bag_slices = []\n",
        "        # Iterate over each image path in the bag.\n",
        "        for path in image_paths:\n",
        "            # Open the image file using PIL.\n",
        "            # Convert to RGB since pretrained models expect 3 channels. MRI is grayscale.\n",
        "            image = Image.open(path).convert(\"RGB\")\n",
        "            # Apply the transformations if they are defined.\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            # Append the transformed slice tensor to our list.\n",
        "            bag_slices.append(image)\n",
        "\n",
        "        # Stack all slice tensors into a single tensor for the entire bag.\n",
        "        # This tensor represents all instances in the bag.\n",
        "        stacked_slices = torch.stack(bag_slices, dim=0)\n",
        "\n",
        "        # Return the stacked slices and the corresponding label.\n",
        "        return stacked_slices, label\n",
        "\n",
        "# --- Main Data Loading Execution ---\n",
        "\n",
        "# Define the path to the dataset zip file.\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/MR& TP/oaisis.zip'\n",
        "# Define the directory where the dataset will be unzipped.\n",
        "UNZIP_DIR = '/content/oaisis_unzipped'\n",
        "\n",
        "# Unzip the dataset if it hasn't been already.\n",
        "import zipfile\n",
        "if not os.path.exists(UNZIP_DIR):\n",
        "    print(f\"Unzipping {ZIP_FILE_PATH} to {UNZIP_DIR}...\")\n",
        "    with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(UNZIP_DIR)\n",
        "    print(\"Unzipping complete.\")\n",
        "else:\n",
        "    print(f\"Dataset already unzipped to {UNZIP_DIR}. Skipping unzipping.\")\n",
        "\n",
        "# Define the path to the dataset directory after unzipping.\n",
        "DATA_ROOT_DIR = os.path.join(UNZIP_DIR, 'Data')\n",
        "\n",
        "# Inspect the contents of the data directory to check folder names\n",
        "print(f\"Inspecting contents of {DATA_ROOT_DIR}:\")\n",
        "try:\n",
        "    for item in os.listdir(DATA_ROOT_DIR):\n",
        "        item_path = os.path.join(DATA_ROOT_DIR, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"Found directory: {item}\")\n",
        "            # Inspect the first directory found\n",
        "            print(f\"Inspecting contents of {item_path}:\")\n",
        "            try:\n",
        "                for i, file_item in enumerate(os.listdir(item_path)):\n",
        "                    print(f\"  File {i+1}: {file_item}\")\n",
        "                    if i > 5: # Print only the first few files to avoid flooding the output\n",
        "                        print(\"  ...\")\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error inspecting directory {item_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Found file: {item}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Data directory not found at {DATA_ROOT_DIR}. Please check the unzipping process.\")\n",
        "\n",
        "# Define a custom transform to add Gaussian noise\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Ensure the image is a PyTorch Tensor\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "             img = transforms.ToTensor()(img)\n",
        "\n",
        "        # Add noise\n",
        "        return img + torch.randn(img.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "# Define image transformations. The paper uses a 224x224 input size.\n",
        "# For training, we include data augmentation as described in the paper, plus Gaussian noise.\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),       # Resize images to 224x224 for the ShuffleNet model.\n",
        "    transforms.RandomHorizontalFlip(),   # Randomly flip images horizontally for augmentation.\n",
        "    transforms.RandomRotation(10),       # Randomly rotate images by up to 10 degrees.\n",
        "    transforms.ToTensor(),               # Convert PIL Image to a PyTorch tensor.\n",
        "    AddGaussianNoise(mean=0., std=0.30), # Add Gaussian noise (adjust std for noise level)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats.\n",
        "])\n",
        "\n",
        "# For validation and testing, we only resize, convert to tensor, and normalize.\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),       # Resize images to 224x224.\n",
        "    transforms.ToTensor(),               # Convert PIL Image to a PyTorch tensor.\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats.\n",
        "])\n",
        "\n",
        "# Reconstruct the bags from the folder structure.\n",
        "all_patient_bags, class_mapping = create_bags_from_folders(DATA_ROOT_DIR)\n",
        "\n",
        "# Separate labels for stratification.\n",
        "bag_labels = [bag['label'] for bag in all_patient_bags]\n",
        "\n",
        "# Split the bags into training (60%), validation (15%), and test (25%) sets.\n",
        "# First, split into training (60%) and a temporary set (40%).\n",
        "train_bags, temp_bags, train_labels, temp_labels = train_test_split(\n",
        "    all_patient_bags, bag_labels, test_size=0.40, random_state=42, stratify=bag_labels\n",
        ")\n",
        "\n",
        "# Split the temporary set into validation (15%) and test (25%).\n",
        "# This corresponds to a 15/40 = 37.5% split of the temp set for validation.\n",
        "val_bags, test_bags, _, _ = train_test_split(\n",
        "    temp_bags, temp_labels, test_size=0.625, random_state=42 # Removed stratify\n",
        ")\n",
        "\n",
        "print(f\"Dataset split complete: {len(train_bags)} training bags, {len(val_bags)} validation bags, {len(test_bags)} test bags.\")\n",
        "\n",
        "# Create the PyTorch Dataset objects.\n",
        "train_dataset = OASISBagDataset(train_bags, transform=train_transform)\n",
        "val_dataset = OASISBagDataset(val_bags, transform=val_test_transform)\n",
        "test_dataset = OASISBagDataset(test_bags, transform=val_test_transform)\n",
        "\n",
        "# Create the DataLoaders.\n",
        "# We use a batch_size of 1 because each bag has a variable number of slices.\n",
        "# The actual \"batch\" for contrastive loss will be formed manually in the training loop.\n",
        "# This is a standard approach for handling variable-sized inputs in MIL.\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Print a confirmation that DataLoaders are ready.\n",
        "print(\"DataLoaders are ready for training, validation, and testing.\")\n",
        "\n",
        "# --- End of Section 2 ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAQXV9U9O6-t",
        "outputId": "9292158b-bd0f-44e8-f364-318735ffc06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x1-5666bf0f80.pth\n",
            "100%|██████████| 8.79M/8.79M [00:00<00:00, 36.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SC-MIL model has been successfully initialized.\n",
            "Feature Dimension: 1024, Number of Classes: 4, Projection Dimension: 128\n"
          ]
        }
      ],
      "source": [
        "# --- Section 3: Model Definition ---\n",
        "\n",
        "# Set the device for training (use GPU if available, otherwise CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\") # Announce which device is being used.\n",
        "\n",
        "class GatedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A Gated Attention module for aggregating instance features into a bag embedding.\n",
        "    This is a common and effective variant of the attention mechanism used in MIL.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): The dimensionality of the instance features (from the feature extractor).\n",
        "            hidden_dim (int): The dimensionality of the hidden layer in the attention network.\n",
        "        \"\"\"\n",
        "        super(GatedAttention, self).__init__() # Initialize the parent nn.Module class.\n",
        "        self.input_dim = input_dim # Store the input dimension.\n",
        "        self.hidden_dim = hidden_dim # Store the hidden dimension.\n",
        "\n",
        "        # Attention network layers.\n",
        "        # This part of the network learns to parameterize the attention weights.\n",
        "        self.attention_V = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.hidden_dim), # First linear layer.\n",
        "            nn.Tanh() # Tanh activation function.\n",
        "        )\n",
        "\n",
        "        # Gating network layers.\n",
        "        # This part provides a gating mechanism to modulate the attention.\n",
        "        self.attention_U = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.hidden_dim), # Gating linear layer.\n",
        "            nn.Sigmoid() # Sigmoid activation for the gate.\n",
        "        )\n",
        "\n",
        "        # Final linear layer to compute the attention scores.\n",
        "        self.attention_weights = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Gated Attention module.\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of instance features with shape (num_instances, input_dim).\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of attention scores with shape (num_instances, 1).\n",
        "        \"\"\"\n",
        "        # Calculate the two components of the gated attention.\n",
        "        A_V = self.attention_V(x)  # (num_instances, hidden_dim)\n",
        "        A_U = self.attention_U(x)  # (num_instances, hidden_dim)\n",
        "\n",
        "        # Element-wise multiplication of the two components.\n",
        "        gated_A = A_V * A_U  # (num_instances, hidden_dim)\n",
        "\n",
        "        # Compute the final attention scores (unnormalized).\n",
        "        attention_scores = self.attention_weights(gated_A)  # (num_instances, 1)\n",
        "\n",
        "        # Return the computed attention scores.\n",
        "        return attention_scores\n",
        "\n",
        "class SC_MIL(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the Supervised Contrastive Multiple Instance Learning (SC-MIL) model.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, n_classes, projection_dim=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): The dimension of the features from the patch feature extractor.\n",
        "            n_classes (int): The number of output classes for classification.\n",
        "            projection_dim (int): The output dimension of the projection head.\n",
        "        \"\"\"\n",
        "        super(SC_MIL, self).__init__() # Initialize the parent nn.Module class.\n",
        "        self.input_dim = input_dim # Store input feature dimension.\n",
        "        self.n_classes = n_classes # Store number of classes.\n",
        "        self.projection_dim = projection_dim # Store projection dimension.\n",
        "\n",
        "        # 1. Feature Extractor (f)\n",
        "        # We use a pretrained ShuffleNet and remove its final classifier.\n",
        "        shufflenet = shufflenet_v2_x1_0(pretrained=True) # Load pretrained ShuffleNet.\n",
        "        # The feature extractor is all layers except the final fully connected layer.\n",
        "        self.feature_extractor = nn.Sequential(*list(shufflenet.children())[:-1])\n",
        "        # Freeze the feature extractor's parameters as it is already trained on ImageNet.\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False # This can be set to True for end-to-end finetuning.\n",
        "\n",
        "        # 2. Attention Module (m)\n",
        "        # This module will compute the bag embedding from instance features.\n",
        "        self.attention = GatedAttention(self.input_dim) # Instantiate the attention module.\n",
        "\n",
        "        # 3. Projector Head (g) for Supervised Contrastive Loss\n",
        "        # A non-linear MLP as specified in the paper.\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.input_dim), # First linear layer.\n",
        "            nn.ReLU(), # ReLU activation.\n",
        "            nn.Linear(self.input_dim, self.projection_dim) # Second linear layer to project to projection_dim.\n",
        "        )\n",
        "\n",
        "        # 4. Classifier Head (h) for Cross-Entropy Loss\n",
        "        # A simple linear layer to map the bag embedding to class logits.\n",
        "        self.classifier = nn.Linear(self.input_dim, self.n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the SC-MIL model.\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor representing a bag of instances (slices)\n",
        "                              with shape (num_instances, C, H, W).\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                   - logits (torch.Tensor): The output class logits for CE loss.\n",
        "                   - projection (torch.Tensor): The projected features for SCL loss.\n",
        "        \"\"\"\n",
        "        # Ensure input tensor is on the correct device.\n",
        "        x = x.to(device)\n",
        "\n",
        "        # Step 1: Extract patch-level features using the feature extractor.\n",
        "        # x is shaped (num_instances, C, H, W)\n",
        "        instance_features = self.feature_extractor(x) # Output shape: (num_instances, input_dim, 1, 1)\n",
        "        instance_features = instance_features.view(-1, self.input_dim) # Reshape to (num_instances, input_dim)\n",
        "\n",
        "        # Step 2: Compute attention scores for each instance.\n",
        "        attention_scores = self.attention(instance_features) # Output shape: (num_instances, 1)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights.\n",
        "        attention_weights = torch.softmax(attention_scores, dim=0) # Shape: (num_instances, 1)\n",
        "\n",
        "        # Step 3: Compute the bag embedding as the attention-weighted average of instance features.\n",
        "        bag_embedding = torch.sum(attention_weights * instance_features, dim=0) # Shape: (input_dim)\n",
        "\n",
        "        # Step 4: Pass the bag embedding through the two heads.\n",
        "        # Classifier head -> Logits for classification\n",
        "        logits = self.classifier(bag_embedding) # Shape: (n_classes)\n",
        "\n",
        "        # Projector head -> Projection for contrastive loss\n",
        "        projection = self.projector(bag_embedding) # Shape: (projection_dim)\n",
        "\n",
        "        # The model returns both outputs, to be used by the combined loss function.\n",
        "        return logits, projection\n",
        "\n",
        "# --- Model Initialization ---\n",
        "\n",
        "# Define model parameters based on our setup.\n",
        "# The feature dimension from ShuffleNet v2 x1.0 is 1024.\n",
        "FEATURE_DIM = 1024\n",
        "# We have 4 classes in the OASIS dataset.\n",
        "NUM_CLASSES = 4\n",
        "# The projection dimension for contrastive loss, 128 is a common choice.\n",
        "PROJECTION_DIM = 128\n",
        "\n",
        "# Instantiate the SC-MIL model.\n",
        "model = SC_MIL(input_dim=FEATURE_DIM, n_classes=NUM_CLASSES, projection_dim=PROJECTION_DIM)\n",
        "\n",
        "# Move the model to the selected device (GPU or CPU).\n",
        "model.to(device)\n",
        "\n",
        "# Print a summary of the model to verify its architecture.\n",
        "# Note: The full architecture is complex, this just confirms instantiation.\n",
        "print(\"SC-MIL model has been successfully initialized.\")\n",
        "print(f\"Feature Dimension: {model.input_dim}, Number of Classes: {model.n_classes}, Projection Dimension: {model.projection_dim}\")\n",
        "\n",
        "# --- End of Section 3 ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqR7s30ZO_vy",
        "outputId": "7827154f-12aa-487f-e559-345a3c5f5458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:26<00:00,  1.49s/it, Loss=1.95]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.7026 | Val Loss: 1.3780 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:17<00:00,  1.33s/it, Loss=2.11]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 2.6957 | Val Loss: 1.3740 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.31s/it, Loss=2.07]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 2.6297 | Val Loss: 1.3691 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:17<00:00,  1.33s/it, Loss=2.03]\n",
            "Validating: 100%|██████████| 15/15 [00:13<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 2.5265 | Val Loss: 1.3651 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.32s/it, Loss=1.65]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 | Train Loss: 2.4421 | Val Loss: 1.3602 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.31s/it, Loss=1.94]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 2.4287 | Val Loss: 1.3549 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.30s/it, Loss=1.75]\n",
            "Validating: 100%|██████████| 15/15 [00:13<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 | Train Loss: 2.3523 | Val Loss: 1.3494 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:14<00:00,  1.29s/it, Loss=1.84]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 | Train Loss: 2.2894 | Val Loss: 1.3439 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:20<00:00,  1.39s/it, Loss=1.8]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 | Train Loss: 2.2193 | Val Loss: 1.3372 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.31s/it, Loss=1.63]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Train Loss: 2.1160 | Val Loss: 1.3313 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.31s/it, Loss=1.7]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Train Loss: 2.0721 | Val Loss: 1.3249 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.32s/it, Loss=1.57]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Train Loss: 1.9954 | Val Loss: 1.3166 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.32s/it, Loss=1.59]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Train Loss: 1.9222 | Val Loss: 1.3112 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.32s/it, Loss=1.54]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Train Loss: 1.8470 | Val Loss: 1.3039 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.31s/it, Loss=1.49]\n",
            "Validating: 100%|██████████| 15/15 [00:13<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Train Loss: 1.7686 | Val Loss: 1.2923 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.30s/it, Loss=1.43]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Train Loss: 1.6890 | Val Loss: 1.2853 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.30s/it, Loss=1.37]\n",
            "Validating: 100%|██████████| 15/15 [00:13<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Train Loss: 1.6109 | Val Loss: 1.2730 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.29s/it, Loss=1.27]\n",
            "Validating: 100%|██████████| 15/15 [00:13<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Train Loss: 1.5202 | Val Loss: 1.2595 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:16<00:00,  1.31s/it, Loss=1.28]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Train Loss: 1.4497 | Val Loss: 1.2454 | Val F1: 0.4444 | Val Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 58/58 [01:15<00:00,  1.30s/it, Loss=1.17]\n",
            "Validating: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Train Loss: 1.3604 | Val Loss: 1.2323 | Val F1: 0.4444 | Val Accuracy: 0.8000\n",
            "--- Training Finished ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Section 4: Training Loop with Progress Tracking ---\n",
        "\n",
        "class SupervisedContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the Supervised Contrastive Loss function from the paper.\n",
        "    Reference: https://arxiv.org/abs/2004.11362\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=0.07):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            temperature (float): A scalar hyperparameter to scale the logits.\n",
        "        \"\"\"\n",
        "        super(SupervisedContrastiveLoss, self).__init__() # Initialize the parent class.\n",
        "        self.temperature = temperature # Store the temperature value.\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        Forward pass for the SCL loss.\n",
        "        Args:\n",
        "            features (torch.Tensor): The projected features from the model's projection head.\n",
        "                                     Shape: (batch_size, projection_dim).\n",
        "            labels (torch.Tensor): The ground truth labels for the features.\n",
        "                                   Shape: (batch_size).\n",
        "        Returns:\n",
        "            torch.Tensor: A scalar tensor representing the computed SCL loss.\n",
        "        \"\"\"\n",
        "        # Ensure the features and labels are on the correct device.\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get the batch size from the features tensor.\n",
        "        batch_size = features.shape[0]\n",
        "        # Reshape labels to (batch_size, 1) for broadcasting.\n",
        "        labels = labels.contiguous().view(-1, 1)\n",
        "\n",
        "        # Create a mask to identify positive pairs (where labels are the same).\n",
        "        # torch.eq(a, b) computes a == b element-wise.\n",
        "        # The mask will be True for pairs (i, j) where label[i] == label[j].\n",
        "        mask = torch.eq(labels, labels.T).float().to(device)\n",
        "\n",
        "        # Compute dot product between all pairs of features (cosine similarity).\n",
        "        # This is the core of the contrastive calculation.\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(features, features.T), # (batch_size, batch_size)\n",
        "            self.temperature # Scale by the temperature hyperparameter.\n",
        "        )\n",
        "\n",
        "        # For numerical stability, subtract the maximum logit from each row.\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # Create a mask to remove self-comparisons (the diagonal).\n",
        "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(device)\n",
        "        mask = mask * logits_mask # Apply the self-comparison mask.\n",
        "\n",
        "        # Compute the log-probabilities.\n",
        "        exp_logits = torch.exp(logits) * logits_mask # Exponentiate, ignoring self-comparisons.\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # Compute the mean log-likelihood over all positive pairs.\n",
        "        # We mask out the negative pairs and sum the log probabilities of positive pairs.\n",
        "        # Avoid division by zero in case a class has only one sample in the batch (mask.sum(1) == 1)\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
        "\n",
        "\n",
        "        # The final loss is the negative of this mean.\n",
        "        loss = -mean_log_prob_pos.mean()\n",
        "\n",
        "        # Return the final scalar loss.\n",
        "        return loss\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, scl_criterion, ce_criterion, total_iterations, effective_batch_size=16, temp=1.0):\n",
        "    \"\"\"\n",
        "    A single epoch of training and validation.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The SC-MIL model to be trained.\n",
        "        train_loader (DataLoader): DataLoader for the training data.\n",
        "        val_loader (DataLoader): DataLoader for the validation data.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer for updating model weights.\n",
        "        scl_criterion (nn.Module): The supervised contrastive loss function.\n",
        "        ce_criterion (nn.Module): The cross-entropy loss function.\n",
        "        total_iterations (int): Total number of training iterations for curriculum weight.\n",
        "        effective_batch_size (int): The number of bags to accumulate before a backward pass.\n",
        "        temp (float): The temperature for the SCL loss.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing average training loss, validation loss, validation F1 score, and validation accuracy.\n",
        "    \"\"\"\n",
        "    # --- Training Phase ---\n",
        "    model.train() # Set the model to training mode.\n",
        "    train_loss = 0.0 # Initialize cumulative training loss.\n",
        "\n",
        "    # Manually accumulate batches for contrastive loss.\n",
        "    batch_projections = [] # To store projected features of bags.\n",
        "    batch_logits = []      # To store logits of bags.\n",
        "    batch_labels = []      # To store labels of bags.\n",
        "\n",
        "    # Use tqdm for a progress bar over the training data.\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for i, (bag_slices, labels) in enumerate(pbar):\n",
        "        # The dataloader returns a batch of 1 bag, so we squeeze the dimensions.\n",
        "        bag_slices = bag_slices.squeeze(0) # Shape: (num_slices, C, H, W)\n",
        "        labels = labels.squeeze(0)         # Shape: (1)\n",
        "\n",
        "        # Perform a forward pass through the model.\n",
        "        logits, projection = model(bag_slices)\n",
        "\n",
        "        # Append the results to our manual batch lists.\n",
        "        batch_logits.append(logits)\n",
        "        batch_projections.append(projection)\n",
        "        batch_labels.append(labels)\n",
        "\n",
        "        # When the manual batch is full, perform a training step.\n",
        "        if len(batch_labels) >= effective_batch_size or i == len(train_loader) - 1:\n",
        "            # Convert lists to tensors.\n",
        "            logits_tensor = torch.stack(batch_logits).to(device)\n",
        "            projections_tensor = torch.stack(batch_projections).to(device)\n",
        "            labels_tensor = torch.stack(batch_labels).to(device)\n",
        "\n",
        "            # Calculate the two loss components.\n",
        "            loss_scl = scl_criterion(projections_tensor, labels_tensor) # SCL on the batch.\n",
        "            loss_ce = ce_criterion(logits_tensor, labels_tensor)       # CE on the batch.\n",
        "\n",
        "            # Calculate the curriculum weight `beta_t` as per the paper.\n",
        "            current_iter = epoch * len(train_loader) + i + 1 # Corrected current iteration calculation\n",
        "            beta_t = 1.0 - (current_iter / total_iterations) # Linear decay from 1 to 0.\n",
        "            beta_t = max(0.0, beta_t) # Ensure beta_t doesn't go below 0\n",
        "\n",
        "\n",
        "            # Combine the losses using the curriculum weight.\n",
        "            total_loss = beta_t * loss_scl + (1 - beta_t) * loss_ce\n",
        "\n",
        "            # Backpropagation.\n",
        "            optimizer.zero_grad() # Clear previous gradients.\n",
        "            total_loss.backward() # Compute gradients of the total loss.\n",
        "            optimizer.step()      # Update model parameters.\n",
        "\n",
        "            # Accumulate training loss.\n",
        "            train_loss += total_loss.item()\n",
        "\n",
        "            # Clear the manual batch lists for the next accumulation.\n",
        "            batch_projections, batch_logits, batch_labels = [], [], []\n",
        "\n",
        "            # Update progress bar description.\n",
        "            pbar.set_postfix({\"Loss\": total_loss.item()})\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval() # Set the model to evaluation mode.\n",
        "    val_loss = 0.0 # Initialize cumulative validation loss.\n",
        "    all_preds, all_labels = [], [] # To store predictions and labels for F1 score and accuracy.\n",
        "\n",
        "    # Disable gradient calculations for validation.\n",
        "    with torch.no_grad():\n",
        "        for bag_slices, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            bag_slices = bag_slices.squeeze(0) # Get the bag tensor.\n",
        "            labels = labels.to(device) # Send labels to device.\n",
        "\n",
        "            # Forward pass. We only need logits for validation.\n",
        "            logits, _ = model(bag_slices)\n",
        "\n",
        "            # Calculate cross-entropy loss for validation.\n",
        "            loss = ce_criterion(logits.unsqueeze(0), labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Get predictions by finding the class with the highest logit.\n",
        "            preds = torch.argmax(logits, dim=0)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average losses and metrics.\n",
        "    avg_train_loss = train_loss / (len(train_loader) / effective_batch_size)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_train_loss, avg_val_loss, val_f1, val_accuracy\n",
        "\n",
        "# --- Training Execution ---\n",
        "\n",
        "# Hyperparameters from the paper and for our setup.\n",
        "EPOCHS = 20 # Number of training epochs. Can be increased for better performance.\n",
        "LEARNING_RATE = 1e-4 # Learning rate as specified in the paper.\n",
        "EFFECTIVE_BATCH_SIZE = 16 # Number of bags to process before an update.\n",
        "TEMPERATURE = 1.0 # Temperature for SCL, as used in the paper's experiments.\n",
        "\n",
        "# Instantiate the optimizer.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Instantiate the loss functions.\n",
        "scl_loss_fn = SupervisedContrastiveLoss(temperature=TEMPERATURE)\n",
        "ce_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Calculate total number of iterations for the curriculum weight.\n",
        "# This assumes one pass over the data per epoch.\n",
        "total_training_iterations = len(train_loader) * EPOCHS\n",
        "\n",
        "# Lists to store metrics for plotting later.\n",
        "train_loss_history, val_loss_history, val_f1_history, val_accuracy_history = [], [], [], []\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "# Main training loop over epochs.\n",
        "for epoch in range(EPOCHS):\n",
        "    # Perform one epoch of training and validation.\n",
        "    avg_train_loss, avg_val_loss, val_f1, val_accuracy = train_and_validate(\n",
        "        model, train_loader, val_loader, optimizer, scl_loss_fn, ce_loss_fn, total_training_iterations, EFFECTIVE_BATCH_SIZE, TEMPERATURE\n",
        "    )\n",
        "\n",
        "    # Store the metrics.\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "    val_f1_history.append(val_f1)\n",
        "    val_accuracy_history.append(val_accuracy)\n",
        "\n",
        "    # Print epoch results.\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_f1:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "print(\"--- Training Finished ---\")\n",
        "\n",
        "# --- End of Section 4 ---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1cq-_gCoxCx3luakivAteqgBYPm6Z-TjU",
      "authorship_tag": "ABX9TyOfdOBCcAUm4BPFpSwL98jR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}