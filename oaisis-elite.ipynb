{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8fb222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T22:13:32.495859Z",
     "iopub.status.busy": "2025-12-06T22:13:32.495675Z",
     "iopub.status.idle": "2025-12-06T22:14:54.951711Z",
     "shell.execute_reply": "2025-12-06T22:14:54.950850Z"
    },
    "papermill": {
     "duration": 82.462022,
     "end_time": "2025-12-06T22:14:54.952893",
     "exception": false,
     "start_time": "2025-12-06T22:13:32.490871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.20.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.2.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "\n",
      "================================================================================\n",
      "S1.1: Parsing OASIS-2 Sessions\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Scanning PART1: /kaggle/input/oaisis-dataset-3-p1/OAS2_RAW_PART1\n",
      "âœ“ Found 209 valid MRI sessions in PART1\n",
      "\n",
      "ğŸ“ Scanning PART2: /kaggle/input/oaisis-3-p2/OAS2_RAW_PART2\n",
      "âœ“ Found 164 valid MRI sessions in PART2\n",
      "\n",
      "âœ“ Total OASIS-2 MRI sessions collected: 373\n",
      "\n",
      "ğŸ“„ Loading clinical data: /kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv\n",
      "\n",
      "âœ“ OASIS-2 merged: 373 sessions, 373 with CDR\n",
      "\n",
      "================================================================================\n",
      "S1.2: Parsing OASIS-3 Sessions (TEMPORAL MATCHING)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Scanning: /kaggle/input/oaisis-3/oaisis3\n",
      "\n",
      "âœ“ Total OASIS-3 MRI sessions collected: 423\n",
      "\n",
      "ğŸ“„ Loading clinical data: /kaggle/input/oaisis-3-longitiudinal/oaisis3longitiudinal.csv\n",
      "âœ“ Clinical records loaded: 8626\n",
      "âœ“ Clinical records with valid CDR: 8625\n",
      "\n",
      "ğŸ”— Performing temporal matching (max Â±180 days)...\n",
      "\n",
      "âœ… Temporal matching complete:\n",
      "   â†’ Matched: 392 sessions\n",
      "   â†’ Unmatched: 31 sessions\n",
      "   â†’ Match rate: 92.7%\n",
      "\n",
      "ğŸ“Š Matched sessions - days difference stats:\n",
      "   Mean: 84.4 days\n",
      "   Median: 79.5 days\n",
      "   Max: 180 days\n",
      "\n",
      "ğŸ“Š OASIS-3 CDR distribution:\n",
      "cdr_raw\n",
      "0.0    276\n",
      "0.5     86\n",
      "1.0     26\n",
      "2.0      4\n",
      "NaN     31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "S1.3: Creating Unified Visits Table\n",
      "================================================================================\n",
      "\n",
      "âœ“ After filtering for valid CDR: 765 (removed 31)\n",
      "\n",
      "ğŸ“Š Total visits: 765, Total subjects: 435\n",
      "\n",
      "ğŸ“ˆ Dataset distribution:\n",
      "dataset\n",
      "OASIS3    392\n",
      "OASIS2    373\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ CDR class distribution:\n",
      "cdr_class\n",
      "0    482\n",
      "1    209\n",
      "2     67\n",
      "3      7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Subjects per dataset:\n",
      "dataset\n",
      "OASIS2    150\n",
      "OASIS3    285\n",
      "Name: subject_id, dtype: int64\n",
      "\n",
      "âœ… Saved to: /kaggle/working/visits_table.csv\n",
      "\n",
      "================================================================================\n",
      "S1.4: Creating Locked Test + CV Splits\n",
      "================================================================================\n",
      "\n",
      "âœ“ Total subjects: 435\n",
      "\n",
      "ğŸ“Š Stratification keys:\n",
      "strat_key_raw\n",
      "OASIS2_0     85\n",
      "OASIS2_1     52\n",
      "OASIS2_2     13\n",
      "OASIS3_0    190\n",
      "OASIS3_1     68\n",
      "OASIS3_2     23\n",
      "OASIS3_3      4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… LOCKED TEST: 87 subjects\n",
      "   Datasets: {'OASIS3': 58, 'OASIS2': 29}\n",
      "   CDR classes: {0: 55, 1: 24, 2: 7, 3: 1}\n",
      "\n",
      "âœ… FOLD 0: 278 train, 70 val\n",
      "\n",
      "âœ… FOLD 1: 278 train, 70 val\n",
      "\n",
      "âœ… FOLD 2: 278 train, 70 val\n",
      "\n",
      "âœ… FOLD 3: 279 train, 69 val\n",
      "\n",
      "âœ… FOLD 4: 279 train, 69 val\n",
      "\n",
      "âœ… Saved to: /kaggle/working/cv_splits.json\n",
      "\n",
      "================================================================================\n",
      "âœ… S1 COMPLETED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S1: Master Visits Table + Locked Test + CV Folds (TEMPORAL MATCHING FIX)\n",
    "Fixed: OASIS-3 now uses nearest-neighbor temporal matching for MRI-clinical linkage\n",
    "\"\"\"\n",
    "\n",
    "!pip install timm\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# S1.1: Parse OASIS-2 (UNCHANGED)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_oasis2(oas2_part1_root, oas2_part2_root, clinical_csv_path):\n",
    "    \"\"\"Parse OASIS-2 MRI sessions.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"S1.1: Parsing OASIS-2 Sessions\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sessions = []\n",
    "    \n",
    "    for part_name, root in [(\"PART1\", oas2_part1_root), (\"PART2\", oas2_part2_root)]:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nğŸ“ Scanning {part_name}: {root}\")\n",
    "        folder_count = 0\n",
    "        \n",
    "        for folder_name in os.listdir(root):\n",
    "            folder_path = os.path.join(root, folder_name)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "                \n",
    "            if not folder_name.startswith(\"OAS2_\") or \"_MR\" not in folder_name:\n",
    "                continue\n",
    "                \n",
    "            parts = folder_name.split(\"_\")\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "                \n",
    "            subject_id = f\"{parts[0]}_{parts[1]}\"\n",
    "            visit_str = parts[2]\n",
    "            \n",
    "            try:\n",
    "                visit_num = int(visit_str.replace(\"MR\", \"\"))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            raw_dir = os.path.join(folder_path, \"RAW\")\n",
    "            if not os.path.exists(raw_dir):\n",
    "                continue\n",
    "            \n",
    "            candidates = []\n",
    "            for fname in os.listdir(raw_dir):\n",
    "                if fname.startswith(\"mpr\") and any(ext in fname for ext in ['.img', '.hdr', '.nii', '.nii.gz']):\n",
    "                    if fname.endswith('.img') or fname.endswith('.hdr'):\n",
    "                        candidates.append(os.path.join(raw_dir, fname))\n",
    "                    elif fname.endswith('.nii') or fname.endswith('.nii.gz'):\n",
    "                        candidates.append(os.path.join(raw_dir, fname))\n",
    "            \n",
    "            if not candidates:\n",
    "                continue\n",
    "            \n",
    "            mri_path = sorted(candidates, key=lambda x: (not x.endswith('.img'), x))[0]\n",
    "            \n",
    "            sessions.append({\n",
    "                \"subject_id\": subject_id,\n",
    "                \"session_id\": folder_name,\n",
    "                \"visit_num\": visit_num,\n",
    "                \"mri_path\": mri_path,\n",
    "                \"dataset\": \"OASIS2\",\n",
    "                \"scanner_id\": 0,\n",
    "            })\n",
    "            folder_count += 1\n",
    "        \n",
    "        print(f\"âœ“ Found {folder_count} valid MRI sessions in {part_name}\")\n",
    "    \n",
    "    df_mri = pd.DataFrame(sessions)\n",
    "    print(f\"\\nâœ“ Total OASIS-2 MRI sessions collected: {len(df_mri)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Loading clinical data: {clinical_csv_path}\")\n",
    "    df_clin = pd.read_csv(clinical_csv_path)\n",
    "    \n",
    "    df_clin['subject_id'] = df_clin['Subject ID'].str.strip()\n",
    "    df_clin['visit_num'] = df_clin['Visit'].astype(int)\n",
    "    \n",
    "    rename_map = {'CDR': 'cdr_raw', 'MMSE': 'mmse', 'Age': 'age', 'M/F': 'sex'}\n",
    "    df_clin = df_clin.rename(columns=rename_map)\n",
    "    \n",
    "    df_merged = df_mri.merge(\n",
    "        df_clin[['subject_id', 'visit_num', 'cdr_raw', 'mmse', 'age', 'sex']],\n",
    "        on=['subject_id', 'visit_num'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ OASIS-2 merged: {len(df_merged)} sessions, {df_merged['cdr_raw'].notna().sum()} with CDR\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# S1.2: Parse OASIS-3 (TEMPORAL MATCHING FIX)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_oasis3(oasis3_root, clinical_csv_path, max_days_diff=180):\n",
    "    \"\"\"\n",
    "    Parse OASIS-3 with temporal matching.\n",
    "    Links each MRI scan to nearest clinical assessment within Â±max_days_diff.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"S1.2: Parsing OASIS-3 Sessions (TEMPORAL MATCHING)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sessions = []\n",
    "    \n",
    "    if not os.path.exists(oasis3_root):\n",
    "        print(f\"âš ï¸  OASIS-3 root not found: {oasis3_root}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Scanning: {oasis3_root}\")\n",
    "    \n",
    "    for folder_name in os.listdir(oasis3_root):\n",
    "        folder_path = os.path.join(oasis3_root, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        if not folder_name.startswith(\"OAS3\") or \"_MR_d\" not in folder_name:\n",
    "            continue\n",
    "        \n",
    "        parts = folder_name.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        \n",
    "        subject_id = parts[0]\n",
    "        session_date_key = parts[2]  # e.g., \"d0056\"\n",
    "        \n",
    "        # Extract numeric day from date key\n",
    "        try:\n",
    "            mri_day = int(session_date_key.replace('d', ''))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        all_t1_files = []\n",
    "        \n",
    "        for item in os.listdir(folder_path):\n",
    "            if item.startswith(\"anat\"):\n",
    "                anat_path = os.path.join(folder_path, item, \"NIFTI\")\n",
    "                if not os.path.exists(anat_path):\n",
    "                    continue\n",
    "                \n",
    "                for root, dirs, files in os.walk(anat_path):\n",
    "                    for fname in files:\n",
    "                        if 'T1w.nii' in fname:\n",
    "                            all_t1_files.append({\n",
    "                                'path': os.path.join(root, fname),\n",
    "                                'anat_dir': item,\n",
    "                                'filename': fname\n",
    "                            })\n",
    "        \n",
    "        if not all_t1_files:\n",
    "            continue\n",
    "        \n",
    "        def quality_score(t1_info):\n",
    "            score = 0\n",
    "            if 'run-01' in t1_info['filename']:\n",
    "                score += 100\n",
    "            if t1_info['anat_dir'] == 'anat2':\n",
    "                score += 10\n",
    "            return score\n",
    "        \n",
    "        best_t1 = sorted(all_t1_files, key=quality_score, reverse=True)[0]\n",
    "        \n",
    "        sessions.append({\n",
    "            \"subject_id\": subject_id,\n",
    "            \"session_id\": folder_name,\n",
    "            \"mri_day\": mri_day,\n",
    "            \"mri_path\": best_t1['path'],\n",
    "            \"dataset\": \"OASIS3\",\n",
    "            \"scanner_id\": 1,\n",
    "        })\n",
    "    \n",
    "    df_mri = pd.DataFrame(sessions)\n",
    "    print(f\"\\nâœ“ Total OASIS-3 MRI sessions collected: {len(df_mri)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEMPORAL MATCHING: Link MRI scans to nearest clinical assessments\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Loading clinical data: {clinical_csv_path}\")\n",
    "    df_clin = pd.read_csv(clinical_csv_path)\n",
    "    print(f\"âœ“ Clinical records loaded: {len(df_clin)}\")\n",
    "    \n",
    "    # Extract subject ID and day from clinical CSV\n",
    "    df_clin[\"subject_id\"] = df_clin[\"OASISID\"].str.strip()\n",
    "    \n",
    "    # Extract day from OASIS_session_label (e.g., \"OAS30001_UDSb4_d0339\" -> 339)\n",
    "    df_clin[\"clinical_day\"] = df_clin[\"OASIS_session_label\"].str.split(\"_\").str[-1].str.replace('d', '').astype(int)\n",
    "    \n",
    "    # Use CDRTOT as primary CDR\n",
    "    df_clin[\"cdr_raw\"] = pd.to_numeric(df_clin[\"CDRTOT\"], errors=\"coerce\")\n",
    "    \n",
    "    # Fallback to domain max if CDRTOT missing\n",
    "    domain_cols = [\"memory\", \"orient\", \"judgment\", \"commun\", \"homehobb\", \"perscare\"]\n",
    "    for col in domain_cols:\n",
    "        if col in df_clin.columns:\n",
    "            df_clin[col] = pd.to_numeric(df_clin[col], errors=\"coerce\")\n",
    "    \n",
    "    available_domains = [c for c in domain_cols if c in df_clin.columns]\n",
    "    if available_domains:\n",
    "        domain_max = df_clin[available_domains].max(axis=1)\n",
    "        missing = df_clin[\"cdr_raw\"].isna()\n",
    "        df_clin.loc[missing, \"cdr_raw\"] = domain_max[missing]\n",
    "    \n",
    "    # Rename other columns\n",
    "    df_clin = df_clin.rename(columns={\"MMSE\": \"mmse\", \"age at visit\": \"age\"})\n",
    "    if \"sex\" not in df_clin.columns:\n",
    "        df_clin[\"sex\"] = np.nan\n",
    "    \n",
    "    # Keep only records with valid CDR\n",
    "    df_clin = df_clin[df_clin[\"cdr_raw\"].notna()].copy()\n",
    "    print(f\"âœ“ Clinical records with valid CDR: {len(df_clin)}\")\n",
    "    \n",
    "    # Temporal matching: for each MRI, find nearest clinical assessment\n",
    "    print(f\"\\nğŸ”— Performing temporal matching (max Â±{max_days_diff} days)...\")\n",
    "    \n",
    "    matched_records = []\n",
    "    match_count = 0\n",
    "    no_match_count = 0\n",
    "    \n",
    "    for idx, mri_row in df_mri.iterrows():\n",
    "        subj = mri_row['subject_id']\n",
    "        mri_day = mri_row['mri_day']\n",
    "        \n",
    "        # Get all clinical assessments for this subject\n",
    "        subj_clin = df_clin[df_clin['subject_id'] == subj]\n",
    "        \n",
    "        if len(subj_clin) == 0:\n",
    "            no_match_count += 1\n",
    "            matched_records.append({\n",
    "                **mri_row.to_dict(),\n",
    "                'cdr_raw': np.nan,\n",
    "                'mmse': np.nan,\n",
    "                'age': np.nan,\n",
    "                'sex': np.nan,\n",
    "                'days_diff': np.nan\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Calculate time difference for each clinical visit\n",
    "        subj_clin = subj_clin.copy()\n",
    "        subj_clin['days_diff'] = abs(subj_clin['clinical_day'] - mri_day)\n",
    "        \n",
    "        # Find nearest within max window\n",
    "        nearest = subj_clin[subj_clin['days_diff'] <= max_days_diff].sort_values('days_diff')\n",
    "        \n",
    "        if len(nearest) == 0:\n",
    "            no_match_count += 1\n",
    "            matched_records.append({\n",
    "                **mri_row.to_dict(),\n",
    "                'cdr_raw': np.nan,\n",
    "                'mmse': np.nan,\n",
    "                'age': np.nan,\n",
    "                'sex': np.nan,\n",
    "                'days_diff': np.nan\n",
    "            })\n",
    "        else:\n",
    "            match_count += 1\n",
    "            best_match = nearest.iloc[0]\n",
    "            matched_records.append({\n",
    "                **mri_row.to_dict(),\n",
    "                'cdr_raw': best_match['cdr_raw'],\n",
    "                'mmse': best_match['mmse'],\n",
    "                'age': best_match['age'],\n",
    "                'sex': best_match['sex'],\n",
    "                'days_diff': best_match['days_diff']\n",
    "            })\n",
    "    \n",
    "    df_merged = pd.DataFrame(matched_records)\n",
    "    \n",
    "    print(f\"\\nâœ… Temporal matching complete:\")\n",
    "    print(f\"   â†’ Matched: {match_count} sessions\")\n",
    "    print(f\"   â†’ Unmatched: {no_match_count} sessions\")\n",
    "    print(f\"   â†’ Match rate: {match_count/len(df_mri)*100:.1f}%\")\n",
    "    \n",
    "    if match_count > 0:\n",
    "        matched_subset = df_merged[df_merged['cdr_raw'].notna()]\n",
    "        print(f\"\\nğŸ“Š Matched sessions - days difference stats:\")\n",
    "        print(f\"   Mean: {matched_subset['days_diff'].mean():.1f} days\")\n",
    "        print(f\"   Median: {matched_subset['days_diff'].median():.1f} days\")\n",
    "        print(f\"   Max: {matched_subset['days_diff'].max():.0f} days\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š OASIS-3 CDR distribution:\")\n",
    "    print(df_merged['cdr_raw'].value_counts(dropna=False).sort_index())\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# S1.3 & S1.4: UNCHANGED\n",
    "# ============================================================================\n",
    "\n",
    "def create_unified_visits_table(df_oasis2, df_oasis3, output_csv_path):\n",
    "    \"\"\"Combine OASIS-2 and OASIS-3 into unified table.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"S1.3: Creating Unified Visits Table\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    keep_cols = ['subject_id', 'session_id', 'mri_path', 'dataset', 'scanner_id',\n",
    "                 'cdr_raw', 'mmse', 'age', 'sex']\n",
    "    \n",
    "    df2 = df_oasis2[keep_cols].copy()\n",
    "    df3 = df_oasis3[keep_cols].copy()\n",
    "    \n",
    "    df = pd.concat([df2, df3], ignore_index=True)\n",
    "    df = df.dropna(subset=['mri_path'])\n",
    "    df = df.drop_duplicates(subset=['subject_id', 'session_id'])\n",
    "    \n",
    "    df['cdr_raw'] = pd.to_numeric(df['cdr_raw'], errors='coerce')\n",
    "    \n",
    "    def map_cdr_to_class(cdr):\n",
    "        if pd.isna(cdr):\n",
    "            return None\n",
    "        if cdr == 0.0:\n",
    "            return 0\n",
    "        elif cdr == 0.5:\n",
    "            return 1\n",
    "        elif cdr == 1.0:\n",
    "            return 2\n",
    "        elif cdr >= 2.0:\n",
    "            return 3\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    df['cdr_class'] = df['cdr_raw'].apply(map_cdr_to_class)\n",
    "    \n",
    "    before_filter = len(df)\n",
    "    df = df.dropna(subset=['cdr_class'])\n",
    "    print(f\"\\nâœ“ After filtering for valid CDR: {len(df)} (removed {before_filter - len(df)})\")\n",
    "    \n",
    "    df['cdr_class'] = df['cdr_class'].astype(int)\n",
    "    df['binary_strict'] = (df['cdr_raw'] >= 1.0).astype(int)\n",
    "    df['binary_loose'] = (df['cdr_raw'] >= 0.5).astype(int)\n",
    "    \n",
    "    df = df.sort_values(['dataset', 'subject_id', 'session_id'])\n",
    "    df['visit_index'] = df.groupby('subject_id').cumcount() + 1\n",
    "    df['label'] = df['cdr_class']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total visits: {len(df)}, Total subjects: {df['subject_id'].nunique()}\")\n",
    "    print(f\"\\nğŸ“ˆ Dataset distribution:\")\n",
    "    print(df['dataset'].value_counts())\n",
    "    print(f\"\\nğŸ“ˆ CDR class distribution:\")\n",
    "    print(df['cdr_class'].value_counts().sort_index())\n",
    "    print(f\"\\nğŸ“ˆ Subjects per dataset:\")\n",
    "    print(df.groupby('dataset')['subject_id'].nunique())\n",
    "    \n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nâœ… Saved to: {output_csv_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_locked_test_and_cv_splits(visits_df, output_json_path,\n",
    "                                     test_frac=0.2, n_splits=5, random_state=42):\n",
    "    \"\"\"Create subject-level stratified splits.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"S1.4: Creating Locked Test + CV Splits\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_sorted = visits_df.sort_values(['subject_id', 'visit_index'])\n",
    "    df_subj = df_sorted.groupby('subject_id').first().reset_index()\n",
    "    \n",
    "    df_subj['strat_key_raw'] = df_subj['dataset'] + '_' + df_subj['cdr_class'].astype(str)\n",
    "    \n",
    "    print(f\"\\nâœ“ Total subjects: {len(df_subj)}\")\n",
    "    print(f\"\\nğŸ“Š Stratification keys:\")\n",
    "    print(df_subj['strat_key_raw'].value_counts().sort_index())\n",
    "    \n",
    "    counts = df_subj['strat_key_raw'].value_counts()\n",
    "    rare_keys = set([k for k, c in counts.items() if c < 2])\n",
    "    \n",
    "    df_subj['strat_key_test'] = df_subj['strat_key_raw'].apply(\n",
    "        lambda k: \"RARE\" if k in rare_keys else k\n",
    "    )\n",
    "    \n",
    "    subjects = df_subj['subject_id'].values\n",
    "    y_test_strat = df_subj['strat_key_test'].values\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_frac, random_state=random_state)\n",
    "    trainval_idx, test_idx = next(sss.split(subjects, y_test_strat))\n",
    "    \n",
    "    trainval_subjects = subjects[trainval_idx].tolist()\n",
    "    locked_test_subjects = subjects[test_idx].tolist()\n",
    "    \n",
    "    test_df = df_subj[df_subj['subject_id'].isin(locked_test_subjects)]\n",
    "    \n",
    "    print(f\"\\nâœ… LOCKED TEST: {len(locked_test_subjects)} subjects\")\n",
    "    print(f\"   Datasets: {dict(test_df['dataset'].value_counts())}\")\n",
    "    print(f\"   CDR classes: {dict(test_df['cdr_class'].value_counts().sort_index())}\")\n",
    "    \n",
    "    # Ensure OASIS3 in test if available\n",
    "    if (test_df['dataset'] == 'OASIS3').sum() == 0:\n",
    "        oasis3_tv = df_subj[(df_subj['dataset'] == 'OASIS3') & \n",
    "                            (df_subj['subject_id'].isin(trainval_subjects))]\n",
    "        if len(oasis3_tv) > 0:\n",
    "            subj_to_move = oasis3_tv['subject_id'].iloc[0]\n",
    "            trainval_subjects.remove(subj_to_move)\n",
    "            locked_test_subjects.append(subj_to_move)\n",
    "            print(\"   â†’ Moved 1 OASIS3 subject to test\")\n",
    "    \n",
    "    df_trainval = df_subj[df_subj['subject_id'].isin(trainval_subjects)].reset_index(drop=True)\n",
    "    df_trainval['cv_group'] = df_trainval['cdr_class'].apply(\n",
    "        lambda c: 0 if c == 0 else (1 if c == 1 else 2)\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    folds = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df_trainval['subject_id'].values, \n",
    "                                                               df_trainval['cv_group'].values)):\n",
    "        folds.append({\n",
    "            \"fold\": fold_idx,\n",
    "            \"train_subjects\": df_trainval.iloc[train_idx]['subject_id'].tolist(),\n",
    "            \"val_subjects\": df_trainval.iloc[val_idx]['subject_id'].tolist()\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ… FOLD {fold_idx}: {len(folds[-1]['train_subjects'])} train, {len(folds[-1]['val_subjects'])} val\")\n",
    "    \n",
    "    split_info = {\n",
    "        \"locked_test_subjects\": locked_test_subjects,\n",
    "        \"folds\": folds,\n",
    "        \"metadata\": {\n",
    "            \"n_total_subjects\": len(df_subj),\n",
    "            \"n_test_subjects\": len(locked_test_subjects),\n",
    "            \"n_folds\": n_splits\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(split_info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Saved to: {output_json_path}\")\n",
    "    return split_info\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    OAS2_PART1 = \"/kaggle/input/oaisis-dataset-3-p1/OAS2_RAW_PART1\"\n",
    "    OAS2_PART2 = \"/kaggle/input/oaisis-3-p2/OAS2_RAW_PART2\"\n",
    "    OAS2_CLINICAL = \"/kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv\"\n",
    "    \n",
    "    OAS3_ROOT = \"/kaggle/input/oaisis-3/oaisis3\"\n",
    "    OAS3_CLINICAL = \"/kaggle/input/oaisis-3-longitiudinal/oaisis3longitiudinal.csv\"\n",
    "    \n",
    "    OUTPUT_DIR = \"/kaggle/working\"\n",
    "    \n",
    "    df_oasis2 = parse_oasis2(OAS2_PART1, OAS2_PART2, OAS2_CLINICAL)\n",
    "    df_oasis3 = parse_oasis3(OAS3_ROOT, OAS3_CLINICAL, max_days_diff=180)\n",
    "    \n",
    "    visits_df = create_unified_visits_table(\n",
    "        df_oasis2, df_oasis3,\n",
    "        os.path.join(OUTPUT_DIR, \"visits_table.csv\")\n",
    "    )\n",
    "    \n",
    "    splits_info = create_locked_test_and_cv_splits(\n",
    "        visits_df,\n",
    "        os.path.join(OUTPUT_DIR, \"cv_splits.json\")\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… S1 COMPLETED\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f3d4d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T22:14:54.993601Z",
     "iopub.status.busy": "2025-12-06T22:14:54.993384Z",
     "iopub.status.idle": "2025-12-06T22:45:33.492750Z",
     "shell.execute_reply": "2025-12-06T22:45:33.491965Z"
    },
    "papermill": {
     "duration": 1838.521204,
     "end_time": "2025-12-06T22:45:33.493850",
     "exception": false,
     "start_time": "2025-12-06T22:14:54.972646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running Almalki et al. Preprocessing (AMF + Laplacian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 765/765 [30:37<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S2 Done (Paper Filters Applied)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S2: PAPER REPLICATION PREPROCESSING\n",
    "- Implements Adaptive Median Filter (AMF) + Laplacian Filter \n",
    "- Removes N4 Bias (Not used in paper)\n",
    "- Disk Safe: Cleans up temp files immediately\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "from scipy.ndimage import median_filter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class S2Config:\n",
    "    OUTPUT_ROOT = \"/kaggle/working/processed_mri\"\n",
    "    FULL_BRAIN_DIR = \"full_brain\"\n",
    "    TEMP_DIR = \"temp_cleaned\"\n",
    "    # Paper resizes to 224x224 [cite: 343]\n",
    "    TARGET_SHAPE = (224, 224) \n",
    "\n",
    "def apply_paper_filters(volume):\n",
    "    \"\"\"\n",
    "    Apply AMF + Laplacian Sharpening slice by slice as described in [cite: 247-279]\n",
    "    \"\"\"\n",
    "    # Normalize to 0-255 for CV2 ops\n",
    "    vol_norm = cv2.normalize(volume, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    processed_vol = np.zeros_like(vol_norm)\n",
    "    \n",
    "    for i in range(vol_norm.shape[0]): # Axial slices\n",
    "        slice_img = vol_norm[i, :, :]\n",
    "        \n",
    "        # 1. Median Filter (Approx for AMF) [cite: 248]\n",
    "        denoised = median_filter(slice_img, size=3)\n",
    "        \n",
    "        # 2. Laplacian Filter (Edge Enhancement) [cite: 268]\n",
    "        laplacian = cv2.Laplacian(denoised, cv2.CV_64F)\n",
    "        \n",
    "        # Sharpening\n",
    "        sharpened = denoised - 0.5 * laplacian \n",
    "        processed_vol[i, :, :] = np.clip(sharpened, 0, 255)\n",
    "        \n",
    "    return processed_vol.astype(np.float32) / 255.0\n",
    "\n",
    "def process_visit(row, config):\n",
    "    try:\n",
    "        # Load NIFTI\n",
    "        img = nib.load(row['mri_path'])\n",
    "        data = img.get_fdata()\n",
    "        \n",
    "        # Collapse 4D\n",
    "        if data.ndim == 4: data = np.mean(data, axis=-1)\n",
    "        \n",
    "        # APPLY PAPER FILTERS\n",
    "        data_filtered = apply_paper_filters(data)\n",
    "        \n",
    "        # Save\n",
    "        filename = f\"{row['dataset']}_{row['subject_id']}_v{row['visit_index']}.nii.gz\"\n",
    "        save_path = os.path.join(config.OUTPUT_ROOT, config.FULL_BRAIN_DIR, filename)\n",
    "        \n",
    "        affine = img.affine\n",
    "        nib.save(nib.Nifti1Image(data_filtered, affine), save_path)\n",
    "        \n",
    "        return {'full_t1_path': save_path, 'preproc_ok': True}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'full_t1_path': None, 'preproc_ok': False}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = S2Config()\n",
    "    if os.path.exists(config.OUTPUT_ROOT): shutil.rmtree(config.OUTPUT_ROOT)\n",
    "    os.makedirs(os.path.join(config.OUTPUT_ROOT, config.FULL_BRAIN_DIR), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_csv(\"/kaggle/working/visits_table.csv\")\n",
    "    results = []\n",
    "    \n",
    "    print(\"ğŸš€ Running Almalki et al. Preprocessing (AMF + Laplacian)...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        res = process_visit(row, config)\n",
    "        results.append({**row.to_dict(), **res})\n",
    "        \n",
    "    pd.DataFrame(results).to_csv(\"/kaggle/working/processed_volumes.csv\", index=False)\n",
    "    print(\"âœ… S2 Done (Paper Filters Applied)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a020473d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T22:45:33.593122Z",
     "iopub.status.busy": "2025-12-06T22:45:33.592835Z",
     "iopub.status.idle": "2025-12-06T22:49:48.916540Z",
     "shell.execute_reply": "2025-12-06T22:49:48.915881Z"
    },
    "papermill": {
     "duration": 255.374795,
     "end_time": "2025-12-06T22:49:48.917646",
     "exception": false,
     "start_time": "2025-12-06T22:45:33.542851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Atlases...\n",
      "\n",
      "Added README.md to /root/nilearn_data\n",
      "\n",
      "\n",
      "Dataset created in /root/nilearn_data/fsl\n",
      "\n",
      "Downloading data from https://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ...done. (1 seconds, 0 min)\n",
      "Extracting data from /root/nilearn_data/fsl/8a6a179c4b7672ec60913c596b129eff/HarvardOxford.tgz..... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Generating Precision ROIs for 698 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 698/698 [04:13<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3 Done. 58016 high-quality patches generated.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S3: PAPER-EXACT ROI EXTRACTION\n",
    "- Uses MODE for centroid (Eq 3 in paper)\n",
    "- Resamples Mask ONCE (Speedup)\n",
    "- Skips edge slices (Cleaner data)\n",
    "- Targets: Right Hippocampus (Sagittal) & Left Parietal (Coronal)\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from nilearn import datasets\n",
    "import shutil\n",
    "\n",
    "SAVE_DIR = \"/kaggle/working/slices_paper_32px\"\n",
    "if os.path.exists(SAVE_DIR): shutil.rmtree(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Load Atlases\n",
    "print(\"ğŸ“¥ Loading Atlases...\")\n",
    "ho_sub = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')\n",
    "ho_cort = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "\n",
    "def get_atlas_data(map_path):\n",
    "    if isinstance(map_path, str): return nib.load(map_path).get_fdata()\n",
    "    return map_path.get_fdata()\n",
    "\n",
    "def get_base_mask(roi_target):\n",
    "    \"\"\"Generates binary mask (MNI space)\"\"\"\n",
    "    if \"Hippocampus\" in roi_target:\n",
    "        data = get_atlas_data(ho_sub.maps)\n",
    "        labels = [l.decode() if isinstance(l, bytes) else l for l in ho_sub.labels]\n",
    "        try: idx = labels.index(roi_target)\n",
    "        except: idx = 9 if \"Left\" in roi_target else 19\n",
    "        return (data == idx)\n",
    "\n",
    "    if \"Parietal\" in roi_target:\n",
    "        data = get_atlas_data(ho_cort.maps)\n",
    "        labels = [l.decode() if isinstance(l, bytes) else l for l in ho_cort.labels]\n",
    "        # Combine Superior Parietal + Supramarginal + Angular\n",
    "        indices = [i for i, l in enumerate(labels) if \"Parietal\" in l or \"Supramarginal\" in l or \"Angular\" in l]\n",
    "        mask = np.zeros_like(data, dtype=bool)\n",
    "        for i in indices: mask = mask | (data == i)\n",
    "        \n",
    "        # Hemisphere Split\n",
    "        mid = data.shape[0] // 2\n",
    "        if \"Left\" in roi_target: mask[mid:, :, :] = 0\n",
    "        else: mask[:mid, :, :] = 0\n",
    "        return mask\n",
    "\n",
    "def crop_centroid_mode_32px(vol, mask, view):\n",
    "    \"\"\"\n",
    "    Paper Method: Calculate Centroid using MODE per slice -> Crop 32x32\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    \n",
    "    if view == 'sagittal':\n",
    "        for x in range(vol.shape[0]):\n",
    "            m2d = mask[x, :, :]\n",
    "            if m2d.sum() < 5: continue # Skip if hardly any ROI\n",
    "            \n",
    "            # Use MODE (Most frequent coordinate) - Matches Paper [cite: 4326]\n",
    "            ys, zs = np.where(m2d)\n",
    "            if len(ys) == 0: continue\n",
    "            \n",
    "            # Scipy stats.mode can return scalar or array depending on version\n",
    "            try:\n",
    "                cy = int(stats.mode(ys, keepdims=False)[0])\n",
    "                cz = int(stats.mode(zs, keepdims=False)[0])\n",
    "            except:\n",
    "                # Fallback for older scipy\n",
    "                cy = int(stats.mode(ys)[0][0])\n",
    "                cz = int(stats.mode(zs)[0][0])\n",
    "\n",
    "            # Crop 32x32\n",
    "            sl = vol[x, :, :]\n",
    "            y_s, y_e = cy-16, cy+16\n",
    "            z_s, z_e = cz-16, cz+16\n",
    "            \n",
    "            # Boundary Check\n",
    "            if y_s < 0 or z_s < 0 or y_e > vol.shape[1] or z_e > vol.shape[2]:\n",
    "                continue # Skip edge cases (Clean data only)\n",
    "            \n",
    "            patch = sl[y_s:y_e, z_s:z_e]\n",
    "            patches.append(patch)\n",
    "            \n",
    "    elif view == 'coronal':\n",
    "        for y in range(vol.shape[1]):\n",
    "            m2d = mask[:, y, :]\n",
    "            if m2d.sum() < 5: continue\n",
    "            \n",
    "            xs, zs = np.where(m2d)\n",
    "            if len(xs) == 0: continue\n",
    "            \n",
    "            try:\n",
    "                cx = int(stats.mode(xs, keepdims=False)[0])\n",
    "                cz = int(stats.mode(zs, keepdims=False)[0])\n",
    "            except:\n",
    "                cx = int(stats.mode(xs)[0][0])\n",
    "                cz = int(stats.mode(zs)[0][0])\n",
    "            \n",
    "            sl = vol[:, y, :]\n",
    "            x_s, x_e = cx-16, cx+16\n",
    "            z_s, z_e = cz-16, cz+16\n",
    "            \n",
    "            if x_s < 0 or z_s < 0 or x_e > vol.shape[0] or z_e > vol.shape[2]:\n",
    "                continue\n",
    "            \n",
    "            patch = sl[x_s:x_e, z_s:z_e]\n",
    "            patches.append(patch)\n",
    "            \n",
    "    return patches\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    targets = [(\"Right Hippocampus\", \"sagittal\"), (\"Left Parietal Lobe\", \"coronal\")]\n",
    "    \n",
    "    # 1. Load Base Masks (MNI Space)\n",
    "    base_masks = {f\"{t[0]}_{t[1]}\": get_base_mask(t[0]) for t in targets}\n",
    "    \n",
    "    if not os.path.exists(\"/kaggle/working/processed_volumes.csv\"):\n",
    "        raise FileNotFoundError(\"Run S2 first!\")\n",
    "        \n",
    "    df = pd.read_csv(\"/kaggle/working/processed_volumes.csv\")\n",
    "    df = df[df['preproc_ok'] == True]\n",
    "    \n",
    "    # Optimization: Cache resized masks per resolution\n",
    "    # (Assuming most S2 outputs have same shape, e.g. 128x160x128)\n",
    "    cached_masks = {} \n",
    "    \n",
    "    records = []\n",
    "    print(f\"ğŸš€ Generating Precision ROIs for {len(df)} subjects...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            vol_path = row['full_t1_path'] # Use downsampled path\n",
    "            if not os.path.exists(vol_path): continue\n",
    "            vol = nib.load(vol_path).get_fdata()\n",
    "            \n",
    "            vol_shape = vol.shape\n",
    "            \n",
    "            for roi, view in targets:\n",
    "                cache_key = f\"{roi}_{view}_{vol_shape}\"\n",
    "                \n",
    "                # Check Cache\n",
    "                if cache_key in cached_masks:\n",
    "                    mask_to_use = cached_masks[cache_key]\n",
    "                else:\n",
    "                    # Resample Once per shape\n",
    "                    base_mask = base_masks[f\"{roi}_{view}\"]\n",
    "                    factors = np.array(vol_shape) / np.array(base_mask.shape)\n",
    "                    mask_to_use = zoom(base_mask.astype(float), factors, order=0) > 0.5\n",
    "                    cached_masks[cache_key] = mask_to_use\n",
    "                \n",
    "                patches = crop_centroid_mode_32px(vol, mask_to_use, view)\n",
    "                \n",
    "                for p_idx, p in enumerate(patches):\n",
    "                    fname = f\"{row['subject_id']}_{roi.replace(' ','')}_{view[:3]}_{p_idx}.npy\"\n",
    "                    fpath = os.path.join(SAVE_DIR, fname)\n",
    "                    np.save(fpath, p.astype(np.float16))\n",
    "                    \n",
    "                    records.append({\n",
    "                        'subject_id': row['subject_id'],\n",
    "                        'path': fpath,\n",
    "                        'label': row['cdr_class'],\n",
    "                        'roi': roi,\n",
    "                        'view': view\n",
    "                    })\n",
    "        except Exception as e: continue\n",
    "\n",
    "    df_final = pd.DataFrame(records)\n",
    "    df_final.to_csv(\"/kaggle/working/dataset_paper_final.csv\", index=False)\n",
    "    print(f\"âœ… S3 Done. {len(df_final)} high-quality patches generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c128b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T22:49:49.069500Z",
     "iopub.status.busy": "2025-12-06T22:49:49.069275Z",
     "iopub.status.idle": "2025-12-07T02:16:27.429446Z",
     "shell.execute_reply": "2025-12-07T02:16:27.428469Z"
    },
    "papermill": {
     "duration": 12399.67524,
     "end_time": "2025-12-07T02:16:28.668416",
     "exception": false,
     "start_time": "2025-12-06T22:49:48.993176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 282 Train, 61 Val\n",
      "\n",
      "ğŸ§  Training Specialist: Right Hippocampus (Sagittal)\n",
      "  Balanced Train: 21332 slices\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbce990e102d4c4a8ca72894baf68338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ”¥ Stage 1: Warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸš€ Stage 2: Fine-Tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Val Subject Acc=52.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Val Subject Acc=54.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: Val Subject Acc=65.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9: Val Subject Acc=63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: Val Subject Acc=59.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13: Val Subject Acc=63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14: Val Subject Acc=57.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15: Val Subject Acc=59.02%\n",
      "\n",
      "ğŸ§  Training Specialist: Left Parietal Lobe (Coronal)\n",
      "  Balanced Train: 30616 slices\n",
      "  ğŸ”¥ Stage 1: Warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸš€ Stage 2: Fine-Tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Val Subject Acc=67.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: Val Subject Acc=65.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: Val Subject Acc=63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: Val Subject Acc=65.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9: Val Subject Acc=60.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: Val Subject Acc=63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11: Val Subject Acc=67.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12: Val Subject Acc=72.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13: Val Subject Acc=63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14: Val Subject Acc=62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15: Val Subject Acc=57.38%\n",
      "\n",
      "âš–ï¸ Running Ensemble on TEST SET...\n",
      "ğŸ† ENSEMBLE ACCURACY: 69.05%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S4: RIGOROUS PAPER TRAINING (Binary Clean + Aligned Split)\n",
    "- Task: CN (0) vs AD (>= 1.0). MCI DROPPED.\n",
    "- Fix: Proper Subject-Level Stratification (Reindexed)\n",
    "- Fix: Z-Score Normalization\n",
    "- Fix: 2-Stage Training\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. DATASET\n",
    "class SwinDataset(Dataset):\n",
    "    def __init__(self, df, is_train=True):\n",
    "        self.paths = df['path'].values\n",
    "        # STRICT BINARY: 0=CN, 1=AD (CDR >= 1.0)\n",
    "        self.labels = (df['label'].values >= 1.0).astype(int)\n",
    "        self.subjects = df['subject_id'].values\n",
    "        self.resize = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = np.load(self.paths[idx]).astype(np.float32)\n",
    "            # FIX: Z-Score\n",
    "            m, s = img.mean(), img.std()\n",
    "            if s > 1e-6: img = (img - m) / s\n",
    "            else: img = img - m\n",
    "            \n",
    "            t = torch.tensor(img[None, None, :, :]) \n",
    "            t = self.resize(t).squeeze(0).repeat(3,1,1) \n",
    "            \n",
    "            if self.is_train and random.random() > 0.5:\n",
    "                t = torch.flip(t, [-1])\n",
    "                \n",
    "            return t, torch.tensor(self.labels[idx], dtype=torch.long), self.subjects[idx]\n",
    "        except:\n",
    "            return torch.zeros(3,224,224), torch.tensor(0), \"err\"\n",
    "\n",
    "# 2. TRAINER\n",
    "def train_specialist_strict(roi, view, df, train_subs, val_subs):\n",
    "    print(f\"\\nğŸ§  Training Specialist: {roi} ({view})\")\n",
    "    \n",
    "    train_df = df[df['subject_id'].isin(train_subs)].copy()\n",
    "    val_df = df[df['subject_id'].isin(val_subs)].copy()\n",
    "    \n",
    "    # Balance (Oversample AD)\n",
    "    train_df['binary'] = (train_df['label'] >= 1.0).astype(int)\n",
    "    # Filter out MCI rows if any remain\n",
    "    train_df = train_df[train_df['label'] != 0.5]\n",
    "    \n",
    "    cnt_0 = len(train_df[train_df['binary']==0])\n",
    "    cnt_1 = len(train_df[train_df['binary']==1])\n",
    "    \n",
    "    if cnt_1 > 0 and cnt_0 > 0:\n",
    "        target = max(cnt_0, cnt_1)\n",
    "        lst = []\n",
    "        for c in [0, 1]:\n",
    "            sub = train_df[train_df['binary'] == c]\n",
    "            if len(sub) > 0:\n",
    "                lst.append(sub.sample(target, replace=True))\n",
    "        train_df = pd.concat(lst)\n",
    "    \n",
    "    print(f\"  Balanced Train: {len(train_df)} slices\")\n",
    "    \n",
    "    train_loader = DataLoader(SwinDataset(train_df, True), batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(SwinDataset(val_df, False), batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_acc = 0; best_wts = None\n",
    "    \n",
    "    # STAGE 1: Warmup\n",
    "    print(\"  ğŸ”¥ Stage 1: Warmup...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"head\" not in name: param.requires_grad = False\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        for img, lbl, _ in tqdm(train_loader, desc=f\"Warmup {epoch+1}\", leave=False):\n",
    "            img, lbl = img.to(device), lbl.to(device)\n",
    "            opt.zero_grad(); criterion(model(img), lbl).backward(); opt.step()\n",
    "            \n",
    "    # STAGE 2: Fine-Tuning\n",
    "    print(\"  ğŸš€ Stage 2: Fine-Tuning...\")\n",
    "    for param in model.parameters(): param.requires_grad = True\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for img, lbl, _ in tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "            img, lbl = img.to(device), lbl.to(device)\n",
    "            opt.zero_grad(); criterion(model(img), lbl).backward(); opt.step()\n",
    "            \n",
    "        # Subject Voting Validation\n",
    "        model.eval()\n",
    "        votes = {}; truth = {}\n",
    "        with torch.no_grad():\n",
    "            for img, lbl, sub in val_loader:\n",
    "                img = img.to(device)\n",
    "                probs = torch.softmax(model(img), dim=1).cpu().numpy()\n",
    "                lbl = lbl.numpy()\n",
    "                for i, s in enumerate(sub):\n",
    "                    if s not in votes: votes[s] = []; truth[s] = lbl[i]\n",
    "                    votes[s].append(probs[i])\n",
    "        \n",
    "        cor = 0\n",
    "        for s in votes:\n",
    "            if np.argmax(np.mean(votes[s], axis=0)) == truth[s]: cor += 1\n",
    "        acc = cor / len(votes) if len(votes) > 0 else 0\n",
    "        \n",
    "        print(f\"Ep {epoch+1}: Val Subject Acc={acc*100:.2f}%\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "    return best_wts, best_acc\n",
    "\n",
    "# 3. Main\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(\"/kaggle/working/dataset_paper_final.csv\"):\n",
    "        print(\"âŒ Run S3 first.\")\n",
    "    else:\n",
    "        df = pd.read_csv(\"/kaggle/working/dataset_paper_final.csv\")\n",
    "        \n",
    "        # STRICT FILTER: 0.0 vs >= 1.0 (Drop 0.5)\n",
    "        df_clean = df[(df['label'] == 0) | (df['label'] >= 1.0)].copy()\n",
    "        df_clean['binary'] = (df_clean['label'] >= 1.0).astype(int)\n",
    "        \n",
    "        # Proper Splitting (Aligned)\n",
    "        subjects = df_clean['subject_id'].unique()\n",
    "        sub_labels = df_clean.groupby('subject_id')['binary'].max()\n",
    "        \n",
    "        # Reindex to match subjects array order (CRITICAL FIX)\n",
    "        y_strat = sub_labels.reindex(subjects).values\n",
    "        \n",
    "        train_subs, temp_subs = train_test_split(subjects, test_size=0.3, stratify=y_strat, random_state=42)\n",
    "        \n",
    "        # Reindex temp labels\n",
    "        y_temp = sub_labels.reindex(temp_subs).values\n",
    "        val_subs, test_subs = train_test_split(temp_subs, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "        \n",
    "        print(f\"Split: {len(train_subs)} Train, {len(val_subs)} Val\")\n",
    "        \n",
    "        # Train Models\n",
    "        df1 = df_clean[(df_clean['roi']==\"Right Hippocampus\") & (df_clean['view']==\"sagittal\")]\n",
    "        wts1, acc1 = train_specialist_strict(\"Right Hippocampus\", \"Sagittal\", df1, train_subs, val_subs)\n",
    "        \n",
    "        df2 = df_clean[(df_clean['roi']==\"Left Parietal Lobe\") & (df_clean['view']==\"coronal\")]\n",
    "        wts2, acc2 = train_specialist_strict(\"Left Parietal Lobe\", \"Coronal\", df2, train_subs, val_subs)\n",
    "        \n",
    "        # Ensemble Test\n",
    "        print(\"\\nâš–ï¸ Running Ensemble on TEST SET...\")\n",
    "        m1 = timm.create_model('swin_tiny_patch4_window7_224', num_classes=2).to(device); m1.load_state_dict(wts1); m1.eval()\n",
    "        m2 = timm.create_model('swin_tiny_patch4_window7_224', num_classes=2).to(device); m2.load_state_dict(wts2); m2.eval()\n",
    "        \n",
    "        test_df1 = df1[df1['subject_id'].isin(test_subs)]\n",
    "        test_df2 = df2[df2['subject_id'].isin(test_subs)]\n",
    "        \n",
    "        l1 = DataLoader(SwinDataset(test_df1, False), batch_size=32, shuffle=False)\n",
    "        l2 = DataLoader(SwinDataset(test_df2, False), batch_size=32, shuffle=False)\n",
    "        \n",
    "        final_preds = {}; targets = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img, lbl, sub in l1:\n",
    "                p = torch.softmax(m1(img.to(device)), 1).cpu().numpy()\n",
    "                for i, s in enumerate(sub):\n",
    "                    if s not in final_preds: final_preds[s] = np.zeros(2); targets[s] = lbl[i].item()\n",
    "                    final_preds[s] += p[i] * acc1\n",
    "            for img, lbl, sub in l2:\n",
    "                p = torch.softmax(m2(img.to(device)), 1).cpu().numpy()\n",
    "                for i, s in enumerate(sub):\n",
    "                    if s in final_preds: final_preds[s] += p[i] * acc2\n",
    "                    \n",
    "        y_p = [np.argmax(final_preds[s]) for s in final_preds]\n",
    "        y_t = [targets[s] for s in final_preds]\n",
    "        \n",
    "        print(f\"ğŸ† ENSEMBLE ACCURACY: {balanced_accuracy_score(y_t, y_p)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63a689",
   "metadata": {
    "papermill": {
     "duration": 1.341955,
     "end_time": "2025-12-07T02:16:31.355346",
     "exception": false,
     "start_time": "2025-12-07T02:16:30.013391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1371285e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T02:16:33.940095Z",
     "iopub.status.busy": "2025-12-07T02:16:33.939391Z",
     "iopub.status.idle": "2025-12-07T02:20:39.158394Z",
     "shell.execute_reply": "2025-12-07T02:20:39.157621Z"
    },
    "papermill": {
     "duration": 246.460173,
     "end_time": "2025-12-07T02:20:39.159551",
     "exception": false,
     "start_time": "2025-12-07T02:16:32.699378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Extracting Original Slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 698/698 [04:05<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Ready. Total Originals: 34900\n",
      "Augmentation Factors set: [ 1  3 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S3: PAPER DATASET (DISK OPTIMIZED)\n",
    "- Extracts ONLY original slices (Saves ~60GB of space)\n",
    "- Marks augmentation targets (x3, x25) in the CSV for S4 to handle\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_slice(arr, path):\n",
    "    np.save(path, arr.astype(np.float16))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"/kaggle/working/processed_volumes.csv\")\n",
    "    df = df[df['preproc_ok'] == True]\n",
    "    \n",
    "    save_dir = \"/kaggle/working/slices_final\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    records = []\n",
    "    \n",
    "    print(\"ğŸš€ Extracting Original Slices...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            vol = nib.load(row['full_t1_path']).get_fdata()\n",
    "            # Middle 50 slices (Active brain regions)\n",
    "            center = vol.shape[2] // 2\n",
    "            slices = vol[:, :, center-25:center+25]\n",
    "            \n",
    "            # Determine Augmentation Factor based on Paper Table 3\n",
    "            # Mild -> x3 | Moderate -> x25 | Others -> x1\n",
    "            aug_factor = 1\n",
    "            if row['cdr_class'] == 1: aug_factor = 3\n",
    "            elif row['cdr_class'] >= 2: aug_factor = 25\n",
    "            \n",
    "            for z in range(slices.shape[2]):\n",
    "                sl = slices[:, :, z]\n",
    "                # Resize to 224x224 (Paper Standard)\n",
    "                sl = zoom(sl, (224/sl.shape[0], 224/sl.shape[1]), order=1)\n",
    "                \n",
    "                fname = f\"{row['subject_id']}_{z}.npy\"\n",
    "                fpath = os.path.join(save_dir, fname)\n",
    "                save_slice(sl, fpath)\n",
    "                \n",
    "                records.append({\n",
    "                    'path': fpath, \n",
    "                    'label': row['cdr_class'], \n",
    "                    'aug_factor': aug_factor  # Store the instruction, not the files\n",
    "                })\n",
    "                    \n",
    "        except: continue\n",
    "\n",
    "    df_final = pd.DataFrame(records)\n",
    "    df_final.to_csv(\"/kaggle/working/dataset_paper.csv\", index=False)\n",
    "    print(f\"âœ… Dataset Ready. Total Originals: {len(df_final)}\")\n",
    "    print(\"Augmentation Factors set:\", df_final['aug_factor'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2a7778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T02:20:41.964554Z",
     "iopub.status.busy": "2025-12-07T02:20:41.964260Z",
     "iopub.status.idle": "2025-12-07T04:42:24.593386Z",
     "shell.execute_reply": "2025-12-07T04:42:24.592522Z"
    },
    "papermill": {
     "duration": 8504.07413,
     "end_time": "2025-12-07T04:42:24.594658",
     "exception": false,
     "start_time": "2025-12-07T02:20:40.520528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Training (Batch 16 + Accumulation 4 + AMP)...\n",
      "Dataset (Train) initialized: 27920 originals -> 108720 virtual samples\n",
      "Dataset (Test) initialized: 6980 originals -> 6980 virtual samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 183MB/s]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:10<00:00, 12.35it/s, acc=87.1%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Val Bal Acc=64.96%\n",
      "ğŸ† New Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:09<00:00, 12.37it/s, acc=93.5%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Val Bal Acc=66.26%\n",
      "ğŸ† New Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:07<00:00, 12.40it/s, acc=94.1%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: Val Bal Acc=59.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:10<00:00, 12.35it/s, acc=94.4%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: Val Bal Acc=64.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:16<00:00, 12.22it/s, acc=94.4%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: Val Bal Acc=60.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:17<00:00, 12.20it/s, acc=94.7%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: Val Bal Acc=71.51%\n",
      "ğŸ† New Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:16<00:00, 12.20it/s, acc=94.6%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: Val Bal Acc=66.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:18<00:00, 12.16it/s, acc=94.8%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: Val Bal Acc=68.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:19<00:00, 12.15it/s, acc=94.7%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9: Val Bal Acc=88.29%\n",
      "ğŸ† New Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:20<00:00, 12.13it/s, acc=94.8%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: Val Bal Acc=62.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:21<00:00, 12.11it/s, acc=94.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11: Val Bal Acc=87.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:16<00:00, 12.22it/s, acc=94.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12: Val Bal Acc=71.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:15<00:00, 12.24it/s, acc=94.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13: Val Bal Acc=69.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:14<00:00, 12.26it/s, acc=94.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14: Val Bal Acc=87.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6795/6795 [09:15<00:00, 12.24it/s, acc=94.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15: Val Bal Acc=91.63%\n",
      "ğŸ† New Best!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S4: PAPER MODEL (INDEX FIXED + MEMORY SAFE)\n",
    "- Fixes IndexError by resetting DataFrame index\n",
    "- Fixes OOM by using Batch Size 16 + Gradient Accumulation\n",
    "- Replicates 98% Accuracy Methodology\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "\n",
    "# Memory Cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PAPER ARCHITECTURE\n",
    "# ============================================================================\n",
    "class HybridResNetViT(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.backbone[0] = nn.Conv2d(1, 64, 7, 2, 3, bias=False)\n",
    "        self.patch_embed = nn.Conv2d(2048, 64, kernel_size=1) \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=2048, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, 64))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 64))\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "        self.fc = nn.Sequential(nn.Linear(64, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        x = x + self.pos_embed\n",
    "        cls = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(self.norm(x[:, 0]))\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATASET (FIXED INDEXING)\n",
    "# ============================================================================\n",
    "class PaperAugDataset(Dataset):\n",
    "    def __init__(self, df, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # CRITICAL FIX: Reset index so iloc works correctly\n",
    "        self.df = df.reset_index(drop=True) \n",
    "        \n",
    "        self.indices = []\n",
    "        # Build index list with repetition based on aug_factor\n",
    "        for idx, row in self.df.iterrows():\n",
    "            count = int(row['aug_factor']) if is_train else 1\n",
    "            self.indices.extend([idx] * count)\n",
    "            \n",
    "        print(f\"Dataset ({'Train' if is_train else 'Test'}) initialized: {len(self.df)} originals -> {len(self.indices)} virtual samples\")\n",
    "\n",
    "    def __len__(self): return len(self.indices)\n",
    "\n",
    "    def transform(self, img_tensor):\n",
    "        if random.random() > 0.5: img_tensor = TF.hflip(img_tensor)\n",
    "        angle = random.uniform(-15, 15)\n",
    "        img_tensor = TF.rotate(img_tensor, angle)\n",
    "        return img_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        \n",
    "        # Now safe because index is 0..N\n",
    "        row = self.df.iloc[real_idx] \n",
    "        \n",
    "        img = np.load(row['path']).astype(np.float32)\n",
    "        tensor = torch.tensor(img[None, :, :]) \n",
    "        \n",
    "        if self.is_train and row['aug_factor'] > 1:\n",
    "            tensor = self.transform(tensor)\n",
    "            \n",
    "        return tensor, torch.tensor(row['label'], dtype=torch.long)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRAINING (SAFE MODE)\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Training (Batch 16 + Accumulation 4 + AMP)...\")\n",
    "    \n",
    "    df = pd.read_csv(\"/kaggle/working/dataset_paper.csv\")\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    \n",
    "    # Batch 16 is Safe for T4\n",
    "    train_loader = DataLoader(PaperAugDataset(train_df, True), batch_size=16, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(PaperAugDataset(test_df, False), batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model = HybridResNetViT(num_classes=4).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_acc = 0\n",
    "    accum_steps = 4  # 16 * 4 = 64 (Paper's Effective Batch Size)\n",
    "    \n",
    "    for epoch in range(15): \n",
    "        model.train()\n",
    "        correct = 0; total = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for i, (imgs, lbls) in enumerate(pbar):\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, lbls) / accum_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "            pbar.set_postfix({'acc': f\"{100*correct/total:.1f}%\"})\n",
    "            \n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in test_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                with autocast():\n",
    "                    outputs = model(imgs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(lbls.cpu().numpy())\n",
    "        \n",
    "        acc = balanced_accuracy_score(all_targets, all_preds) * 100\n",
    "        print(f\"Ep {epoch+1}: Val Bal Acc={acc:.2f}%\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"paper_model_best.pth\")\n",
    "            print(\"ğŸ† New Best!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1980,
     "sourceId": 3398,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8690252,
     "sourceId": 13667832,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8691044,
     "sourceId": 13668930,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8746096,
     "sourceId": 13745173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8747805,
     "sourceId": 13747487,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23346.529714,
   "end_time": "2025-12-07T04:42:35.634237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-06T22:13:29.104523",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4e5f041935f4410288220864e8afefe7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ed14c13e2514fc0a85227d3ae78ae51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ba8e9b0767a4169b8bb8644b9c5b049": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9882b6ccae144dc687e9ef350f2d62fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad1c576d76334505a32bd03a2b0d40b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfdbb2880a0647bfa02ba2fd0b3a03c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c5b9b964e2e14c30bdad1c30ad73a244": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dbce990e102d4c4a8ca72894baf68338": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f0bd79de0f964c44b430b5d6389a42b5",
        "IPY_MODEL_ee6272c4d5454336b77eebd2cd7876f6",
        "IPY_MODEL_e1aea6b9ea574c1daa43e43429c20be4"
       ],
       "layout": "IPY_MODEL_4e5f041935f4410288220864e8afefe7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e1aea6b9ea574c1daa43e43429c20be4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5b9b964e2e14c30bdad1c30ad73a244",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9882b6ccae144dc687e9ef350f2d62fb",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡114M/114Mâ€‡[00:01&lt;00:00,â€‡92.7MB/s]"
      }
     },
     "ee6272c4d5454336b77eebd2cd7876f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6ed14c13e2514fc0a85227d3ae78ae51",
       "max": 114286722.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bfdbb2880a0647bfa02ba2fd0b3a03c5",
       "tabbable": null,
       "tooltip": null,
       "value": 114286722.0
      }
     },
     "f0bd79de0f964c44b430b5d6389a42b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ad1c576d76334505a32bd03a2b0d40b1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7ba8e9b0767a4169b8bb8644b9c5b049",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:â€‡100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
