{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02adc192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:16:07.992920Z",
     "iopub.status.busy": "2025-11-29T19:16:07.992671Z",
     "iopub.status.idle": "2025-11-29T19:16:33.230766Z",
     "shell.execute_reply": "2025-11-29T19:16:33.229886Z"
    },
    "papermill": {
     "duration": 25.249823,
     "end_time": "2025-11-29T19:16:33.232342",
     "exception": false,
     "start_time": "2025-11-29T19:16:07.982519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ SNIPPET S1: DATA AUDIT & SUBJECT/VISIT TABLES (REVISION 3)\n",
      "   KEY FIX: OASIS-3 nearest-neighbor temporal matching\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Scanning OASIS-2 MRI directories\n",
      "======================================================================\n",
      "\n",
      "Scanning PART1: /kaggle/input/oaisis-dataset-3-p1/OAS2_RAW_PART1\n",
      "  ‚úì Found 209 valid MRI sessions\n",
      "\n",
      "Scanning PART2: /kaggle/input/oaisis-3-p2/OAS2_RAW_PART2\n",
      "  ‚úì Found 164 valid MRI sessions\n",
      "\n",
      "üìä OASIS-2 Total: 373 MRI sessions from 150 subjects\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Scanning OASIS-3 MRI directories\n",
      "======================================================================\n",
      "Root: /kaggle/input/oaisis-3/oaisis3\n",
      "  ‚úì Found 423 valid T1w sessions\n",
      "\n",
      "üìä OASIS-3 Total: 423 MRI sessions from 300 subjects\n",
      "\n",
      "üîç Sample MRI sessions (first 5):\n",
      "   OAS30354_MR_d0056 ‚Üí subject=OAS30354, mri_days=56\n",
      "   OAS30083_MR_d3827 ‚Üí subject=OAS30083, mri_days=3827\n",
      "   OAS31019_MR_d1370 ‚Üí subject=OAS31019, mri_days=1370\n",
      "   OAS30208_MR_d1703 ‚Üí subject=OAS30208, mri_days=1703\n",
      "   OAS30830_MR_d0030 ‚Üí subject=OAS30830, mri_days=30\n",
      "\n",
      "======================================================================\n",
      "STEP 3a: Loading OASIS-2 Clinical CSV\n",
      "======================================================================\n",
      "Path: /kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv\n",
      "‚úì Loaded 373 rows, 15 columns\n",
      "\n",
      "‚úì Column mapping:\n",
      "  subject_col     -> 'Subject ID'\n",
      "  visit_col       -> 'Visit'\n",
      "  sex_col         -> 'M/F'\n",
      "  age_col         -> 'Age'\n",
      "  mmse_col        -> 'MMSE'\n",
      "  cdr_col         -> 'CDR'\n",
      "\n",
      "‚úì Standardized 373 clinical records\n",
      "\n",
      "======================================================================\n",
      "STEP 3b: Loading OASIS-3 Clinical CSV\n",
      "======================================================================\n",
      "Path: /kaggle/input/oaisis-3-longitiudinal/oaisis3longitiudinal.csv\n",
      "‚úì Loaded 8626 rows, 23 columns\n",
      "\n",
      "‚úì Column mapping (hard-coded for OASIS-3):\n",
      "  subject_col     -> 'OASISID'\n",
      "  session_col     -> 'OASIS_session_label'\n",
      "  age_col         -> 'age at visit'\n",
      "  mmse_col        -> 'MMSE'\n",
      "  cdr_col         -> 'CDRTOT'\n",
      "  days_col        -> 'days_to_visit'\n",
      "\n",
      "‚úì Standardized 8626 OASIS-3 clinical records\n",
      "\n",
      "üìä OASIS-3 CDR/MMSE Sanity Check:\n",
      "               CDR         MMSE\n",
      "count  8625.000000  7593.000000\n",
      "mean      0.187478    28.110628\n",
      "std       0.399566     3.024424\n",
      "min       0.000000     0.000000\n",
      "25%       0.000000    28.000000\n",
      "50%       0.000000    29.000000\n",
      "75%       0.000000    30.000000\n",
      "max       3.000000    30.000000\n",
      "\n",
      "CDR unique values: [0.0, 0.5, 1.0, 2.0, 3.0]\n",
      "\n",
      "üîç Sample clinical records (first 5):\n",
      "   subject=OAS30001, days=0, CDR=0.0, label=OAS30001_UDSb4_d0000\n",
      "   subject=OAS30001, days=339, CDR=0.0, label=OAS30001_UDSb4_d0339\n",
      "   subject=OAS30001, days=722, CDR=0.0, label=OAS30001_UDSb4_d0722\n",
      "   subject=OAS30001, days=1106, CDR=0.0, label=OAS30001_UDSb4_d1106\n",
      "   subject=OAS30001, days=1456, CDR=0.0, label=OAS30001_UDSb4_d1456\n",
      "\n",
      "======================================================================\n",
      "STEP 4a: Merging OASIS-2 MRI + Clinical\n",
      "======================================================================\n",
      "MRI visits before merge: 373\n",
      "Clinical records: 373\n",
      "‚úì After merge: 373 visits\n",
      "‚úì After CDR/MMSE filter: 371 visits (dropped 2)\n",
      "‚úì After label filter (CN/AD only): 248 visits (dropped 123)\n",
      "\n",
      "======================================================================\n",
      "STEP 4b: Merging OASIS-3 MRI + Clinical (nearest-day matching)\n",
      "======================================================================\n",
      "MRI visits before merge: 423\n",
      "Clinical records: 8626\n",
      "‚úì MRI sessions with valid days: 423\n",
      "‚úì Clinical records with valid days: 8626\n",
      "‚úì Total (subject, MRI, clinical) combinations: 3617\n",
      "‚úì After day_diff ‚â§ 365 filter: 696 pairs (dropped 2921)\n",
      "‚úì After selecting nearest clinical visit per MRI: 416 visits\n",
      "\n",
      "üîç Sample matches (first 5):\n",
      "   MRI: OAS30001_MR_d0129 (day 129)\n",
      "     ‚Üí Clinical: OAS30001_UDSb4_d0000 (day 0)\n",
      "     ‚Üí Diff: 129 days, CDR=0.0, MMSE=28.0\n",
      "   MRI: OAS30001_MR_d0757 (day 757)\n",
      "     ‚Üí Clinical: OAS30001_UDSb4_d0722 (day 722)\n",
      "     ‚Üí Diff: 35 days, CDR=0.0, MMSE=30.0\n",
      "   MRI: OAS30006_MR_d2341 (day 2341)\n",
      "     ‚Üí Clinical: OAS30006_UDSb4_d2267 (day 2267)\n",
      "     ‚Üí Diff: 74 days, CDR=0.0, MMSE=30.0\n",
      "   MRI: OAS30006_MR_d2342 (day 2342)\n",
      "     ‚Üí Clinical: OAS30006_UDSb4_d2267 (day 2267)\n",
      "     ‚Üí Diff: 75 days, CDR=0.0, MMSE=30.0\n",
      "   MRI: OAS30011_MR_d0055 (day 55)\n",
      "     ‚Üí Clinical: OAS30011_UDSb4_d0000 (day 0)\n",
      "     ‚Üí Diff: 55 days, CDR=0.0, MMSE=28.0\n",
      "\n",
      "‚úì After CDR/MMSE filter: 415 visits (dropped 1)\n",
      "‚úì After label filter (CN/AD only): 327 visits (dropped 88)\n",
      "‚úì Assigned visit_index using days_to_visit\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Building Unified Visits Table\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Saved visits_table.csv: 575 visits\n",
      "\n",
      "üìä OASIS-3 Temporal Matching Quality:\n",
      "   Mean day difference: 92.7 days\n",
      "   Median day difference: 83.0 days\n",
      "   Max day difference: 302.0 days\n",
      "   Within 30 days: 35 visits (10.7%)\n",
      "   Within 90 days: 180 visits (55.0%)\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Building Subjects Table\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Saved subjects_table.csv: 346 subjects\n",
      "\n",
      "======================================================================\n",
      "STEP 7: QUALITY CONTROL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä VISIT-LEVEL STATISTICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Visits by dataset and label:\n",
      "          CN  AD\n",
      "dataset         \n",
      "OASIS2   206  42\n",
      "OASIS3   293  34\n",
      "\n",
      "Total visits:             575\n",
      "CN visits:                499 (86.8%)\n",
      "AD visits:                76 (13.2%)\n",
      "CN:AD ratio:              6.57:1\n",
      "\n",
      "\n",
      "üìä SUBJECT-LEVEL STATISTICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Subjects by dataset:\n",
      "  OASIS2      111 subjects\n",
      "  OASIS3      235 subjects\n",
      "  TOTAL       346 subjects\n",
      "\n",
      "\n",
      "üìä LONGITUDINAL STRUCTURE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Subjects with ‚â•2 visits:\n",
      "  OASIS2       89 subjects\n",
      "  OASIS3       92 subjects\n",
      "\n",
      "Distribution of visits per subject:\n",
      "   1 visits:  165 subjects\n",
      "   2 visits:  146 subjects\n",
      "   3 visits:   25 subjects\n",
      "   4 visits:    7 subjects\n",
      "   5 visits:    3 subjects\n",
      "\n",
      "\n",
      "üìä CONVERTER ANALYSIS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Subjects with BOTH CN and AD labels (potential converters): 1\n",
      "\n",
      "Top 10 converters:\n",
      "    dataset subject_id  n_CN_visits  n_AD_visits  n_visits_total\n",
      "271  OASIS3   OAS30830            1            1               2\n",
      "\n",
      "\n",
      "üìä DATA QUALITY CHECKS\n",
      "----------------------------------------------------------------------\n",
      "‚úì Missing mri_path values: 0 (should be 0)\n",
      "‚úì Age range: 60.0 - 98.0 years (mean: 75.1)\n",
      "‚úì MMSE range: 4 - 30 (mean: 28.0)\n",
      "\n",
      "\n",
      "üìä SUCCESS CRITERIA ASSESSMENT\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ PASS  Total visits ‚â• 400\n",
      "‚úÖ PASS  Both datasets present\n",
      "‚úÖ PASS  CN:AD ratio 3:1 to 10:1\n",
      "‚úÖ PASS  OASIS-2 has CN and AD\n",
      "‚úÖ PASS  OASIS-3 has CN and AD\n",
      "‚úÖ PASS  Longitudinal subjects ‚â• 50\n",
      "‚úÖ PASS  No missing MRI paths\n",
      "\n",
      "======================================================================\n",
      "üéâ ALL SUCCESS CRITERIA MET - S1 ACCEPTED\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìã PREVIEW: visits_table.csv\n",
      "======================================================================\n",
      "\n",
      "First 5 OASIS-2 visits:\n",
      "subject_id visit_id  visit_index  CDR  MMSE  Age  label\n",
      " OAS2_0079      MR2            2  1.0  16.0 71.0      1\n",
      " OAS2_0044      MR1            1  1.0  21.0 68.0      1\n",
      " OAS2_0056      MR2            2  0.0  30.0 73.0      0\n",
      " OAS2_0062      MR3            3  0.0  29.0 83.0      0\n",
      " OAS2_0062      MR2            2  0.0  30.0 81.0      0\n",
      "\n",
      "First 5 OASIS-3 visits:\n",
      "subject_id visit_id  visit_index  CDR  MMSE   Age  label  day_diff\n",
      "  OAS30001    d0129            1  0.0  28.0 65.19      0     129.0\n",
      "  OAS30001    d0757            2  0.0  30.0 67.17      0      35.0\n",
      "  OAS30006    d2341            1  0.0  30.0 68.34      0      74.0\n",
      "  OAS30006    d2342            2  0.0  30.0 68.34      0      75.0\n",
      "  OAS30011    d0055            1  0.0  28.0 78.52      0      55.0\n",
      "\n",
      "======================================================================\n",
      "üìã PREVIEW: subjects_table.csv (first 10 rows)\n",
      "======================================================================\n",
      "dataset  domain_id subject_id Sex  baseline_age  n_visits_total  n_CN_visits  n_AD_visits  has_longitudinal\n",
      " OASIS2          0  OAS2_0001   M          87.0               2            2            0                 1\n",
      " OASIS2          0  OAS2_0004   F          88.0               2            2            0                 1\n",
      " OASIS2          0  OAS2_0005   M          80.0               2            2            0                 1\n",
      " OASIS2          0  OAS2_0007   M          73.0               2            0            2                 1\n",
      " OASIS2          0  OAS2_0008   F          93.0               2            2            0                 1\n",
      " OASIS2          0  OAS2_0012   F          78.0               3            3            0                 1\n",
      " OASIS2          0  OAS2_0013   F          81.0               3            3            0                 1\n",
      " OASIS2          0  OAS2_0014   M          77.0               1            0            1                 0\n",
      " OASIS2          0  OAS2_0017   M          80.0               3            3            0                 1\n",
      " OASIS2          0  OAS2_0018   F          87.0               2            2            0                 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S1: Data Audit & Subject/Visit Tables (REVISION 3 - Nearest-Day Matching)\n",
    "Builds canonical visits_table.csv and subjects_table.csv from OASIS-2 + OASIS-3\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def standardize_oasis2_subject(raw_id):\n",
    "    \"\"\"Ensure OASIS-2 subject ID is in format OAS2_XXXX\"\"\"\n",
    "    if pd.isna(raw_id):\n",
    "        return None\n",
    "    raw_id = str(raw_id).strip()\n",
    "    if raw_id.startswith(\"OAS2_\"):\n",
    "        return raw_id\n",
    "    elif raw_id.isdigit():\n",
    "        return f\"OAS2_{raw_id.zfill(4)}\"\n",
    "    else:\n",
    "        parts = raw_id.replace(\"OAS\", \"\").replace(\"_\", \"\").strip()\n",
    "        if parts.isdigit():\n",
    "            return f\"OAS2_{parts.zfill(4)}\"\n",
    "    return raw_id\n",
    "\n",
    "def choose_canonical_t1(candidates):\n",
    "    \"\"\"Select preferred T1w file from multiple candidates (prefer run-01)\"\"\"\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    \n",
    "    # Prefer run-01\n",
    "    for c in candidates:\n",
    "        if 'run-01' in c or 'run-1' in c:\n",
    "            return c\n",
    "    \n",
    "    # Otherwise take first alphabetically\n",
    "    return sorted(candidates)[0]\n",
    "\n",
    "def assign_oasis3_visit_index(df):\n",
    "    \"\"\"\n",
    "    Assign chronological visit_index (1, 2, 3...) based on days_to_visit.\n",
    "    For subjects with multiple visits, sort by days and number sequentially.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"subject_id\", \"days_to_visit\"])\n",
    "    df[\"visit_index\"] = df.groupby(\"subject_id\").cumcount() + 1\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: SCAN OASIS-2 MRI DIRECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "def scan_oasis2_mri(root_part1, root_part2):\n",
    "    \"\"\"Scan OASIS-2 Part 1 and Part 2 for MRI visit directories\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: Scanning OASIS-2 MRI directories\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for root_label, root_path in [(\"PART1\", root_part1), (\"PART2\", root_part2)]:\n",
    "        print(f\"\\nScanning {root_label}: {root_path}\")\n",
    "        \n",
    "        if not os.path.exists(root_path):\n",
    "            print(f\"  ‚ö†Ô∏è  WARNING: Path does not exist, skipping\")\n",
    "            continue\n",
    "        \n",
    "        subdirs = [d for d in os.listdir(root_path) \n",
    "                   if os.path.isdir(os.path.join(root_path, d))]\n",
    "        \n",
    "        count_valid = 0\n",
    "        count_missing = 0\n",
    "        \n",
    "        for dir_name in subdirs:\n",
    "            if not dir_name.startswith(\"OAS2_\"):\n",
    "                continue\n",
    "            \n",
    "            # Parse: OAS2_0001_MR1 -> subject=\"OAS2_0001\", visit=\"MR1\", index=1\n",
    "            parts = dir_name.split(\"_\")\n",
    "            if len(parts) != 3:\n",
    "                print(f\"  ‚ö†Ô∏è  Unexpected format: {dir_name}\")\n",
    "                continue\n",
    "            \n",
    "            subject_id = f\"{parts[0]}_{parts[1]}\"  # \"OAS2_0001\"\n",
    "            visit_id = parts[2]                     # \"MR1\"\n",
    "            \n",
    "            # Extract visit index\n",
    "            try:\n",
    "                visit_index = int(visit_id.replace(\"MR\", \"\"))\n",
    "            except ValueError:\n",
    "                print(f\"  ‚ö†Ô∏è  Cannot parse visit index: {dir_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Check for MRI file\n",
    "            visit_dir = os.path.join(root_path, dir_name, \"RAW\")\n",
    "            mri_path = os.path.join(visit_dir, \"mpr-1.nifti.hdr\")\n",
    "            \n",
    "            if not os.path.exists(mri_path):\n",
    "                alt_path = os.path.join(visit_dir, \"mpr-1.hdr\")\n",
    "                if os.path.exists(alt_path):\n",
    "                    mri_path = alt_path\n",
    "                else:\n",
    "                    count_missing += 1\n",
    "                    continue\n",
    "            \n",
    "            records.append({\n",
    "                \"dataset\": \"OASIS2\",\n",
    "                \"domain_id\": 0,\n",
    "                \"subject_id\": subject_id,\n",
    "                \"visit_id\": visit_id,\n",
    "                \"visit_index\": visit_index,\n",
    "                \"mri_path\": mri_path,\n",
    "            })\n",
    "            count_valid += 1\n",
    "        \n",
    "        print(f\"  ‚úì Found {count_valid} valid MRI sessions\")\n",
    "        if count_missing > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Skipped {count_missing} sessions (missing MRI file)\")\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"\\nüìä OASIS-2 Total: {len(df)} MRI sessions from {df['subject_id'].nunique()} subjects\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: SCAN OASIS-3 MRI DIRECTORIES (FIXED - Extract mri_days)\n",
    "# ============================================================================\n",
    "\n",
    "def scan_oasis3_mri(root_o3):\n",
    "    \"\"\"\n",
    "    Scan OASIS-3 for T1w NIfTI files\n",
    "    FIX: Extract numeric mri_days from directory name for temporal matching\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Scanning OASIS-3 MRI directories\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Root: {root_o3}\")\n",
    "    \n",
    "    if not os.path.exists(root_o3):\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Path does not exist\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    records = []\n",
    "    subdirs = [d for d in os.listdir(root_o3) \n",
    "               if os.path.isdir(os.path.join(root_o3, d))]\n",
    "    \n",
    "    count_valid = 0\n",
    "    count_no_t1 = 0\n",
    "    count_bad_days = 0\n",
    "    \n",
    "    for dir_name in subdirs:\n",
    "        if not dir_name.startswith(\"OAS3\"):\n",
    "            continue\n",
    "        \n",
    "        if \"_MR_\" not in dir_name:\n",
    "            continue\n",
    "        \n",
    "        # Parse: \"OAS30006_MR_d2341\" -> subject=\"OAS30006\", visit=\"d2341\", days=2341\n",
    "        mri_session_label = dir_name  # Keep full label for reference\n",
    "        \n",
    "        subject_part, _, visit_part = dir_name.partition(\"_MR_\")\n",
    "        subject_id = subject_part  # \"OAS30006\"\n",
    "        visit_id = visit_part      # \"d2341\"\n",
    "        \n",
    "        # FIX: Extract numeric days from visit_id\n",
    "        try:\n",
    "            mri_days = int(visit_id.replace(\"d\", \"\"))  # 2341\n",
    "        except ValueError:\n",
    "            count_bad_days += 1\n",
    "            mri_days = np.nan\n",
    "        \n",
    "        # Search for T1w NIfTI files\n",
    "        session_dir = os.path.join(root_o3, dir_name)\n",
    "        t1_candidates = []\n",
    "        \n",
    "        for anat_dir in glob.glob(os.path.join(session_dir, \"anat*\", \"NIFTI\")):\n",
    "            t1_files = glob.glob(os.path.join(anat_dir, \"**\", \"*T1w.nii\"), recursive=True)\n",
    "            t1_candidates.extend(t1_files)\n",
    "        \n",
    "        if len(t1_candidates) == 0:\n",
    "            count_no_t1 += 1\n",
    "            continue\n",
    "        \n",
    "        mri_path = choose_canonical_t1(t1_candidates)\n",
    "        \n",
    "        records.append({\n",
    "            \"dataset\": \"OASIS3\",\n",
    "            \"domain_id\": 1,\n",
    "            \"subject_id\": subject_id,\n",
    "            \"mri_session_label\": mri_session_label,  # Full MRI directory name\n",
    "            \"visit_id\": visit_id,                    # \"d2341\"\n",
    "            \"mri_days\": mri_days,                    # ‚Üê NEW: numeric days for matching\n",
    "            \"visit_index\": None,                     # Will assign after merge\n",
    "            \"mri_path\": mri_path,\n",
    "        })\n",
    "        count_valid += 1\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"  ‚úì Found {count_valid} valid T1w sessions\")\n",
    "    if count_no_t1 > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipped {count_no_t1} sessions (no T1w file)\")\n",
    "    if count_bad_days > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipped {count_bad_days} sessions (invalid day format)\")\n",
    "    \n",
    "    print(f\"\\nüìä OASIS-3 Total: {len(df)} MRI sessions from {df['subject_id'].nunique()} subjects\")\n",
    "    \n",
    "    # DEBUG: Show sample data\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\nüîç Sample MRI sessions (first 5):\")\n",
    "        for _, row in df.head(5).iterrows():\n",
    "            print(f\"   {row['mri_session_label']} ‚Üí subject={row['subject_id']}, mri_days={row['mri_days']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: LOAD CLINICAL CSVs\n",
    "# ============================================================================\n",
    "\n",
    "def load_oasis2_clinical(csv_path):\n",
    "    \"\"\"Load and standardize OASIS-2 clinical CSV\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3a: Loading OASIS-2 Clinical CSV\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Path: {csv_path}\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"OASIS-2 clinical CSV not found: {csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úì Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Auto-detect columns\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if 'subject' in col_lower or col_lower == 'id':\n",
    "            col_map['subject_col'] = col\n",
    "        elif 'visit' in col_lower and 'mri' not in col_lower:\n",
    "            col_map['visit_col'] = col\n",
    "        elif col_lower == 'cdr':\n",
    "            col_map['cdr_col'] = col\n",
    "        elif 'mmse' in col_lower:\n",
    "            col_map['mmse_col'] = col\n",
    "        elif col_lower == 'age':\n",
    "            col_map['age_col'] = col\n",
    "        elif 'm/f' in col_lower or col_lower == 'sex':\n",
    "            col_map['sex_col'] = col\n",
    "    \n",
    "    # Validate\n",
    "    required = ['subject_col', 'visit_col', 'cdr_col', 'mmse_col']\n",
    "    missing = [k for k in required if k not in col_map]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in OASIS-2 CSV: {missing}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Column mapping:\")\n",
    "    for k, v in col_map.items():\n",
    "        print(f\"  {k:15} -> '{v}'\")\n",
    "    \n",
    "    # Standardize\n",
    "    df['subject_id'] = df[col_map['subject_col']].apply(standardize_oasis2_subject)\n",
    "    df['visit_index'] = pd.to_numeric(df[col_map['visit_col']], errors='coerce')\n",
    "    df['CDR'] = pd.to_numeric(df[col_map['cdr_col']], errors='coerce')\n",
    "    df['MMSE'] = pd.to_numeric(df[col_map['mmse_col']], errors='coerce')\n",
    "    \n",
    "    if 'age_col' in col_map:\n",
    "        df['Age'] = pd.to_numeric(df[col_map['age_col']], errors='coerce')\n",
    "    if 'sex_col' in col_map:\n",
    "        df['Sex'] = df[col_map['sex_col']].astype(str).str.strip()\n",
    "    \n",
    "    # Select columns\n",
    "    keep_cols = ['subject_id', 'visit_index', 'CDR', 'MMSE']\n",
    "    if 'Age' in df.columns:\n",
    "        keep_cols.append('Age')\n",
    "    if 'Sex' in df.columns:\n",
    "        keep_cols.append('Sex')\n",
    "    \n",
    "    df_clean = df[keep_cols].copy()\n",
    "    print(f\"\\n‚úì Standardized {len(df_clean)} clinical records\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def load_oasis3_clinical(csv_path):\n",
    "    \"\"\"\n",
    "    Load and standardize OASIS-3 clinical CSV\n",
    "    FIXED: Keep clinical session label separate, use days_to_visit for matching\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3b: Loading OASIS-3 Clinical CSV\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Path: {csv_path}\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"OASIS-3 clinical CSV not found: {csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úì Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Hard-coded column mapping for OASIS-3\n",
    "    subject_col = \"OASISID\"\n",
    "    session_col = \"OASIS_session_label\"\n",
    "    cdr_col = \"CDRTOT\"\n",
    "    mmse_col = \"MMSE\"\n",
    "    age_col = \"age at visit\"\n",
    "    days_col = \"days_to_visit\"\n",
    "    \n",
    "    # Validate\n",
    "    required_cols = [subject_col, session_col, cdr_col, mmse_col, age_col, days_col]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in OASIS-3 CSV: {missing}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Column mapping (hard-coded for OASIS-3):\")\n",
    "    print(f\"  subject_col     -> '{subject_col}'\")\n",
    "    print(f\"  session_col     -> '{session_col}'\")\n",
    "    print(f\"  age_col         -> '{age_col}'\")\n",
    "    print(f\"  mmse_col        -> '{mmse_col}'\")\n",
    "    print(f\"  cdr_col         -> '{cdr_col}'\")\n",
    "    print(f\"  days_col        -> '{days_col}'\")\n",
    "    \n",
    "    # Standardize IDs\n",
    "    df['subject_id'] = df[subject_col].astype(str).str.strip()\n",
    "    \n",
    "    # Keep clinical session label (different from MRI label!)\n",
    "    df['clin_session_label'] = df[session_col].astype(str).str.strip()\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={\n",
    "        cdr_col: \"CDR\",\n",
    "        mmse_col: \"MMSE\",\n",
    "        age_col: \"Age\",\n",
    "        days_col: \"days_to_visit\",\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    df['CDR'] = pd.to_numeric(df['CDR'], errors='coerce')\n",
    "    df['MMSE'] = pd.to_numeric(df['MMSE'], errors='coerce')\n",
    "    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    df['days_to_visit'] = pd.to_numeric(df['days_to_visit'], errors='coerce')\n",
    "    \n",
    "    # Select columns\n",
    "    keep_cols = ['subject_id', 'clin_session_label', 'CDR', 'MMSE', 'Age', 'days_to_visit']\n",
    "    df_clean = df[keep_cols].copy()\n",
    "    \n",
    "    print(f\"\\n‚úì Standardized {len(df_clean)} OASIS-3 clinical records\")\n",
    "    \n",
    "    # Sanity check\n",
    "    print(\"\\nüìä OASIS-3 CDR/MMSE Sanity Check:\")\n",
    "    print(df_clean[['CDR', 'MMSE']].describe())\n",
    "    print(f\"\\nCDR unique values: {sorted(df_clean['CDR'].dropna().unique())}\")\n",
    "    \n",
    "    # DEBUG: Show sample data\n",
    "    print(f\"\\nüîç Sample clinical records (first 5):\")\n",
    "    for _, row in df_clean.head(5).iterrows():\n",
    "        print(f\"   subject={row['subject_id']}, days={row['days_to_visit']}, CDR={row['CDR']}, label={row['clin_session_label']}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: MERGE MRI + CLINICAL\n",
    "# ============================================================================\n",
    "\n",
    "def merge_oasis2(mri_df, clinical_df):\n",
    "    \"\"\"Merge OASIS-2 MRI visits with clinical data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4a: Merging OASIS-2 MRI + Clinical\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"MRI visits before merge: {len(mri_df)}\")\n",
    "    print(f\"Clinical records: {len(clinical_df)}\")\n",
    "    \n",
    "    merged = mri_df.merge(\n",
    "        clinical_df,\n",
    "        on=['subject_id', 'visit_index'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì After merge: {len(merged)} visits\")\n",
    "    \n",
    "    # Filter: require valid CDR and MMSE\n",
    "    before_filter = len(merged)\n",
    "    merged = merged.dropna(subset=['CDR', 'MMSE'])\n",
    "    print(f\"‚úì After CDR/MMSE filter: {len(merged)} visits (dropped {before_filter - len(merged)})\")\n",
    "    \n",
    "    # Assign labels\n",
    "    def assign_label(cdr):\n",
    "        if cdr == 0:\n",
    "            return 0   # CN\n",
    "        elif cdr >= 1:\n",
    "            return 1   # AD\n",
    "        else:\n",
    "            return -1  # Exclude\n",
    "    \n",
    "    merged['label'] = merged['CDR'].apply(assign_label)\n",
    "    \n",
    "    # Keep only CN/AD\n",
    "    before_label = len(merged)\n",
    "    merged = merged[merged['label'].isin([0, 1])]\n",
    "    print(f\"‚úì After label filter (CN/AD only): {len(merged)} visits (dropped {before_label - len(merged)})\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge_oasis3(mri_df, clinical_df, max_day_diff=365):\n",
    "    \"\"\"\n",
    "    Merge OASIS-3 MRI visits with clinical data using nearest-neighbor temporal matching\n",
    "    \n",
    "    FIXED: Match by (subject_id + nearest days_to_visit) instead of exact session label\n",
    "    \n",
    "    Args:\n",
    "        mri_df: MRI sessions with mri_days\n",
    "        clinical_df: Clinical assessments with days_to_visit\n",
    "        max_day_diff: Maximum allowed day difference (default 365 days = 1 year)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4b: Merging OASIS-3 MRI + Clinical (nearest-day matching)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"MRI visits before merge: {len(mri_df)}\")\n",
    "    print(f\"Clinical records: {len(clinical_df)}\")\n",
    "    \n",
    "    # Drop MRI sessions without valid mri_days\n",
    "    mri_valid = mri_df.dropna(subset=[\"mri_days\"]).copy()\n",
    "    mri_valid[\"mri_days\"] = mri_valid[\"mri_days\"].astype(float)\n",
    "    print(f\"‚úì MRI sessions with valid days: {len(mri_valid)}\")\n",
    "    \n",
    "    # Drop clinical records without valid days_to_visit\n",
    "    clinical_valid = clinical_df.dropna(subset=[\"days_to_visit\"]).copy()\n",
    "    clinical_valid[\"days_to_visit\"] = clinical_valid[\"days_to_visit\"].astype(float)\n",
    "    print(f\"‚úì Clinical records with valid days: {len(clinical_valid)}\")\n",
    "    \n",
    "    # Cartesian join on subject_id (all combinations within each subject)\n",
    "    merged = mri_valid.merge(\n",
    "        clinical_valid,\n",
    "        on=\"subject_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_mri\", \"_clin\"),\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Total (subject, MRI, clinical) combinations: {len(merged)}\")\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        print(\"‚ùå No subject overlap between MRI and clinical for OASIS-3\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Compute absolute day difference\n",
    "    merged[\"day_diff\"] = (merged[\"mri_days\"] - merged[\"days_to_visit\"]).abs()\n",
    "    \n",
    "    # Filter to reasonable temporal proximity (‚â§ max_day_diff)\n",
    "    if max_day_diff is not None:\n",
    "        before = len(merged)\n",
    "        merged = merged[merged[\"day_diff\"] <= max_day_diff]\n",
    "        print(f\"‚úì After day_diff ‚â§ {max_day_diff} filter: {len(merged)} pairs (dropped {before - len(merged)})\")\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        print(\"‚ùå No MRI‚Äìclinical pairs within the day_diff threshold\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # For each MRI session, keep the clinical row with MINIMUM day_diff\n",
    "    idx = merged.groupby(\"mri_session_label\")[\"day_diff\"].idxmin()\n",
    "    best = merged.loc[idx].copy()\n",
    "    \n",
    "    print(f\"‚úì After selecting nearest clinical visit per MRI: {len(best)} visits\")\n",
    "    \n",
    "    # Show example matches\n",
    "    print(f\"\\nüîç Sample matches (first 5):\")\n",
    "    for _, row in best.head(5).iterrows():\n",
    "        print(f\"   MRI: {row['mri_session_label']} (day {row['mri_days']:.0f})\")\n",
    "        print(f\"     ‚Üí Clinical: {row['clin_session_label']} (day {row['days_to_visit']:.0f})\")\n",
    "        print(f\"     ‚Üí Diff: {row['day_diff']:.0f} days, CDR={row['CDR']}, MMSE={row['MMSE']}\")\n",
    "    \n",
    "    # Filter: require valid CDR & MMSE\n",
    "    before_filter = len(best)\n",
    "    best = best.dropna(subset=[\"CDR\", \"MMSE\"])\n",
    "    print(f\"\\n‚úì After CDR/MMSE filter: {len(best)} visits (dropped {before_filter - len(best)})\")\n",
    "    \n",
    "    # Assign labels\n",
    "    def assign_label(cdr):\n",
    "        if cdr == 0:\n",
    "            return 0   # CN\n",
    "        elif cdr >= 1:\n",
    "            return 1   # AD\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    best[\"label\"] = best[\"CDR\"].apply(assign_label)\n",
    "    before_label = len(best)\n",
    "    best = best[best[\"label\"].isin([0, 1])]\n",
    "    print(f\"‚úì After label filter (CN/AD only): {len(best)} visits (dropped {before_label - len(best)})\")\n",
    "    \n",
    "    # Assign visit_index chronologically using days_to_visit\n",
    "    best = assign_oasis3_visit_index(best)\n",
    "    print(\"‚úì Assigned visit_index using days_to_visit\")\n",
    "    \n",
    "    # Standardize output columns to match OASIS-2\n",
    "    best[\"dataset\"] = \"OASIS3\"\n",
    "    best[\"domain_id\"] = 1\n",
    "    best[\"session_label\"] = best[\"mri_session_label\"]  # Use MRI label as primary identifier\n",
    "    \n",
    "    # Select final columns\n",
    "    out_cols = [\n",
    "        \"dataset\",\n",
    "        \"domain_id\",\n",
    "        \"subject_id\",\n",
    "        \"session_label\",\n",
    "        \"visit_id\",\n",
    "        \"visit_index\",\n",
    "        \"mri_path\",\n",
    "        \"CDR\",\n",
    "        \"MMSE\",\n",
    "        \"label\",\n",
    "        \"Age\",\n",
    "        \"days_to_visit\",\n",
    "        \"day_diff\",  # Keep for QC\n",
    "    ]\n",
    "    \n",
    "    # Only keep columns that exist\n",
    "    out_cols = [c for c in out_cols if c in best.columns]\n",
    "    best_out = best[out_cols].copy()\n",
    "    \n",
    "    return best_out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: BUILD VISITS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "def build_visits_table(o2_merged, o3_merged, output_path):\n",
    "    \"\"\"Combine OASIS-2 and OASIS-3 into unified visits table\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Building Unified Visits Table\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Add session_label to OASIS-2 for schema consistency (set to None)\n",
    "    if 'session_label' not in o2_merged.columns:\n",
    "        o2_merged['session_label'] = None\n",
    "    \n",
    "    # Add day_diff to OASIS-2 for schema consistency (set to 0)\n",
    "    if 'day_diff' not in o2_merged.columns:\n",
    "        o2_merged['day_diff'] = 0.0\n",
    "    \n",
    "    df_all = pd.concat([o2_merged, o3_merged], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Column ordering\n",
    "    base_cols = [\n",
    "        'dataset', 'domain_id', 'subject_id', 'session_label', 'visit_id', \n",
    "        'visit_index', 'mri_path', 'CDR', 'MMSE', 'label'\n",
    "    ]\n",
    "    \n",
    "    # Add optional columns\n",
    "    optional_cols = ['Age', 'Sex', 'days_to_visit', 'day_diff']\n",
    "    cols = base_cols + [c for c in optional_cols if c in df_all.columns]\n",
    "    \n",
    "    df_all = df_all[cols]\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert df_all['mri_path'].notna().all(), \"‚ùå Found missing mri_path values\"\n",
    "    \n",
    "    # Save\n",
    "    df_all.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved visits_table.csv: {len(df_all)} visits\")\n",
    "    \n",
    "    # OASIS-3 temporal matching quality report\n",
    "    if 'day_diff' in df_all.columns and 'OASIS3' in df_all['dataset'].values:\n",
    "        o3_diffs = df_all[df_all['dataset'] == 'OASIS3']['day_diff']\n",
    "        print(f\"\\nüìä OASIS-3 Temporal Matching Quality:\")\n",
    "        print(f\"   Mean day difference: {o3_diffs.mean():.1f} days\")\n",
    "        print(f\"   Median day difference: {o3_diffs.median():.1f} days\")\n",
    "        print(f\"   Max day difference: {o3_diffs.max():.1f} days\")\n",
    "        print(f\"   Within 30 days: {(o3_diffs <= 30).sum()} visits ({100*(o3_diffs <= 30).sum()/len(o3_diffs):.1f}%)\")\n",
    "        print(f\"   Within 90 days: {(o3_diffs <= 90).sum()} visits ({100*(o3_diffs <= 90).sum()/len(o3_diffs):.1f}%)\")\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: BUILD SUBJECTS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "def build_subjects_table(visits_df, output_path):\n",
    "    \"\"\"Aggregate visits into subject-level table\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: Building Subjects Table\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    grouped = visits_df.groupby(['dataset', 'subject_id', 'domain_id'])\n",
    "    \n",
    "    records = []\n",
    "    for (dataset, subject_id, domain_id), g in grouped:\n",
    "        # Baseline age\n",
    "        ages = g['Age'].dropna().tolist() if 'Age' in g.columns else []\n",
    "        baseline_age = min(ages) if len(ages) > 0 else None\n",
    "        \n",
    "        # Sex\n",
    "        if 'Sex' in g.columns:\n",
    "            sex_vals = g['Sex'].dropna().tolist()\n",
    "            sex = Counter(sex_vals).most_common(1)[0][0] if len(sex_vals) > 0 else None\n",
    "        else:\n",
    "            sex = None\n",
    "        \n",
    "        # Visit counts\n",
    "        n_total = len(g)\n",
    "        n_cn = int((g['label'] == 0).sum())\n",
    "        n_ad = int((g['label'] == 1).sum())\n",
    "        \n",
    "        records.append({\n",
    "            'dataset': dataset,\n",
    "            'domain_id': domain_id,\n",
    "            'subject_id': subject_id,\n",
    "            'Sex': sex,\n",
    "            'baseline_age': baseline_age,\n",
    "            'n_visits_total': n_total,\n",
    "            'n_CN_visits': n_cn,\n",
    "            'n_AD_visits': n_ad,\n",
    "            'has_longitudinal': int(n_total >= 2),\n",
    "        })\n",
    "    \n",
    "    subjects_df = pd.DataFrame(records)\n",
    "    subjects_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved subjects_table.csv: {len(subjects_df)} subjects\")\n",
    "    \n",
    "    return subjects_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: QC LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "def log_qc(subjects_df, visits_df):\n",
    "    \"\"\"Comprehensive quality control logging\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 7: QUALITY CONTROL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Visit-level statistics\n",
    "    print(\"\\nüìä VISIT-LEVEL STATISTICS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    visit_counts = visits_df.groupby(['dataset', 'label']).size().unstack(fill_value=0)\n",
    "    visit_counts.columns = ['CN', 'AD']\n",
    "    print(\"\\nVisits by dataset and label:\")\n",
    "    print(visit_counts)\n",
    "    \n",
    "    cn_count = int((visits_df['label'] == 0).sum())\n",
    "    ad_count = int((visits_df['label'] == 1).sum())\n",
    "    ratio = cn_count / max(ad_count, 1)\n",
    "    \n",
    "    print(f\"\\n{'Total visits:':<25} {len(visits_df)}\")\n",
    "    print(f\"{'CN visits:':<25} {cn_count} ({100*cn_count/len(visits_df):.1f}%)\")\n",
    "    print(f\"{'AD visits:':<25} {ad_count} ({100*ad_count/len(visits_df):.1f}%)\")\n",
    "    print(f\"{'CN:AD ratio:':<25} {ratio:.2f}:1\")\n",
    "    \n",
    "    # Subject-level statistics\n",
    "    print(\"\\n\\nüìä SUBJECT-LEVEL STATISTICS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    subj_by_dataset = subjects_df.groupby('dataset')['subject_id'].count()\n",
    "    print(\"\\nSubjects by dataset:\")\n",
    "    for dataset, count in subj_by_dataset.items():\n",
    "        print(f\"  {dataset:<10} {count:>4} subjects\")\n",
    "    print(f\"  {'TOTAL':<10} {len(subjects_df):>4} subjects\")\n",
    "    \n",
    "    # Longitudinal structure\n",
    "    print(\"\\n\\nüìä LONGITUDINAL STRUCTURE\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    long_by_dataset = subjects_df[subjects_df['has_longitudinal'] == 1].groupby('dataset')['subject_id'].count()\n",
    "    print(\"\\nSubjects with ‚â•2 visits:\")\n",
    "    for dataset, count in long_by_dataset.items():\n",
    "        print(f\"  {dataset:<10} {count:>4} subjects\")\n",
    "    \n",
    "    visit_dist = subjects_df['n_visits_total'].value_counts().sort_index()\n",
    "    print(\"\\nDistribution of visits per subject:\")\n",
    "    for n_visits, count in visit_dist.items():\n",
    "        print(f\"  {n_visits:>2} visits: {count:>4} subjects\")\n",
    "    \n",
    "    # Converter detection\n",
    "    print(\"\\n\\nüìä CONVERTER ANALYSIS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    converters = subjects_df[(subjects_df['n_CN_visits'] > 0) & (subjects_df['n_AD_visits'] > 0)]\n",
    "    print(f\"\\nSubjects with BOTH CN and AD labels (potential converters): {len(converters)}\")\n",
    "    \n",
    "    if len(converters) > 0:\n",
    "        print(\"\\nTop 10 converters:\")\n",
    "        print(converters[['dataset', 'subject_id', 'n_CN_visits', 'n_AD_visits', 'n_visits_total']].head(10))\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(\"\\n\\nüìä DATA QUALITY CHECKS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    missing_mri = visits_df['mri_path'].isna().sum()\n",
    "    print(f\"‚úì Missing mri_path values: {missing_mri} (should be 0)\")\n",
    "    \n",
    "    if 'Age' in visits_df.columns:\n",
    "        age_range = visits_df['Age'].dropna()\n",
    "        if len(age_range) > 0:\n",
    "            print(f\"‚úì Age range: {age_range.min():.1f} - {age_range.max():.1f} years (mean: {age_range.mean():.1f})\")\n",
    "    \n",
    "    if 'MMSE' in visits_df.columns:\n",
    "        mmse_range = visits_df['MMSE'].dropna()\n",
    "        if len(mmse_range) > 0:\n",
    "            print(f\"‚úì MMSE range: {mmse_range.min():.0f} - {mmse_range.max():.0f} (mean: {mmse_range.mean():.1f})\")\n",
    "    \n",
    "    # Success criteria assessment\n",
    "    print(\"\\n\\nüìä SUCCESS CRITERIA ASSESSMENT\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    checks = []\n",
    "    checks.append((\"Total visits ‚â• 400\", len(visits_df) >= 400))\n",
    "    checks.append((\"Both datasets present\", len(visits_df['dataset'].unique()) == 2))\n",
    "    checks.append((\"CN:AD ratio 3:1 to 10:1\", 3 <= ratio <= 10))\n",
    "    \n",
    "    # Check both datasets have CN and AD\n",
    "    o2_has_both = False\n",
    "    o3_has_both = False\n",
    "    if 'OASIS2' in visit_counts.index:\n",
    "        o2_has_both = (visit_counts.loc['OASIS2']['CN'] > 0) and (visit_counts.loc['OASIS2']['AD'] > 0)\n",
    "    if 'OASIS3' in visit_counts.index:\n",
    "        o3_has_both = (visit_counts.loc['OASIS3']['CN'] > 0) and (visit_counts.loc['OASIS3']['AD'] > 0)\n",
    "    \n",
    "    checks.append((\"OASIS-2 has CN and AD\", o2_has_both))\n",
    "    checks.append((\"OASIS-3 has CN and AD\", o3_has_both))\n",
    "    checks.append((\"Longitudinal subjects ‚â• 50\", (subjects_df['has_longitudinal'] == 1).sum() >= 50))\n",
    "    checks.append((\"No missing MRI paths\", missing_mri == 0))\n",
    "    \n",
    "    all_pass = all([c[1] for c in checks])\n",
    "    \n",
    "    for check_name, passed in checks:\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        print(f\"{status}  {check_name}\")\n",
    "    \n",
    "    if all_pass:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéâ ALL SUCCESS CRITERIA MET - S1 ACCEPTED\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚ö†Ô∏è  SOME CRITERIA NOT MET - REVIEW REQUIRED\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return all_pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_S1_pipeline():\n",
    "    \"\"\"Execute complete S1 data audit pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üî¨ SNIPPET S1: DATA AUDIT & SUBJECT/VISIT TABLES (REVISION 3)\")\n",
    "    print(\"   KEY FIX: OASIS-3 nearest-neighbor temporal matching\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1-2: Scan MRI directories\n",
    "    o2_mri = scan_oasis2_mri(\n",
    "        root_part1=\"/kaggle/input/oaisis-dataset-3-p1/OAS2_RAW_PART1\",\n",
    "        root_part2=\"/kaggle/input/oaisis-3-p2/OAS2_RAW_PART2\",\n",
    "    )\n",
    "    \n",
    "    o3_mri = scan_oasis3_mri(\n",
    "        root_o3=\"/kaggle/input/oaisis-3/oaisis3\",\n",
    "    )\n",
    "    \n",
    "    # Step 3: Load clinical CSVs\n",
    "    o2_clin = load_oasis2_clinical(\"/kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv\")\n",
    "    o3_clin = load_oasis3_clinical(\"/kaggle/input/oaisis-3-longitiudinal/oaisis3longitiudinal.csv\")\n",
    "    \n",
    "    # Step 4: Merge\n",
    "    o2_merged = merge_oasis2(o2_mri, o2_clin)\n",
    "    o3_merged = merge_oasis3(o3_mri, o3_clin, max_day_diff=365)\n",
    "    \n",
    "    # Step 5-6: Build tables\n",
    "    visits_df = build_visits_table(\n",
    "        o2_merged, o3_merged,\n",
    "        output_path=\"visits_table.csv\"\n",
    "    )\n",
    "    \n",
    "    subjects_df = build_subjects_table(\n",
    "        visits_df,\n",
    "        output_path=\"subjects_table.csv\"\n",
    "    )\n",
    "    \n",
    "    # Step 7: QC logging\n",
    "    all_pass = log_qc(subjects_df, visits_df)\n",
    "    \n",
    "    return visits_df, subjects_df, all_pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visits_df, subjects_df, success = run_S1_pipeline()\n",
    "    \n",
    "    # Display previews\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã PREVIEW: visits_table.csv\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nFirst 5 OASIS-2 visits:\")\n",
    "    o2_sample = visits_df[visits_df['dataset'] == 'OASIS2'].head(5)[\n",
    "        ['subject_id', 'visit_id', 'visit_index', 'CDR', 'MMSE', 'Age', 'label']\n",
    "    ]\n",
    "    print(o2_sample.to_string(index=False))\n",
    "    \n",
    "    if 'OASIS3' in visits_df['dataset'].values:\n",
    "        print(\"\\nFirst 5 OASIS-3 visits:\")\n",
    "        o3_sample = visits_df[visits_df['dataset'] == 'OASIS3'].head(5)[\n",
    "            ['subject_id', 'visit_id', 'visit_index', 'CDR', 'MMSE', 'Age', 'label', 'day_diff']\n",
    "        ]\n",
    "        print(o3_sample.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã PREVIEW: subjects_table.csv (first 10 rows)\")\n",
    "    print(\"=\"*70)\n",
    "    print(subjects_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b16e3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T19:16:33.244795Z",
     "iopub.status.busy": "2025-11-29T19:16:33.244578Z",
     "iopub.status.idle": "2025-11-29T20:06:53.335897Z",
     "shell.execute_reply": "2025-11-29T20:06:53.334916Z"
    },
    "papermill": {
     "duration": 3020.099507,
     "end_time": "2025-11-29T20:06:53.337382",
     "exception": false,
     "start_time": "2025-11-29T19:16:33.237875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ SNIPPET S2: 3D PREPROCESSING (EXTENDED - Full-Brain + ROI)\n",
      "======================================================================\n",
      "\n",
      "Outputs:\n",
      "  - Full MNI resolution: whole_brain/\n",
      "  - Downsampled full-brain (128, 160, 128): full_brain/\n",
      "  - Hippocampal ROI (80, 80, 80): hippo_roi/\n",
      "  - Hippocampal extended (112, 112, 80): hippo_roi_ext/\n",
      "\n",
      "======================================================================\n",
      "Loading MNI Template, Mask, and Atlas (Auto-Detection)\n",
      "======================================================================\n",
      "‚úì Fetching MNI152 ICBM152 2009c template...\n",
      "\n",
      "Added README.md to /root/nilearn_data\n",
      "\n",
      "\n",
      "Dataset created in /root/nilearn_data/icbm152_2009\n",
      "\n",
      "Downloading data from https://osf.io/7pj92/download ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ...done. (2 seconds, 0 min)\n",
      "Extracting data from /root/nilearn_data/icbm152_2009/e05b733c275cab0eec856067143c9dc9/download..... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MNI template: shape=(197, 233, 189)\n",
      "‚úì Brain mask: 1886539 voxels\n",
      "‚úì Saved MNI template: /kaggle/working/processed_mri/mni_template/mni_template.nii.gz\n",
      "\n",
      "‚úì Loading Harvard-Oxford subcortical atlas...\n",
      "\n",
      "Dataset created in /root/nilearn_data/fsl\n",
      "\n",
      "Downloading data from https://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ...done. (1 seconds, 0 min)\n",
      "Extracting data from /root/nilearn_data/fsl/8a6a179c4b7672ec60913c596b129eff/HarvardOxford.tgz..... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Atlas loaded: shape=(91, 109, 91)\n",
      "\n",
      "‚úì Auto-detecting hippocampal labels...\n",
      "  Found 22 atlas regions\n",
      "  Found: 'Left Hippocampus' at index 9\n",
      "  Found: 'Right Hippocampus' at index 19\n",
      "\n",
      "‚úì Detected hippocampal labels:\n",
      "   Left:  9\n",
      "   Right: 19\n",
      "\n",
      "üìä Hippocampus validation:\n",
      "   Left (label 9): 691 voxels\n",
      "   Right (label 19): 700 voxels\n",
      "\n",
      "‚úì Hippocampal bbox: (27, 64, 42, 61, 22, 39), voxels: 1391\n",
      "\n",
      "‚úì Loaded 575 visits\n",
      "   {'OASIS3': 327, 'OASIS2': 248}\n",
      "\n",
      "======================================================================\n",
      "Processing 575 visits...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/575 [21:40<36:24,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/575 [21:49<40:06,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/575 [22:14<33:17,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/575 [23:44<25:25,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/575 [23:54<25:23,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/575 [24:29<23:02,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/575 [24:38<22:43,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/575 [24:43<22:50,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 294/575 [24:48<22:52,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/575 [24:53<22:50,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/575 [25:57<25:59,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/575 [26:26<21:24,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/575 [28:07<21:25,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/575 [28:55<20:02,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/575 [29:20<19:24,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/575 [29:25<19:05,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/575 [30:00<21:31,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/575 [30:44<17:09,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/575 [30:49<17:43,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/575 [31:38<19:38,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/575 [32:18<16:08,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 380/575 [32:23<16:10,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/575 [32:29<16:15,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/575 [32:33<16:02,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/575 [32:38<15:40,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 388/575 [33:04<16:06,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/575 [33:52<15:35,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/575 [33:57<15:26,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/575 [34:06<14:39,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/575 [34:11<14:25,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/575 [34:26<14:03,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/575 [35:00<16:00,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/575 [35:17<15:28,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 416/575 [35:37<17:08,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/575 [35:52<14:31,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/575 [35:57<13:59,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 445/575 [38:14<11:09,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/575 [39:03<11:19,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/575 [39:22<11:29,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 465/575 [40:09<11:16,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 467/575 [40:21<10:52,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 479/575 [41:25<08:37,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 480/575 [41:31<08:36,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 482/575 [41:46<10:26,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 483/575 [41:51<09:34,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 494/575 [42:57<08:36,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 498/575 [43:18<06:52,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 509/575 [44:18<05:53,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 510/575 [44:22<05:37,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 512/575 [44:32<05:21,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 521/575 [45:19<04:56,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 524/575 [45:39<05:15,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 540/575 [47:00<02:52,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 545/575 [47:28<02:54,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 549/575 [47:52<02:35,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 550/575 [47:57<02:22,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 555/575 [48:22<01:42,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 571/575 [49:52<00:23,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\n",
      "      ‚úì SimpleITK fallback succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 575/575 [50:11<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ S2 COMPLETE\n",
      "   Successful: 575/575 (100.0%)\n",
      "   Failed: 0\n",
      "\n",
      "üìä Summary:\n",
      "   Datasets: {'OASIS3': 327, 'OASIS2': 248}\n",
      "   Labels: CN=499, AD=76\n",
      "   Mean intensity: 516.185\n",
      "   Full-brain volumes: 575/575 exist\n",
      "\n",
      "======================================================================\n",
      "VALIDATION (Data-Aware)\n",
      "======================================================================\n",
      "\n",
      "Criteria:\n",
      "  Overall success rate: 100.0% (target: ‚â•80%)\n",
      "  OASIS3 success rate:  100.0% (target: ‚â•70%)\n",
      "  AD visits available:  76 (target: ‚â•50)\n",
      "\n",
      "‚úÖ PASS: S2 preprocessing meets quality thresholds\n",
      "   (0 failures due to corrupted source files)\n",
      "\n",
      "‚úì Cleaned up 575 temp files\n",
      "\n",
      "======================================================================\n",
      "üìã Sample Output:\n",
      "======================================================================\n",
      "subject_id  visit_index dataset  label                                                             full_t1_path\n",
      " OAS2_0079            2  OASIS2      1 /kaggle/working/processed_mri/full_brain/FULL_OASIS2_OAS2_0079_v2.nii.gz\n",
      " OAS2_0044            1  OASIS2      1 /kaggle/working/processed_mri/full_brain/FULL_OASIS2_OAS2_0044_v1.nii.gz\n",
      " OAS2_0056            2  OASIS2      0 /kaggle/working/processed_mri/full_brain/FULL_OASIS2_OAS2_0056_v2.nii.gz\n",
      " OAS2_0062            3  OASIS2      0 /kaggle/working/processed_mri/full_brain/FULL_OASIS2_OAS2_0062_v3.nii.gz\n",
      " OAS2_0062            2  OASIS2      0 /kaggle/working/processed_mri/full_brain/FULL_OASIS2_OAS2_0062_v2.nii.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S2: 3D Preprocessing + Hippocampal ROI + Full-Brain Volume (EXTENDED)\n",
    "\n",
    "NEW: Also generates downsampled full-brain volumes for whole-brain CNN analysis\n",
    "- Hippocampal ROI extraction (for XAI/regional analysis)\n",
    "- Full T1w downsampled volumes (for full-brain CNN)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Nilearn for template loading\n",
    "from nilearn import datasets as nilearn_datasets\n",
    "\n",
    "# SimpleITK for registration\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class S2Config:\n",
    "    \"\"\"Central configuration for S2 preprocessing\"\"\"\n",
    "    \n",
    "    MNI_RESOLUTION = 2\n",
    "    \n",
    "    # Hippocampal atlas labels (auto-detected at runtime)\n",
    "    HIPPO_LEFT_LABEL = None\n",
    "    HIPPO_RIGHT_LABEL = None\n",
    "    \n",
    "    # Output directories\n",
    "    OUTPUT_ROOT = \"/kaggle/working/processed_mri\"\n",
    "    WHOLE_BRAIN_DIR = \"whole_brain\"      # Full MNI resolution\n",
    "    FULL_BRAIN_DIR = \"full_brain\"        # NEW: Downsampled for CNN\n",
    "    HIPPO_ROI_DIR = \"hippo_roi\"\n",
    "    HIPPO_EXT_DIR = \"hippo_roi_ext\"\n",
    "    QC_DIR = \"qc\"\n",
    "    TEMP_DIR = \"temp_cleaned\"\n",
    "    \n",
    "    # ROI parameters\n",
    "    HIPPO_ROI_MARGIN = 10\n",
    "    HIPPO_EXT_MARGIN = 24\n",
    "    TARGET_HIPPO_SHAPE = (80, 80, 80)\n",
    "    TARGET_EXT_SHAPE = (112, 112, 80)\n",
    "    \n",
    "    # NEW: Full-brain CNN input shape (downsampled from MNI ~193x229x193)\n",
    "    TARGET_FULL_SHAPE = (128, 160, 128)  # Manageable for 3D CNN\n",
    "    \n",
    "    # Registration parameters\n",
    "    RIGID_ITERATIONS = 200\n",
    "    \n",
    "    # Intensity normalization\n",
    "    PERCENTILE_LOW = 1\n",
    "    PERCENTILE_HIGH = 99\n",
    "    \n",
    "    # QC sampling\n",
    "    QC_SAMPLE_SIZE = 20\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def collapse_4d_to_3d(data):\n",
    "    \"\"\"Collapse 4D MRI volume to 3D\"\"\"\n",
    "    if data.ndim == 3:\n",
    "        return data\n",
    "    \n",
    "    if data.ndim == 4:\n",
    "        if data.shape[-1] == 1:\n",
    "            return data[..., 0]\n",
    "        else:\n",
    "            print(f\"      4D volume with {data.shape[-1]} timepoints, averaging...\")\n",
    "            return np.mean(data, axis=-1)\n",
    "    \n",
    "    data_squeezed = np.squeeze(data)\n",
    "    if data_squeezed.ndim == 3:\n",
    "        return data_squeezed\n",
    "    \n",
    "    raise ValueError(f\"Cannot collapse {data.ndim}D volume with shape {data.shape} to 3D\")\n",
    "\n",
    "\n",
    "def find_actual_oasis3_file(raw_path):\n",
    "    \"\"\"OASIS-3 files are in nested BIDS directories\"\"\"\n",
    "    path_obj = Path(raw_path)\n",
    "    \n",
    "    if path_obj.is_dir():\n",
    "        nii_files = list(path_obj.glob(\"*.nii\")) + list(path_obj.glob(\"*.nii.gz\"))\n",
    "        if nii_files:\n",
    "            for f in nii_files:\n",
    "                if \"T1w\" in f.name or \"t1\" in f.name.lower():\n",
    "                    return str(f)\n",
    "            return str(nii_files[0])\n",
    "    \n",
    "    if path_obj.exists() and path_obj.is_file():\n",
    "        return str(path_obj)\n",
    "    \n",
    "    if not path_obj.exists():\n",
    "        parent = path_obj.parent\n",
    "        if parent.exists() and parent.is_dir():\n",
    "            nii_files = list(parent.glob(\"*.nii\")) + list(parent.glob(\"*.nii.gz\"))\n",
    "            if nii_files:\n",
    "                for f in nii_files:\n",
    "                    if \"T1w\" in f.name:\n",
    "                        return str(f)\n",
    "                return str(nii_files[0])\n",
    "    \n",
    "    gz_path = Path(str(path_obj) + \".gz\")\n",
    "    if gz_path.exists():\n",
    "        return str(gz_path)\n",
    "    \n",
    "    raise FileNotFoundError(f\"Cannot find actual NIfTI file for: {raw_path}\")\n",
    "\n",
    "\n",
    "def prepare_volume_for_sitk(raw_path, dataset, temp_dir):\n",
    "    \"\"\"\n",
    "    Load raw volume, handle 4D‚Üí3D, clean headers, save for SimpleITK\n",
    "    \n",
    "    FIXED: SimpleITK fallback for corrupted OASIS-3 files\n",
    "    \"\"\"\n",
    "    # Find actual file\n",
    "    if dataset == \"OASIS3\":\n",
    "        actual_file = find_actual_oasis3_file(raw_path)\n",
    "    else:\n",
    "        actual_file = raw_path\n",
    "    \n",
    "    # Try loading with nibabel (standard path)\n",
    "    data = None\n",
    "    affine = None\n",
    "    \n",
    "    try:\n",
    "        img = nib.load(actual_file)\n",
    "        data = img.get_fdata()\n",
    "        affine = img.affine\n",
    "    except Exception as e:\n",
    "        # Check if it's a byte-size mismatch error (corrupted file)\n",
    "        error_msg = str(e).lower()\n",
    "        is_corruption = (\"expected\" in error_msg and \"bytes\" in error_msg and \"got\" in error_msg)\n",
    "        \n",
    "        if not is_corruption:\n",
    "            raise\n",
    "        \n",
    "        # Try SimpleITK fallback\n",
    "        print(f\"      ‚ö†Ô∏è nibabel failed (corrupted file), trying SimpleITK fallback...\")\n",
    "        try:\n",
    "            sitk_img = sitk.ReadImage(str(actual_file))\n",
    "            arr = sitk.GetArrayFromImage(sitk_img)\n",
    "            data = np.transpose(arr, (2, 1, 0))\n",
    "            \n",
    "            spacing = sitk_img.GetSpacing()\n",
    "            origin = sitk_img.GetOrigin()\n",
    "            \n",
    "            affine = np.eye(4)\n",
    "            affine[0, 0] = spacing[0]\n",
    "            affine[1, 1] = spacing[1]\n",
    "            affine[2, 2] = spacing[2]\n",
    "            affine[0, 3] = origin[0]\n",
    "            affine[1, 3] = origin[1]\n",
    "            affine[2, 3] = origin[2]\n",
    "            \n",
    "            print(f\"      ‚úì SimpleITK fallback succeeded\")\n",
    "            \n",
    "        except Exception as sitk_error:\n",
    "            raise ValueError(f\"Both nibabel and SimpleITK failed. Original: {e}, SimpleITK: {sitk_error}\")\n",
    "    \n",
    "    if data is None:\n",
    "        raise ValueError(\"Failed to load data from file\")\n",
    "    \n",
    "    # Collapse 4D ‚Üí 3D if needed\n",
    "    data_3d = collapse_4d_to_3d(data)\n",
    "    \n",
    "    # Validate 3D data\n",
    "    if data_3d.size == 0:\n",
    "        raise ValueError(\"Empty data array\")\n",
    "    \n",
    "    if np.all(data_3d == 0):\n",
    "        raise ValueError(\"All-zero volume\")\n",
    "    \n",
    "    if data_3d.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D, got {data_3d.ndim}D with shape {data_3d.shape}\")\n",
    "    \n",
    "    # Create cleaned image\n",
    "    img_clean = nib.Nifti1Image(data_3d.astype(np.float32), affine)\n",
    "    \n",
    "    # Generate unique temp filename\n",
    "    hash_suffix = hashlib.md5(actual_file.encode()).hexdigest()[:8]\n",
    "    temp_filename = f\"{dataset}_{hash_suffix}_cleaned.nii.gz\"\n",
    "    temp_path = os.path.join(temp_dir, temp_filename)\n",
    "    \n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save cleaned version\n",
    "    nib.save(img_clean, temp_path)\n",
    "    \n",
    "    return temp_path\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPLATE LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_mni_template_and_mask(config):\n",
    "    \"\"\"Load MNI152 template with auto-detected hippocampal labels\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Loading MNI Template, Mask, and Atlas (Auto-Detection)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Fetch MNI152 template\n",
    "        print(\"‚úì Fetching MNI152 ICBM152 2009c template...\")\n",
    "        mni_data = nilearn_datasets.fetch_icbm152_2009()\n",
    "        \n",
    "        # Robust load\n",
    "        if isinstance(mni_data['t1'], str):\n",
    "            mni_template = nib.load(mni_data['t1'])\n",
    "        else:\n",
    "            mni_template = mni_data['t1']\n",
    "        \n",
    "        if isinstance(mni_data['mask'], str):\n",
    "            mni_brain_mask = nib.load(mni_data['mask'])\n",
    "        else:\n",
    "            mni_brain_mask = mni_data['mask']\n",
    "        \n",
    "        print(f\"‚úì MNI template: shape={mni_template.shape}\")\n",
    "        print(f\"‚úì Brain mask: {int(mni_brain_mask.get_fdata().sum())} voxels\")\n",
    "        \n",
    "        # Save template for SimpleITK\n",
    "        template_dir = os.path.join(config.OUTPUT_ROOT, \"mni_template\")\n",
    "        os.makedirs(template_dir, exist_ok=True)\n",
    "        mni_template_path = os.path.join(template_dir, \"mni_template.nii.gz\")\n",
    "        \n",
    "        nib.save(mni_template, mni_template_path)\n",
    "        print(f\"‚úì Saved MNI template: {mni_template_path}\")\n",
    "        \n",
    "        # Fetch Harvard-Oxford atlas\n",
    "        print(\"\\n‚úì Loading Harvard-Oxford subcortical atlas...\")\n",
    "        ho_data = nilearn_datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')\n",
    "        \n",
    "        # Robust load\n",
    "        raw_maps = ho_data.maps\n",
    "        if isinstance(raw_maps, str):\n",
    "            hippo_atlas = nib.load(raw_maps)\n",
    "        elif isinstance(raw_maps, nib.nifti1.Nifti1Image):\n",
    "            hippo_atlas = raw_maps\n",
    "        else:\n",
    "            hippo_atlas = raw_maps\n",
    "        \n",
    "        print(f\"‚úì Atlas loaded: shape={hippo_atlas.shape}\")\n",
    "        \n",
    "        # AUTO-DETECT HIPPOCAMPAL LABELS\n",
    "        print(\"\\n‚úì Auto-detecting hippocampal labels...\")\n",
    "        \n",
    "        labels = ho_data.labels\n",
    "        labels = [lab.decode(\"utf-8\") if isinstance(lab, bytes) else lab for lab in labels]\n",
    "        \n",
    "        print(f\"  Found {len(labels)} atlas regions\")\n",
    "        \n",
    "        left_idx = None\n",
    "        right_idx = None\n",
    "        \n",
    "        for idx, name in enumerate(labels):\n",
    "            name_lower = name.lower()\n",
    "            if \"left\" in name_lower and \"hippocampus\" in name_lower:\n",
    "                left_idx = idx\n",
    "                print(f\"  Found: '{name}' at index {idx}\")\n",
    "            if \"right\" in name_lower and \"hippocampus\" in name_lower:\n",
    "                right_idx = idx\n",
    "                print(f\"  Found: '{name}' at index {idx}\")\n",
    "        \n",
    "        if left_idx is None or right_idx is None:\n",
    "            print(\"\\n  ‚ö†Ô∏è  Could not find hippocampal labels. Available:\")\n",
    "            for idx, name in enumerate(labels[:20]):\n",
    "                print(f\"    {idx}: {name}\")\n",
    "            raise ValueError(f\"Hippocampal labels not found. Left={left_idx}, Right={right_idx}\")\n",
    "        \n",
    "        # Update config\n",
    "        config.HIPPO_LEFT_LABEL = left_idx\n",
    "        config.HIPPO_RIGHT_LABEL = right_idx\n",
    "        \n",
    "        print(f\"\\n‚úì Detected hippocampal labels:\")\n",
    "        print(f\"   Left:  {config.HIPPO_LEFT_LABEL}\")\n",
    "        print(f\"   Right: {config.HIPPO_RIGHT_LABEL}\")\n",
    "        \n",
    "        # Validate\n",
    "        atlas_data = hippo_atlas.get_fdata()\n",
    "        left_count = int(np.sum(atlas_data == config.HIPPO_LEFT_LABEL))\n",
    "        right_count = int(np.sum(atlas_data == config.HIPPO_RIGHT_LABEL))\n",
    "        \n",
    "        print(f\"\\nüìä Hippocampus validation:\")\n",
    "        print(f\"   Left (label {config.HIPPO_LEFT_LABEL}): {left_count} voxels\")\n",
    "        print(f\"   Right (label {config.HIPPO_RIGHT_LABEL}): {right_count} voxels\")\n",
    "        \n",
    "        if left_count == 0 or right_count == 0:\n",
    "            raise ValueError(f\"Hippocampal labels have no voxels! Left={left_count}, Right={right_count}\")\n",
    "        \n",
    "        return mni_template, mni_brain_mask, hippo_atlas, mni_template_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå CRITICAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REGISTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def rigid_register_to_mni_sitk(moving_path, fixed_sitk):\n",
    "    \"\"\"Rigidly register moving image to MNI using SimpleITK\"\"\"\n",
    "    moving_sitk = sitk.ReadImage(str(moving_path))\n",
    "    \n",
    "    if moving_sitk.GetSize()[0] == 0:\n",
    "        raise ValueError(\"Moving image has zero size\")\n",
    "    \n",
    "    registration = sitk.ImageRegistrationMethod()\n",
    "    registration.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "    registration.SetMetricSamplingStrategy(registration.RANDOM)\n",
    "    registration.SetMetricSamplingPercentage(0.01)\n",
    "    registration.SetInterpolator(sitk.sitkLinear)\n",
    "    registration.SetOptimizerAsGradientDescent(\n",
    "        learningRate=1.0, numberOfIterations=200,\n",
    "        convergenceMinimumValue=1e-6, convergenceWindowSize=10\n",
    "    )\n",
    "    registration.SetOptimizerScalesFromPhysicalShift()\n",
    "    \n",
    "    initial_transform = sitk.CenteredTransformInitializer(\n",
    "        fixed_sitk, moving_sitk, sitk.Euler3DTransform(),\n",
    "        sitk.CenteredTransformInitializerFilter.GEOMETRY\n",
    "    )\n",
    "    registration.SetInitialTransform(initial_transform, inPlace=False)\n",
    "    \n",
    "    final_transform = registration.Execute(fixed_sitk, moving_sitk)\n",
    "    \n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetReferenceImage(fixed_sitk)\n",
    "    resampler.SetInterpolator(sitk.sitkLinear)\n",
    "    resampler.SetDefaultPixelValue(0)\n",
    "    resampler.SetTransform(final_transform)\n",
    "    \n",
    "    registered_sitk = resampler.Execute(moving_sitk)\n",
    "    \n",
    "    data_mni = sitk.GetArrayFromImage(registered_sitk)\n",
    "    data_mni = np.transpose(data_mni, (2, 1, 0))\n",
    "    \n",
    "    return data_mni.astype(np.float32), True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ROI & RESIZING UTILITIES (NEW: resize_to_shape)\n",
    "# ============================================================================\n",
    "\n",
    "def resize_to_shape(data, target_shape):\n",
    "    \"\"\"\n",
    "    Resize 3D volume to target shape using trilinear interpolation\n",
    "    \n",
    "    Args:\n",
    "        data: 3D numpy array\n",
    "        target_shape: tuple (D, H, W) target dimensions\n",
    "    \n",
    "    Returns:\n",
    "        resized: 3D numpy array with target_shape\n",
    "    \"\"\"\n",
    "    if data.shape == target_shape:\n",
    "        return data\n",
    "    \n",
    "    zoom_factors = (\n",
    "        target_shape[0] / data.shape[0],\n",
    "        target_shape[1] / data.shape[1],\n",
    "        target_shape[2] / data.shape[2],\n",
    "    )\n",
    "    \n",
    "    # Use scipy.ndimage.zoom with trilinear interpolation (order=1)\n",
    "    resized = zoom(data, zoom_factors, order=1, mode='constant', cval=0)\n",
    "    \n",
    "    return resized.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_hippocampal_mask_and_bbox(hippo_atlas, config):\n",
    "    \"\"\"Extract hippocampal mask and bounding box\"\"\"\n",
    "    atlas_data = hippo_atlas.get_fdata()\n",
    "    \n",
    "    hippo_mask_left = (atlas_data == config.HIPPO_LEFT_LABEL)\n",
    "    hippo_mask_right = (atlas_data == config.HIPPO_RIGHT_LABEL)\n",
    "    hippo_mask_total = hippo_mask_left | hippo_mask_right\n",
    "    \n",
    "    coords = np.where(hippo_mask_total)\n",
    "    if len(coords[0]) == 0:\n",
    "        raise ValueError(\"Empty hippocampal mask!\")\n",
    "    \n",
    "    x_min, x_max = coords[0].min(), coords[0].max()\n",
    "    y_min, y_max = coords[1].min(), coords[1].max()\n",
    "    z_min, z_max = coords[2].min(), coords[2].max()\n",
    "    \n",
    "    return hippo_mask_total, (x_min, x_max, y_min, y_max, z_min, z_max)\n",
    "\n",
    "\n",
    "def crop_roi_with_margin(data, bbox, margin, target_shape=None):\n",
    "    \"\"\"Crop ROI with margin\"\"\"\n",
    "    x_min, x_max, y_min, y_max, z_min, z_max = bbox\n",
    "    \n",
    "    x0 = max(x_min - margin, 0)\n",
    "    x1 = min(x_max + margin + 1, data.shape[0])\n",
    "    y0 = max(y_min - margin, 0)\n",
    "    y1 = min(y_max + margin + 1, data.shape[1])\n",
    "    z0 = max(z_min - margin, 0)\n",
    "    z1 = min(z_max + margin + 1, data.shape[2])\n",
    "    \n",
    "    cropped = data[x0:x1, y0:y1, z0:z1]\n",
    "    \n",
    "    if target_shape is not None:\n",
    "        cropped = pad_or_crop_to_shape(cropped, target_shape)\n",
    "    \n",
    "    return cropped, (x0, x1, y0, y1, z0, z1)\n",
    "\n",
    "\n",
    "def pad_or_crop_to_shape(data, target_shape):\n",
    "    \"\"\"Pad or crop to target shape\"\"\"\n",
    "    current_shape = data.shape\n",
    "    output = np.zeros(target_shape, dtype=data.dtype)\n",
    "    \n",
    "    slices_out = []\n",
    "    slices_in = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        if current_shape[i] <= target_shape[i]:\n",
    "            start = (target_shape[i] - current_shape[i]) // 2\n",
    "            end = start + current_shape[i]\n",
    "            slices_out.append(slice(start, end))\n",
    "            slices_in.append(slice(None))\n",
    "        else:\n",
    "            start = (current_shape[i] - target_shape[i]) // 2\n",
    "            end = start + target_shape[i]\n",
    "            slices_out.append(slice(None))\n",
    "            slices_in.append(slice(start, end))\n",
    "    \n",
    "    output[tuple(slices_out)] = data[tuple(slices_in)]\n",
    "    return output\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING PIPELINE (EXTENDED with full-brain volume)\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_visit(row, mni_template_sitk, mni_brain_mask, mni_affine, \n",
    "                     hippo_mask, bbox, config, temp_dir_absolute):\n",
    "    \"\"\"\n",
    "    Complete preprocessing for one visit\n",
    "    \n",
    "    NEW: Also generates downsampled full-brain volume for CNN\n",
    "    \"\"\"\n",
    "    dataset = row['dataset']\n",
    "    subject_id = row['subject_id']\n",
    "    visit_index = row['visit_index']\n",
    "    mri_path = row['mri_path']\n",
    "    \n",
    "    cleaned_path = prepare_volume_for_sitk(mri_path, dataset, temp_dir_absolute)\n",
    "    data_mni, reg_success = rigid_register_to_mni_sitk(cleaned_path, mni_template_sitk)\n",
    "    \n",
    "    if not reg_success:\n",
    "        raise ValueError(\"Registration failed\")\n",
    "    \n",
    "    mask_data = mni_brain_mask.get_fdata() > 0\n",
    "    masked_vals = data_mni[mask_data]\n",
    "    \n",
    "    if masked_vals.size == 0:\n",
    "        raise ValueError(\"No brain voxels after masking\")\n",
    "    \n",
    "    p1 = np.percentile(masked_vals, config.PERCENTILE_LOW)\n",
    "    p99 = np.percentile(masked_vals, config.PERCENTILE_HIGH)\n",
    "    \n",
    "    if (p99 - p1) <= 1e-6:\n",
    "        raise ValueError(f\"Invalid percentile range: p1={p1:.2f}, p99={p99:.2f}\")\n",
    "    \n",
    "    data_norm = np.clip((data_mni - p1) / (p99 - p1), 0, 1).astype(np.float32)\n",
    "    data_norm[~mask_data] = 0.0\n",
    "    \n",
    "    # Step 4a: Save whole-brain volume (full MNI resolution)\n",
    "    wb_dir = os.path.join(config.OUTPUT_ROOT, config.WHOLE_BRAIN_DIR)\n",
    "    os.makedirs(wb_dir, exist_ok=True)\n",
    "    wb_path = os.path.join(wb_dir, f\"MNI_{dataset}_{subject_id}_v{visit_index}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data_norm, mni_affine), wb_path)\n",
    "    \n",
    "    # Step 4b: Create downsampled full-brain volume for CNN (NEW)\n",
    "    full_dir = os.path.join(config.OUTPUT_ROOT, config.FULL_BRAIN_DIR)\n",
    "    os.makedirs(full_dir, exist_ok=True)\n",
    "    \n",
    "    data_full = resize_to_shape(data_norm, config.TARGET_FULL_SHAPE)\n",
    "    \n",
    "    full_path = os.path.join(\n",
    "        full_dir,\n",
    "        f\"FULL_{dataset}_{subject_id}_v{visit_index}.nii.gz\"\n",
    "    )\n",
    "    nib.save(nib.Nifti1Image(data_full, mni_affine), full_path)\n",
    "    \n",
    "    # Step 5: Extract hippocampal ROI\n",
    "    hippo_roi, _ = crop_roi_with_margin(\n",
    "        data_norm, bbox, margin=config.HIPPO_ROI_MARGIN,\n",
    "        target_shape=config.TARGET_HIPPO_SHAPE\n",
    "    )\n",
    "    roi_dir = os.path.join(config.OUTPUT_ROOT, config.HIPPO_ROI_DIR)\n",
    "    os.makedirs(roi_dir, exist_ok=True)\n",
    "    roi_path = os.path.join(roi_dir, f\"ROI_{dataset}_{subject_id}_v{visit_index}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(hippo_roi, mni_affine), roi_path)\n",
    "    \n",
    "    # Step 6: Extract extended ROI\n",
    "    hippo_ext, _ = crop_roi_with_margin(\n",
    "        data_norm, bbox, margin=config.HIPPO_EXT_MARGIN,\n",
    "        target_shape=config.TARGET_EXT_SHAPE\n",
    "    )\n",
    "    ext_dir = os.path.join(config.OUTPUT_ROOT, config.HIPPO_EXT_DIR)\n",
    "    os.makedirs(ext_dir, exist_ok=True)\n",
    "    ext_path = os.path.join(ext_dir, f\"ROIEXT_{dataset}_{subject_id}_v{visit_index}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(hippo_ext, mni_affine), ext_path)\n",
    "    \n",
    "    return {\n",
    "        'mni_path': wb_path,\n",
    "        'full_t1_path': full_path,      # NEW: Downsampled full-brain for CNN\n",
    "        'hippo_roi_path': roi_path,\n",
    "        'hippo_ext_path': ext_path,\n",
    "        'p1': float(p1),\n",
    "        'p99': float(p99),\n",
    "        'mean_intensity': float(np.mean(masked_vals)),\n",
    "        'std_intensity': float(np.std(masked_vals)),\n",
    "        'nonzero_voxels': int(np.sum(mask_data)),\n",
    "        'preproc_ok': True,\n",
    "        'error_msg': ''\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QC\n",
    "# ============================================================================\n",
    "\n",
    "def generate_qc_overlay(data_mni, hippo_mask, output_path):\n",
    "    \"\"\"Generate QC overlay\"\"\"\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    gs = gridspec.GridSpec(1, 3, figure=fig)\n",
    "    \n",
    "    coords = np.where(hippo_mask)\n",
    "    if len(coords[0]) > 0:\n",
    "        center_x, center_y, center_z = [int(np.mean(c)) for c in coords]\n",
    "    else:\n",
    "        center_x, center_y, center_z = [s//2 for s in data_mni.shape]\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(data_mni[:, :, center_z].T, cmap='gray', origin='lower')\n",
    "    ax1.contour(hippo_mask[:, :, center_z].T, colors='red', linewidths=1.5)\n",
    "    ax1.set_title(f'Axial (z={center_z})')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.imshow(data_mni[:, center_y, :].T, cmap='gray', origin='lower')\n",
    "    ax2.contour(hippo_mask[:, center_y, :].T, colors='red', linewidths=1.5)\n",
    "    ax2.set_title(f'Coronal (y={center_y})')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.imshow(data_mni[center_x, :, :].T, cmap='gray', origin='lower')\n",
    "    ax3.contour(hippo_mask[center_x, :, :].T, colors='red', linewidths=1.5)\n",
    "    ax3.set_title(f'Sagittal (x={center_x})')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def setup_s2_environment(config):\n",
    "    \"\"\"Create output directories\"\"\"\n",
    "    for subdir in [config.WHOLE_BRAIN_DIR, config.FULL_BRAIN_DIR,  # NEW: FULL_BRAIN_DIR\n",
    "                   config.HIPPO_ROI_DIR, config.HIPPO_EXT_DIR, config.QC_DIR]:\n",
    "        path = os.path.join(config.OUTPUT_ROOT, subdir)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def run_s2_pipeline(visits_csv_path=\"/kaggle/working/visits_table.csv\", config=None):\n",
    "    \"\"\"Execute S2 preprocessing pipeline\"\"\"\n",
    "    if config is None:\n",
    "        config = S2Config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üî¨ SNIPPET S2: 3D PREPROCESSING (EXTENDED - Full-Brain + ROI)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOutputs:\")\n",
    "    print(f\"  - Full MNI resolution: {config.WHOLE_BRAIN_DIR}/\")\n",
    "    print(f\"  - Downsampled full-brain {config.TARGET_FULL_SHAPE}: {config.FULL_BRAIN_DIR}/\")\n",
    "    print(f\"  - Hippocampal ROI {config.TARGET_HIPPO_SHAPE}: {config.HIPPO_ROI_DIR}/\")\n",
    "    print(f\"  - Hippocampal extended {config.TARGET_EXT_SHAPE}: {config.HIPPO_EXT_DIR}/\")\n",
    "    \n",
    "    setup_s2_environment(config)\n",
    "    \n",
    "    temp_dir_absolute = os.path.join(config.OUTPUT_ROOT, config.TEMP_DIR)\n",
    "    os.makedirs(temp_dir_absolute, exist_ok=True)\n",
    "    \n",
    "    mni_template, mni_brain_mask, hippo_atlas, mni_template_path = load_mni_template_and_mask(config)\n",
    "    mni_template_sitk = sitk.ReadImage(mni_template_path)\n",
    "    \n",
    "    hippo_mask, bbox = get_hippocampal_mask_and_bbox(hippo_atlas, config)\n",
    "    print(f\"\\n‚úì Hippocampal bbox: {bbox}, voxels: {int(np.sum(hippo_mask))}\")\n",
    "    \n",
    "    visits_df = pd.read_csv(visits_csv_path)\n",
    "    print(f\"\\n‚úì Loaded {len(visits_df)} visits\")\n",
    "    print(f\"   {visits_df['dataset'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Processing {len(visits_df)} visits...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    n_success = 0\n",
    "    n_fail = 0\n",
    "    qc_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(visits_df.iterrows(), total=len(visits_df), desc=\"Preprocessing\"):\n",
    "        try:\n",
    "            res = preprocess_visit(\n",
    "                row, mni_template_sitk, mni_brain_mask,\n",
    "                mni_template.affine, hippo_mask, bbox, config,\n",
    "                temp_dir_absolute=temp_dir_absolute\n",
    "            )\n",
    "            n_success += 1\n",
    "            \n",
    "            if qc_count < config.QC_SAMPLE_SIZE:\n",
    "                qc_path = os.path.join(\n",
    "                    config.OUTPUT_ROOT, config.QC_DIR,\n",
    "                    f\"qc_{row['dataset']}_{row['subject_id']}_v{row['visit_index']}.png\"\n",
    "                )\n",
    "                data_mni = nib.load(res['mni_path']).get_fdata()\n",
    "                generate_qc_overlay(data_mni, hippo_mask, qc_path)\n",
    "                qc_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            n_fail += 1\n",
    "            if n_fail <= 5:\n",
    "                print(f\"\\n‚ö†Ô∏è  Failed: {row['subject_id']}_v{row['visit_index']}: {str(e)[:100]}\")\n",
    "            \n",
    "            res = {\n",
    "                'mni_path': None, 'full_t1_path': None,  # NEW\n",
    "                'hippo_roi_path': None, 'hippo_ext_path': None,\n",
    "                'p1': np.nan, 'p99': np.nan, 'mean_intensity': np.nan,\n",
    "                'std_intensity': np.nan, 'nonzero_voxels': np.nan,\n",
    "                'preproc_ok': False, 'error_msg': str(e)[:200]\n",
    "            }\n",
    "        \n",
    "        result = {**row.to_dict(), **res}\n",
    "        results.append(result)\n",
    "    \n",
    "    processed_df = pd.DataFrame(results)\n",
    "    output_csv = \"/kaggle/working/processed_volumes.csv\"\n",
    "    processed_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    success_rate = 100*n_success/len(visits_df) if len(visits_df) > 0 else 0\n",
    "    print(f\"\\n‚úÖ S2 COMPLETE\")\n",
    "    print(f\"   Successful: {n_success}/{len(visits_df)} ({success_rate:.1f}%)\")\n",
    "    print(f\"   Failed: {n_fail}\")\n",
    "    \n",
    "    if n_success > 0:\n",
    "        ok_df = processed_df[processed_df['preproc_ok']]\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   Datasets: {ok_df['dataset'].value_counts().to_dict()}\")\n",
    "        print(f\"   Labels: CN={int((ok_df['label']==0).sum())}, AD={int((ok_df['label']==1).sum())}\")\n",
    "        print(f\"   Mean intensity: {ok_df['mean_intensity'].mean():.3f}\")\n",
    "        \n",
    "        # Verify full-brain files exist\n",
    "        full_exist = ok_df['full_t1_path'].apply(lambda x: os.path.exists(x) if pd.notna(x) else False)\n",
    "        print(f\"   Full-brain volumes: {int(full_exist.sum())}/{len(ok_df)} exist\")\n",
    "    \n",
    "    # Validation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION (Data-Aware)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ok_df = processed_df[processed_df['preproc_ok']]\n",
    "    o3_total = int((visits_df['dataset'] == 'OASIS3').sum())\n",
    "    o3_success = int((ok_df['dataset'] == 'OASIS3').sum())\n",
    "    o3_rate = 100 * o3_success / o3_total if o3_total > 0 else 0\n",
    "    ad_count = int((ok_df['label'] == 1).sum())\n",
    "    \n",
    "    print(f\"\\nCriteria:\")\n",
    "    print(f\"  Overall success rate: {success_rate:.1f}% (target: ‚â•80%)\")\n",
    "    print(f\"  OASIS3 success rate:  {o3_rate:.1f}% (target: ‚â•70%)\")\n",
    "    print(f\"  AD visits available:  {ad_count} (target: ‚â•50)\")\n",
    "    \n",
    "    passed = (success_rate >= 80 and o3_rate >= 70 and ad_count >= 50)\n",
    "    \n",
    "    if passed:\n",
    "        print(f\"\\n‚úÖ PASS: S2 preprocessing meets quality thresholds\")\n",
    "        print(f\"   ({n_fail} failures due to corrupted source files)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå FAIL: S2 preprocessing below quality thresholds\")\n",
    "    \n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir_absolute):\n",
    "        n_temp = len(list(Path(temp_dir_absolute).glob(\"*\")))\n",
    "        shutil.rmtree(temp_dir_absolute)\n",
    "        print(f\"\\n‚úì Cleaned up {n_temp} temp files\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processed_df = run_s2_pipeline()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã Sample Output:\")\n",
    "    print(\"=\"*70)\n",
    "    ok_df = processed_df[processed_df['preproc_ok'] == True]\n",
    "    if len(ok_df) > 0:\n",
    "        cols = ['subject_id', 'visit_index', 'dataset', 'label', 'full_t1_path']\n",
    "        print(ok_df[cols].head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fed0186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:06:53.400722Z",
     "iopub.status.busy": "2025-11-29T20:06:53.400493Z",
     "iopub.status.idle": "2025-11-29T20:07:48.724945Z",
     "shell.execute_reply": "2025-11-29T20:07:48.723850Z"
    },
    "papermill": {
     "duration": 55.35726,
     "end_time": "2025-11-29T20:07:48.726126",
     "exception": false,
     "start_time": "2025-11-29T20:06:53.368866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì∏ SNIPPET S3: ROI-Focused Tri-Planar Slice Generator\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Volume source: hippo_ext_path\n",
      "  Output directory: /kaggle/working/slices_roi\n",
      "  Target image size: (224, 224)\n",
      "  Slices per visit: 8 axial + 6 coronal + 2 sagittal = 16\n",
      "\n",
      "‚úì Loaded 575 rows from /kaggle/working/processed_volumes.csv\n",
      "\n",
      "After filtering:\n",
      "  Valid visits: 575\n",
      "  CN (label=0): 499\n",
      "  AD (label=1): 76\n",
      "  Expected total slices: 9200\n",
      "\n",
      "======================================================================\n",
      "Generating slices...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing visits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 575/575 [00:55<00:00, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ S3 COMPLETE: Tri-Planar Slices Generated\n",
      "======================================================================\n",
      "\n",
      "üìä Summary:\n",
      "  Total slices generated: 9200\n",
      "  Visits processed: 575\n",
      "  Slices per visit: 16.0 (expected: 16)\n",
      "\n",
      "  Slice breakdown:\n",
      "    Axial: 4600\n",
      "    Coronal: 3450\n",
      "    Sagittal: 1150\n",
      "\n",
      "  Label distribution:\n",
      "    CN visits: 499\n",
      "    AD visits: 76\n",
      "\n",
      "  Output files:\n",
      "    Images: /kaggle/working/slices_roi/\n",
      "    Metadata: /kaggle/working/slices_metadata_ROI.csv\n",
      "    QC montages: /kaggle/working/slices_qc/ (3 examples)\n",
      "\n",
      "üìã Sample metadata (first 8 slices of first visit):\n",
      "    visit_id plane  plane_slice_rank  global_slice_idx  label  domain_id\n",
      "OAS2_0079_v2 axial                 0                 0      1          0\n",
      "OAS2_0079_v2 axial                 1                 1      1          0\n",
      "OAS2_0079_v2 axial                 2                 2      1          0\n",
      "OAS2_0079_v2 axial                 3                 3      1          0\n",
      "OAS2_0079_v2 axial                 4                 4      1          0\n",
      "OAS2_0079_v2 axial                 5                 5      1          0\n",
      "OAS2_0079_v2 axial                 6                 6      1          0\n",
      "OAS2_0079_v2 axial                 7                 7      1          0\n",
      "\n",
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "‚úÖ PASS: All 9200 image files exist\n",
      "‚úÖ PASS: All visits have exactly 16 slices\n",
      "\n",
      "======================================================================\n",
      "üé¨ Ready for downstream tasks:\n",
      "======================================================================\n",
      "  Next: S5 - DSBN backbone + visit-level attention\n",
      "  Then: S6 - Focal loss training\n",
      "  Then: S7 - XAI (Grad-CAM, hippocampal Dice)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S3: ROI-Focused Tri-Planar Slice Generator (PRODUCTION)\n",
    "\n",
    "Generates 16 slices per visit (8 axial + 6 coronal + 2 sagittal) from hippocampal ROI\n",
    "\n",
    "Input:\n",
    "    /kaggle/working/processed_volumes.csv (from S2)\n",
    "    Uses: hippo_ext_path (112√ó112√ó80 hippocampal-extended ROI)\n",
    "\n",
    "Output:\n",
    "    /kaggle/working/slices_roi/*.png (224√ó224 RGB images)\n",
    "    /kaggle/working/slices_metadata_ROI.csv (metadata for all slices)\n",
    "\n",
    "This feeds directly into:\n",
    "    - S5: DSBN backbone + attention\n",
    "    - S6: Focal loss training\n",
    "    - S7: XAI (Grad-CAM, hippocampal Dice)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class S3Config:\n",
    "    \"\"\"Configuration for tri-planar slice generation\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    PROCESSED_CSV = \"/kaggle/working/processed_volumes.csv\"\n",
    "    VOLUME_COL = \"hippo_ext_path\"  # ROI-focused (use \"full_t1_path\" for full-brain variant)\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_ROOT = \"/kaggle/working/slices_roi\"\n",
    "    METADATA_CSV = \"/kaggle/working/slices_metadata_ROI.csv\"\n",
    "    \n",
    "    # Image parameters\n",
    "    IMG_SIZE = (224, 224)  # H, W for ImageNet backbones\n",
    "    \n",
    "    # Slice counts per plane (K = 16 total)\n",
    "    N_AXIAL = 8      # Superior-inferior\n",
    "    N_CORONAL = 6    # Anterior-posterior\n",
    "    N_SAGITTAL = 2   # Left-right (medial)\n",
    "    \n",
    "    # Quality control\n",
    "    SAVE_QC_EXAMPLES = True\n",
    "    QC_VISITS = 3  # Save example montages for first N visits\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_slice_indices(dim_size, n_slices):\n",
    "    \"\"\"\n",
    "    Compute evenly spaced interior slice indices\n",
    "    \n",
    "    Avoids extreme border slices by using linspace on interior range\n",
    "    \n",
    "    Args:\n",
    "        dim_size: dimension size (e.g., 80 for z-axis)\n",
    "        n_slices: number of slices to extract (e.g., 8)\n",
    "    \n",
    "    Returns:\n",
    "        sorted array of unique integer indices\n",
    "    \n",
    "    Example:\n",
    "        dim_size=80, n_slices=8 ‚Üí [8, 18, 28, 38, 48, 58, 68, 78]\n",
    "    \"\"\"\n",
    "    if n_slices >= dim_size:\n",
    "        return np.arange(dim_size)\n",
    "    \n",
    "    # Use linspace to get n_slices+2 positions, then drop endpoints\n",
    "    positions = np.linspace(0, dim_size - 1, num=n_slices + 2)\n",
    "    inner = positions[1:-1]  # Drop first and last\n",
    "    indices = np.round(inner).astype(int)\n",
    "    \n",
    "    # Ensure uniqueness and sort\n",
    "    indices = sorted(set(indices))\n",
    "    \n",
    "    # If we lost indices due to rounding, fall back to linspace\n",
    "    if len(indices) < n_slices:\n",
    "        positions = np.linspace(0, dim_size - 1, num=n_slices)\n",
    "        indices = np.round(positions).astype(int)\n",
    "        indices = sorted(set(indices))\n",
    "    \n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "def normalize_slice_01(slice_2d):\n",
    "    \"\"\"\n",
    "    Normalize 2D slice to [0, 1] range\n",
    "    \n",
    "    S2 already normalized volumes, but we add safeguard\n",
    "    for numerical stability\n",
    "    \n",
    "    Args:\n",
    "        slice_2d: 2D numpy array\n",
    "    \n",
    "    Returns:\n",
    "        normalized float32 array in [0, 1]\n",
    "    \"\"\"\n",
    "    s = slice_2d.astype(np.float32)\n",
    "    min_val = s.min()\n",
    "    max_val = s.max()\n",
    "    \n",
    "    if max_val <= min_val + 1e-6:\n",
    "        # Constant slice (shouldn't happen with brain data)\n",
    "        return np.zeros_like(s, dtype=np.float32)\n",
    "    \n",
    "    s = (s - min_val) / (max_val - min_val)\n",
    "    return s\n",
    "\n",
    "\n",
    "def resize_to_img(slice_2d, img_size):\n",
    "    \"\"\"\n",
    "    Resize 2D slice to target size with bilinear interpolation\n",
    "    \n",
    "    Args:\n",
    "        slice_2d: 2D array in [0, 1]\n",
    "        img_size: tuple (H, W), e.g., (224, 224)\n",
    "    \n",
    "    Returns:\n",
    "        resized float32 array in [0, 1]\n",
    "    \"\"\"\n",
    "    H, W = img_size\n",
    "    \n",
    "    s_resized = resize(\n",
    "        slice_2d,\n",
    "        (H, W),\n",
    "        order=1,           # Bilinear interpolation\n",
    "        mode='constant',\n",
    "        cval=0.0,\n",
    "        anti_aliasing=True,\n",
    "        preserve_range=True\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    return s_resized\n",
    "\n",
    "\n",
    "def gray_to_rgb(slice_2d):\n",
    "    \"\"\"\n",
    "    Convert grayscale H√óW to RGB H√óW√ó3 by channel replication\n",
    "    \n",
    "    Required for ImageNet pretrained backbones expecting 3 channels\n",
    "    \"\"\"\n",
    "    return np.stack([slice_2d, slice_2d, slice_2d], axis=-1)\n",
    "\n",
    "\n",
    "def save_slice_png(slice_2d, output_path):\n",
    "    \"\"\"\n",
    "    Save 2D slice as PNG image\n",
    "    \n",
    "    Args:\n",
    "        slice_2d: float32 array in [0, 1], shape (H, W) or (H, W, 3)\n",
    "        output_path: path to save PNG\n",
    "    \"\"\"\n",
    "    # Ensure RGB\n",
    "    if slice_2d.ndim == 2:\n",
    "        slice_rgb = gray_to_rgb(slice_2d)\n",
    "    else:\n",
    "        slice_rgb = slice_2d\n",
    "    \n",
    "    # Convert to uint8 [0, 255]\n",
    "    img_uint8 = (slice_rgb * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Save using PIL\n",
    "    img_pil = Image.fromarray(img_uint8)\n",
    "    img_pil.save(output_path)\n",
    "\n",
    "\n",
    "def create_qc_montage(visit_id, slices_dict, output_path):\n",
    "    \"\"\"\n",
    "    Create QC montage showing all 16 slices for a visit\n",
    "    \n",
    "    Args:\n",
    "        visit_id: visit identifier\n",
    "        slices_dict: dict with keys 'axial', 'coronal', 'sagittal', \n",
    "                     values are lists of (H,W) arrays\n",
    "        output_path: path to save montage\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import gridspec\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = gridspec.GridSpec(3, 8, figure=fig, hspace=0.3, wspace=0.1)\n",
    "    \n",
    "    # Axial (top 2 rows, 4 per row)\n",
    "    axial_slices = slices_dict['axial']\n",
    "    for i, slice_2d in enumerate(axial_slices):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        ax.imshow(slice_2d, cmap='gray', origin='lower')\n",
    "        ax.set_title(f'Axial {i}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Coronal (bottom row, first 6 positions)\n",
    "    coronal_slices = slices_dict['coronal']\n",
    "    for i, slice_2d in enumerate(coronal_slices):\n",
    "        ax = fig.add_subplot(gs[2, i])\n",
    "        ax.imshow(slice_2d, cmap='gray', origin='lower')\n",
    "        ax.set_title(f'Coronal {i}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Sagittal (bottom row, last 2 positions)\n",
    "    sagittal_slices = slices_dict['sagittal']\n",
    "    for i, slice_2d in enumerate(sagittal_slices):\n",
    "        ax = fig.add_subplot(gs[2, 6 + i])\n",
    "        ax.imshow(slice_2d, cmap='gray', origin='lower')\n",
    "        ax.set_title(f'Sagittal {i}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.suptitle(f'Visit: {visit_id} (16 slices)', fontsize=14, fontweight='bold')\n",
    "    plt.savefig(output_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_s3_slice_generation(config=None):\n",
    "    \"\"\"\n",
    "    Execute tri-planar slice generation pipeline\n",
    "    \n",
    "    Returns:\n",
    "        meta_df: DataFrame with metadata for all generated slices\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = S3Config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üì∏ SNIPPET S3: ROI-Focused Tri-Planar Slice Generator\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Volume source: {config.VOLUME_COL}\")\n",
    "    print(f\"  Output directory: {config.OUTPUT_ROOT}\")\n",
    "    print(f\"  Target image size: {config.IMG_SIZE}\")\n",
    "    print(f\"  Slices per visit: {config.N_AXIAL} axial + {config.N_CORONAL} coronal + \"\n",
    "          f\"{config.N_SAGITTAL} sagittal = {config.N_AXIAL + config.N_CORONAL + config.N_SAGITTAL}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(config.OUTPUT_ROOT, exist_ok=True)\n",
    "    \n",
    "    if config.SAVE_QC_EXAMPLES:\n",
    "        qc_dir = os.path.join(os.path.dirname(config.OUTPUT_ROOT), \"slices_qc\")\n",
    "        os.makedirs(qc_dir, exist_ok=True)\n",
    "    \n",
    "    # Load processed volumes\n",
    "    df = pd.read_csv(config.PROCESSED_CSV)\n",
    "    print(f\"\\n‚úì Loaded {len(df)} rows from {config.PROCESSED_CSV}\")\n",
    "    \n",
    "    # Filter valid visits\n",
    "    df = df[(df['preproc_ok'] == True) & (df['label'].isin([0, 1]))].copy()\n",
    "    \n",
    "    # Ensure domain_id exists\n",
    "    if 'domain_id' not in df.columns:\n",
    "        df['domain_id'] = (df['dataset'] == 'OASIS3').astype(int)\n",
    "    \n",
    "    # Verify volume files exist\n",
    "    missing_mask = ~df[config.VOLUME_COL].apply(\n",
    "        lambda x: os.path.exists(x) if isinstance(x, str) else False\n",
    "    )\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Removing {missing_mask.sum()} visits with missing {config.VOLUME_COL}\")\n",
    "        df = df[~missing_mask].copy()\n",
    "    \n",
    "    print(f\"\\nAfter filtering:\")\n",
    "    print(f\"  Valid visits: {len(df)}\")\n",
    "    print(f\"  CN (label=0): {int((df['label'] == 0).sum())}\")\n",
    "    print(f\"  AD (label=1): {int((df['label'] == 1).sum())}\")\n",
    "    print(f\"  Expected total slices: {len(df) * (config.N_AXIAL + config.N_CORONAL + config.N_SAGITTAL)}\")\n",
    "    \n",
    "    # Container for metadata\n",
    "    meta_rows = []\n",
    "    qc_counter = 0\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Iterate over visits and generate 16 slices each\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Generating slices...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for visit_idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing visits\"):\n",
    "        subject_id = row['subject_id']\n",
    "        visit_index = row['visit_index']\n",
    "        dataset = row['dataset']\n",
    "        domain_id = int(row['domain_id'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        visit_id = f\"{subject_id}_v{visit_index}\"\n",
    "        vol_path = row[config.VOLUME_COL]\n",
    "        \n",
    "        # Load 3D ROI volume\n",
    "        img = nib.load(vol_path)\n",
    "        vol = img.get_fdata().astype(np.float32)\n",
    "        \n",
    "        if vol.ndim != 3:\n",
    "            print(f\"\\n‚ö†Ô∏è  Skipping {visit_id}: expected 3D volume, got shape {vol.shape}\")\n",
    "            continue\n",
    "        \n",
    "        Dx, Dy, Dz = vol.shape\n",
    "        \n",
    "        # Compute slice indices for each plane\n",
    "        axial_indices = compute_slice_indices(Dz, config.N_AXIAL)\n",
    "        coronal_indices = compute_slice_indices(Dy, config.N_CORONAL)\n",
    "        sagittal_indices = compute_slice_indices(Dx, config.N_SAGITTAL)\n",
    "        \n",
    "        global_idx = 0\n",
    "        qc_slices = {'axial': [], 'coronal': [], 'sagittal': []} if config.SAVE_QC_EXAMPLES else None\n",
    "        \n",
    "        # -------------------- AXIAL SLICES --------------------\n",
    "        for rank, z_idx in enumerate(axial_indices):\n",
    "            # Extract axial slice (x-y plane at z=z_idx)\n",
    "            slice_2d = vol[:, :, z_idx]\n",
    "            \n",
    "            # Normalize and resize\n",
    "            slice_2d = normalize_slice_01(slice_2d)\n",
    "            slice_2d = resize_to_img(slice_2d, config.IMG_SIZE)\n",
    "            \n",
    "            # Save for QC\n",
    "            if qc_slices is not None and qc_counter < config.QC_VISITS:\n",
    "                qc_slices['axial'].append(slice_2d)\n",
    "            \n",
    "            # Save as PNG\n",
    "            fname = f\"{visit_id}_axial_{rank:02d}.png\"\n",
    "            out_path = os.path.join(config.OUTPUT_ROOT, fname)\n",
    "            save_slice_png(slice_2d, out_path)\n",
    "            \n",
    "            # Record metadata\n",
    "            meta_rows.append({\n",
    "                'subject_id': subject_id,\n",
    "                'visit_index': visit_index,\n",
    "                'visit_id': visit_id,\n",
    "                'dataset': dataset,\n",
    "                'domain_id': domain_id,\n",
    "                'label': label,\n",
    "                'plane': 'axial',\n",
    "                'plane_slice_rank': rank,\n",
    "                'global_slice_idx': global_idx,\n",
    "                'img_path': out_path\n",
    "            })\n",
    "            global_idx += 1\n",
    "        \n",
    "        # -------------------- CORONAL SLICES --------------------\n",
    "        for rank, y_idx in enumerate(coronal_indices):\n",
    "            # Extract coronal slice (x-z plane at y=y_idx)\n",
    "            slice_2d = vol[:, y_idx, :]\n",
    "            \n",
    "            slice_2d = normalize_slice_01(slice_2d)\n",
    "            slice_2d = resize_to_img(slice_2d, config.IMG_SIZE)\n",
    "            \n",
    "            if qc_slices is not None and qc_counter < config.QC_VISITS:\n",
    "                qc_slices['coronal'].append(slice_2d)\n",
    "            \n",
    "            fname = f\"{visit_id}_coronal_{rank:02d}.png\"\n",
    "            out_path = os.path.join(config.OUTPUT_ROOT, fname)\n",
    "            save_slice_png(slice_2d, out_path)\n",
    "            \n",
    "            meta_rows.append({\n",
    "                'subject_id': subject_id,\n",
    "                'visit_index': visit_index,\n",
    "                'visit_id': visit_id,\n",
    "                'dataset': dataset,\n",
    "                'domain_id': domain_id,\n",
    "                'label': label,\n",
    "                'plane': 'coronal',\n",
    "                'plane_slice_rank': rank,\n",
    "                'global_slice_idx': global_idx,\n",
    "                'img_path': out_path\n",
    "            })\n",
    "            global_idx += 1\n",
    "        \n",
    "        # -------------------- SAGITTAL SLICES --------------------\n",
    "        for rank, x_idx in enumerate(sagittal_indices):\n",
    "            # Extract sagittal slice (y-z plane at x=x_idx)\n",
    "            slice_2d = vol[x_idx, :, :]\n",
    "            \n",
    "            slice_2d = normalize_slice_01(slice_2d)\n",
    "            slice_2d = resize_to_img(slice_2d, config.IMG_SIZE)\n",
    "            \n",
    "            if qc_slices is not None and qc_counter < config.QC_VISITS:\n",
    "                qc_slices['sagittal'].append(slice_2d)\n",
    "            \n",
    "            fname = f\"{visit_id}_sagittal_{rank:02d}.png\"\n",
    "            out_path = os.path.join(config.OUTPUT_ROOT, fname)\n",
    "            save_slice_png(slice_2d, out_path)\n",
    "            \n",
    "            meta_rows.append({\n",
    "                'subject_id': subject_id,\n",
    "                'visit_index': visit_index,\n",
    "                'visit_id': visit_id,\n",
    "                'dataset': dataset,\n",
    "                'domain_id': domain_id,\n",
    "                'label': label,\n",
    "                'plane': 'sagittal',\n",
    "                'plane_slice_rank': rank,\n",
    "                'global_slice_idx': global_idx,\n",
    "                'img_path': out_path\n",
    "            })\n",
    "            global_idx += 1\n",
    "        \n",
    "        # Generate QC montage for first few visits\n",
    "        if config.SAVE_QC_EXAMPLES and qc_counter < config.QC_VISITS:\n",
    "            qc_path = os.path.join(qc_dir, f\"qc_montage_{visit_id}.png\")\n",
    "            create_qc_montage(visit_id, qc_slices, qc_path)\n",
    "            qc_counter += 1\n",
    "        \n",
    "        # Sanity check\n",
    "        expected_slices = config.N_AXIAL + config.N_CORONAL + config.N_SAGITTAL\n",
    "        if global_idx != expected_slices:\n",
    "            print(f\"\\n‚ö†Ô∏è  Visit {visit_id}: generated {global_idx} slices, expected {expected_slices}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save metadata CSV\n",
    "    # ========================================================================\n",
    "    \n",
    "    meta_df = pd.DataFrame(meta_rows)\n",
    "    meta_df.to_csv(config.METADATA_CSV, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ S3 COMPLETE: Tri-Planar Slices Generated\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Total slices generated: {len(meta_df)}\")\n",
    "    print(f\"  Visits processed: {len(df)}\")\n",
    "    print(f\"  Slices per visit: {len(meta_df) / len(df):.1f} (expected: 16)\")\n",
    "    print(f\"\\n  Slice breakdown:\")\n",
    "    print(f\"    Axial: {int((meta_df['plane'] == 'axial').sum())}\")\n",
    "    print(f\"    Coronal: {int((meta_df['plane'] == 'coronal').sum())}\")\n",
    "    print(f\"    Sagittal: {int((meta_df['plane'] == 'sagittal').sum())}\")\n",
    "    print(f\"\\n  Label distribution:\")\n",
    "    print(f\"    CN visits: {int((meta_df.groupby('visit_id')['label'].first() == 0).sum())}\")\n",
    "    print(f\"    AD visits: {int((meta_df.groupby('visit_id')['label'].first() == 1).sum())}\")\n",
    "    print(f\"\\n  Output files:\")\n",
    "    print(f\"    Images: {config.OUTPUT_ROOT}/\")\n",
    "    print(f\"    Metadata: {config.METADATA_CSV}\")\n",
    "    \n",
    "    if config.SAVE_QC_EXAMPLES:\n",
    "        print(f\"    QC montages: {qc_dir}/ ({qc_counter} examples)\")\n",
    "    \n",
    "    # Show sample metadata\n",
    "    print(\"\\nüìã Sample metadata (first 8 slices of first visit):\")\n",
    "    sample_cols = ['visit_id', 'plane', 'plane_slice_rank', 'global_slice_idx', 'label', 'domain_id']\n",
    "    print(meta_df[sample_cols].head(8).to_string(index=False))\n",
    "    \n",
    "    # Validate\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check all files exist\n",
    "    files_exist = meta_df['img_path'].apply(os.path.exists)\n",
    "    n_missing = (~files_exist).sum()\n",
    "    \n",
    "    if n_missing > 0:\n",
    "        print(f\"‚ùå FAIL: {n_missing} image files missing!\")\n",
    "    else:\n",
    "        print(f\"‚úÖ PASS: All {len(meta_df)} image files exist\")\n",
    "    \n",
    "    # Check slice counts per visit\n",
    "    slices_per_visit = meta_df.groupby('visit_id').size()\n",
    "    expected = config.N_AXIAL + config.N_CORONAL + config.N_SAGITTAL\n",
    "    incorrect_counts = slices_per_visit[slices_per_visit != expected]\n",
    "    \n",
    "    if len(incorrect_counts) > 0:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: {len(incorrect_counts)} visits have incorrect slice counts\")\n",
    "        print(incorrect_counts.head())\n",
    "    else:\n",
    "        print(f\"‚úÖ PASS: All visits have exactly {expected} slices\")\n",
    "    \n",
    "    return meta_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meta_df = run_s3_slice_generation()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé¨ Ready for downstream tasks:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"  Next: S5 - DSBN backbone + visit-level attention\")\n",
    "    print(\"  Then: S6 - Focal loss training\")\n",
    "    print(\"  Then: S7 - XAI (Grad-CAM, hippocampal Dice)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a3d063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:07:48.811444Z",
     "iopub.status.busy": "2025-11-29T20:07:48.811149Z",
     "iopub.status.idle": "2025-11-29T20:07:49.003140Z",
     "shell.execute_reply": "2025-11-29T20:07:49.002032Z"
    },
    "papermill": {
     "duration": 0.235661,
     "end_time": "2025-11-29T20:07:49.004477",
     "exception": false,
     "start_time": "2025-11-29T20:07:48.768816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß© SNIPPET S4: Subject-Level Multi-Site Splits (LOCKED)\n",
      "======================================================================\n",
      "\n",
      "Random seed: 20250126\n",
      "\n",
      "Step 1: Loading visit-level data...\n",
      "  Loaded 575 visits from /kaggle/working/processed_volumes.csv\n",
      "  After filtering: 575 valid CN/AD visits\n",
      "    CN (label=0): 499\n",
      "    AD (label=1): 76\n",
      "\n",
      "Step 2: Building subject-level table...\n",
      "  Total subjects: 346\n",
      "  Stratification groups:\n",
      "    0_OASIS3: 202 subjects\n",
      "    0_OASIS2: 86 subjects\n",
      "    1_OASIS3: 33 subjects\n",
      "    1_OASIS2: 25 subjects\n",
      "\n",
      "Step 3: Creating 80/20 train+val/test split (subject-level)...\n",
      "  Train+Val subjects: 276\n",
      "  Test subjects:      70\n",
      "\n",
      "  Test set composition:\n",
      "    0_OASIS3: 41 subjects\n",
      "    0_OASIS2: 17 subjects\n",
      "    1_OASIS3: 7 subjects\n",
      "    1_OASIS2: 5 subjects\n",
      "  ‚úì No overlap between train+val and test subjects\n",
      "\n",
      "Step 4: Creating 5-fold CV on train+val subjects...\n",
      "  ‚úì All train+val subjects assigned to folds\n",
      "\n",
      "  Fold distribution (subject-level):\n",
      "    Fold 0: 56 subjects\n",
      "    Fold 1: 55 subjects\n",
      "    Fold 2: 55 subjects\n",
      "    Fold 3: 55 subjects\n",
      "    Fold 4: 55 subjects\n",
      "\n",
      "Step 5: Mapping splits to visit level...\n",
      "\n",
      "  Visit-level split summary:\n",
      "    trainval: 463 visits\n",
      "    test: 112 visits\n",
      "\n",
      "  Fold distribution (trainval visits only):\n",
      "    Fold 0: 93 visits\n",
      "    Fold 1: 93 visits\n",
      "    Fold 2: 89 visits\n",
      "    Fold 3: 100 visits\n",
      "    Fold 4: 88 visits\n",
      "\n",
      "  ‚úì Saved visit-level splits to: /kaggle/working/visits_with_splits.csv\n",
      "\n",
      "Step 6: Propagating splits to slice level...\n",
      "  Loaded 9200 slices from /kaggle/working/slices_metadata_ROI.csv\n",
      "  After merge: 9200 slices with split/fold info\n",
      "  ‚úì All visits have exactly 16 slices\n",
      "  ‚úì Saved slice-level splits to: /kaggle/working/slices_metadata_ROI_splits.csv\n",
      "\n",
      "======================================================================\n",
      "‚úÖ S4 COMPLETE: Subject-Level Splits Ready\n",
      "======================================================================\n",
      "\n",
      "üìä Summary:\n",
      "\n",
      "  Subject-level:\n",
      "    Total subjects: 346\n",
      "    Train+Val: 276 (79.8%)\n",
      "    Test: 70 (20.2%)\n",
      "\n",
      "  Visit-level:\n",
      "    Total visits: 575\n",
      "    Train+Val: 463 (80.5%)\n",
      "    Test: 112 (19.5%)\n",
      "\n",
      "  Slice-level:\n",
      "    Total slices: 9200\n",
      "    Train+Val: 7408 (80.5%)\n",
      "    Test: 1792 (19.5%)\n",
      "\n",
      "  Test set label distribution:\n",
      "    CN: 100 (89.3%)\n",
      "    AD: 12 (10.7%)\n",
      "\n",
      "  Fold balance (train+val slices):\n",
      "    Fold 0: 1488 slices (20.1%)\n",
      "    Fold 1: 1488 slices (20.1%)\n",
      "    Fold 2: 1424 slices (19.2%)\n",
      "    Fold 3: 1600 slices (21.6%)\n",
      "    Fold 4: 1408 slices (19.0%)\n",
      "\n",
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "‚úÖ No subject leakage between train+val and test\n",
      "‚úÖ All visits assigned to split\n",
      "‚úÖ All train+val visits assigned to fold\n",
      "‚úÖ Test visits correctly marked (fold_id=-1)\n",
      "‚úÖ All visits have correct slice count (16)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL VALIDATION CHECKS PASSED (5/5)\n",
      "\n",
      "======================================================================\n",
      "üé¨ Ready for downstream tasks:\n",
      "======================================================================\n",
      "  Next: S5 - DSBN backbone + visit-level attention\n",
      "  Then: S6 - Focal loss training\n",
      "  Then: S7 - XAI (Grad-CAM, hippocampal Dice)\n",
      "\n",
      "  Files ready:\n",
      "    - visits_with_splits.csv\n",
      "    - slices_metadata_ROI_splits.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S4: Subject-Level Multi-Site Splits (LOCKED)\n",
    "\n",
    "Creates subject-level stratified splits with locked test set:\n",
    "- 80/20 train+val/test split (stratified by site √ó diagnosis)\n",
    "- 5-fold CV on train+val subjects\n",
    "- Propagates splits to visit and slice levels\n",
    "\n",
    "Input:\n",
    "    /kaggle/working/processed_volumes.csv (from S2)\n",
    "    /kaggle/working/slices_metadata_ROI.csv (from S3)\n",
    "\n",
    "Output:\n",
    "    /kaggle/working/visits_with_splits.csv (visit-level splits)\n",
    "    /kaggle/working/slices_metadata_ROI_splits.csv (slice-level splits)\n",
    "\n",
    "Design:\n",
    "    - Subject-level splits (no leakage)\n",
    "    - Stratified by site (OASIS2/3) √ó diagnosis (CN/AD)\n",
    "    - Fixed random seed for reproducibility\n",
    "    - Test set locked across all experiments\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "RANDOM_SEED = 20250126  # Fixed seed for reproducibility\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def build_s4_splits(\n",
    "    processed_csv=\"/kaggle/working/processed_volumes.csv\",\n",
    "    slices_csv=\"/kaggle/working/slices_metadata_ROI.csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build subject-level stratified splits with locked test set\n",
    "    \n",
    "    Returns:\n",
    "        visits_df: DataFrame with visit-level split information\n",
    "        slices_merged: DataFrame with slice-level split information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üß© SNIPPET S4: Subject-Level Multi-Site Splits (LOCKED)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nRandom seed: {RANDOM_SEED}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. Load visit-level table from S2\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 1: Loading visit-level data...\")\n",
    "    visits_df = pd.read_csv(processed_csv)\n",
    "    print(f\"  Loaded {len(visits_df)} visits from {processed_csv}\")\n",
    "\n",
    "    # Keep only valid CN/AD visits with successful preprocessing\n",
    "    visits_df = visits_df[\n",
    "        (visits_df[\"preproc_ok\"] == True) & \n",
    "        (visits_df[\"label\"].isin([0, 1]))\n",
    "    ].copy()\n",
    "\n",
    "    # Define visit_id consistently with S3\n",
    "    visits_df[\"visit_id\"] = visits_df.apply(\n",
    "        lambda r: f\"{r['subject_id']}_v{r['visit_index']}\", axis=1\n",
    "    )\n",
    "\n",
    "    # Domain id: 0 = OASIS2, 1 = OASIS3\n",
    "    if \"domain_id\" not in visits_df.columns:\n",
    "        visits_df[\"domain_id\"] = (visits_df[\"dataset\"] == \"OASIS3\").astype(int)\n",
    "\n",
    "    print(f\"  After filtering: {len(visits_df)} valid CN/AD visits\")\n",
    "    print(f\"    CN (label=0): {int((visits_df['label'] == 0).sum())}\")\n",
    "    print(f\"    AD (label=1): {int((visits_df['label'] == 1).sum())}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. Build subject-level table for stratified splitting\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 2: Building subject-level table...\")\n",
    "    \n",
    "    subjects_df = (\n",
    "        visits_df.groupby(\"subject_id\")\n",
    "        .agg(\n",
    "            label=(\"label\", \"first\"),  # Use first visit's label\n",
    "            dataset=(\"dataset\", lambda x: x.mode()[0]),  # Most common dataset\n",
    "            n_visits=(\"visit_id\", \"count\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Stratification group key: label_site (e.g., \"0_OASIS2\", \"1_OASIS3\")\n",
    "    subjects_df[\"group\"] = (\n",
    "        subjects_df[\"label\"].astype(str) + \"_\" + subjects_df[\"dataset\"].astype(str)\n",
    "    )\n",
    "\n",
    "    print(f\"  Total subjects: {len(subjects_df)}\")\n",
    "    print(f\"  Stratification groups:\")\n",
    "    for group, count in subjects_df[\"group\"].value_counts().items():\n",
    "        print(f\"    {group}: {count} subjects\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. 80/20 subject-level split ‚Üí locked test set\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 3: Creating 80/20 train+val/test split (subject-level)...\")\n",
    "    \n",
    "    trainval_subj, test_subj = train_test_split(\n",
    "        subjects_df,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=subjects_df[\"group\"]\n",
    "    )\n",
    "\n",
    "    print(f\"  Train+Val subjects: {len(trainval_subj)}\")\n",
    "    print(f\"  Test subjects:      {len(test_subj)}\")\n",
    "    \n",
    "    # Show test set composition\n",
    "    print(f\"\\n  Test set composition:\")\n",
    "    for group, count in test_subj[\"group\"].value_counts().items():\n",
    "        print(f\"    {group}: {count} subjects\")\n",
    "\n",
    "    trainval_subj_ids = set(trainval_subj[\"subject_id\"])\n",
    "    test_subj_ids = set(test_subj[\"subject_id\"])\n",
    "\n",
    "    # Safety check: no overlap\n",
    "    assert trainval_subj_ids.isdisjoint(test_subj_ids), \\\n",
    "        \"‚ùå CRITICAL: Overlap between trainval and test subjects!\"\n",
    "    print(\"  ‚úì No overlap between train+val and test subjects\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 4. 5-fold stratified CV on train+val subjects (subject-level)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 4: Creating 5-fold CV on train+val subjects...\")\n",
    "    \n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=5, \n",
    "        shuffle=True, \n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    trainval_subj = trainval_subj.reset_index(drop=True)\n",
    "    fold_ids = np.full(len(trainval_subj), -1, dtype=int)\n",
    "\n",
    "    for fold_idx, (_, val_index) in enumerate(\n",
    "        skf.split(trainval_subj[\"subject_id\"], trainval_subj[\"group\"])\n",
    "    ):\n",
    "        fold_ids[val_index] = fold_idx\n",
    "\n",
    "    trainval_subj[\"fold_id\"] = fold_ids\n",
    "\n",
    "    # Sanity check\n",
    "    assert not (trainval_subj[\"fold_id\"] == -1).any(), \\\n",
    "        \"‚ùå CRITICAL: Unassigned fold_id found!\"\n",
    "\n",
    "    print(f\"  ‚úì All train+val subjects assigned to folds\")\n",
    "    print(f\"\\n  Fold distribution (subject-level):\")\n",
    "    fold_counts = trainval_subj[\"fold_id\"].value_counts().sort_index()\n",
    "    for fold_id, count in fold_counts.items():\n",
    "        print(f\"    Fold {fold_id}: {count} subjects\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 5. Map subject-level splits back to visits\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 5: Mapping splits to visit level...\")\n",
    "    \n",
    "    # Initialize split columns\n",
    "    visits_df[\"split\"] = \"trainval\"\n",
    "    visits_df[\"fold_id\"] = -1\n",
    "\n",
    "    # Mark test visits\n",
    "    visits_df.loc[visits_df[\"subject_id\"].isin(test_subj_ids), \"split\"] = \"test\"\n",
    "\n",
    "    # Assign fold_id for trainval visits\n",
    "    fold_map = dict(zip(trainval_subj[\"subject_id\"], trainval_subj[\"fold_id\"]))\n",
    "    mask_trainval = visits_df[\"subject_id\"].isin(trainval_subj_ids)\n",
    "    visits_df.loc[mask_trainval, \"fold_id\"] = (\n",
    "        visits_df.loc[mask_trainval, \"subject_id\"].map(fold_map)\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n  Visit-level split summary:\")\n",
    "    for split, count in visits_df[\"split\"].value_counts().items():\n",
    "        print(f\"    {split}: {count} visits\")\n",
    "\n",
    "    print(f\"\\n  Fold distribution (trainval visits only):\")\n",
    "    fold_visit_counts = (\n",
    "        visits_df[visits_df[\"split\"] == \"trainval\"][\"fold_id\"]\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "    )\n",
    "    for fold_id, count in fold_visit_counts.items():\n",
    "        print(f\"    Fold {fold_id}: {count} visits\")\n",
    "\n",
    "    # Save visit-level splits\n",
    "    visits_out = \"/kaggle/working/visits_with_splits.csv\"\n",
    "    visits_df.to_csv(visits_out, index=False)\n",
    "    print(f\"\\n  ‚úì Saved visit-level splits to: {visits_out}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 6. Propagate splits to slice-level metadata\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 6: Propagating splits to slice level...\")\n",
    "    \n",
    "    slices_df = pd.read_csv(slices_csv)\n",
    "    print(f\"  Loaded {len(slices_df)} slices from {slices_csv}\")\n",
    "\n",
    "    # Join on visit_id to get split/fold info\n",
    "    slices_merged = slices_df.merge(\n",
    "        visits_df[[\"visit_id\", \"split\", \"fold_id\"]],\n",
    "        on=\"visit_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    print(f\"  After merge: {len(slices_merged)} slices with split/fold info\")\n",
    "\n",
    "    # Sanity: each visit still has 16 slices\n",
    "    slices_per_visit = slices_merged.groupby(\"visit_id\").size()\n",
    "    expected_slices = 16\n",
    "    \n",
    "    incorrect_visits = slices_per_visit[slices_per_visit != expected_slices]\n",
    "    if len(incorrect_visits) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: {len(incorrect_visits)} visits do not have {expected_slices} slices\")\n",
    "        print(f\"     Expected: {expected_slices}, found range: [{slices_per_visit.min()}, {slices_per_visit.max()}]\")\n",
    "    else:\n",
    "        print(f\"  ‚úì All visits have exactly {expected_slices} slices\")\n",
    "\n",
    "    # Save slice-level splits\n",
    "    slices_out = \"/kaggle/working/slices_metadata_ROI_splits.csv\"\n",
    "    slices_merged.to_csv(slices_out, index=False)\n",
    "    print(f\"  ‚úì Saved slice-level splits to: {slices_out}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 7. Final summary and validation\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ S4 COMPLETE: Subject-Level Splits Ready\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    \n",
    "    # Subject-level\n",
    "    print(f\"\\n  Subject-level:\")\n",
    "    print(f\"    Total subjects: {len(subjects_df)}\")\n",
    "    print(f\"    Train+Val: {len(trainval_subj)} ({len(trainval_subj)/len(subjects_df)*100:.1f}%)\")\n",
    "    print(f\"    Test: {len(test_subj)} ({len(test_subj)/len(subjects_df)*100:.1f}%)\")\n",
    "\n",
    "    # Visit-level\n",
    "    print(f\"\\n  Visit-level:\")\n",
    "    total_visits = len(visits_df)\n",
    "    trainval_visits = int((visits_df[\"split\"] == \"trainval\").sum())\n",
    "    test_visits = int((visits_df[\"split\"] == \"test\").sum())\n",
    "    print(f\"    Total visits: {total_visits}\")\n",
    "    print(f\"    Train+Val: {trainval_visits} ({trainval_visits/total_visits*100:.1f}%)\")\n",
    "    print(f\"    Test: {test_visits} ({test_visits/total_visits*100:.1f}%)\")\n",
    "\n",
    "    # Slice-level\n",
    "    print(f\"\\n  Slice-level:\")\n",
    "    total_slices = len(slices_merged)\n",
    "    trainval_slices = int((slices_merged[\"split\"] == \"trainval\").sum())\n",
    "    test_slices = int((slices_merged[\"split\"] == \"test\").sum())\n",
    "    print(f\"    Total slices: {total_slices}\")\n",
    "    print(f\"    Train+Val: {trainval_slices} ({trainval_slices/total_slices*100:.1f}%)\")\n",
    "    print(f\"    Test: {test_slices} ({test_slices/total_slices*100:.1f}%)\")\n",
    "\n",
    "    # Label distribution in test set\n",
    "    print(f\"\\n  Test set label distribution:\")\n",
    "    test_visits_df = visits_df[visits_df[\"split\"] == \"test\"]\n",
    "    cn_test = int((test_visits_df[\"label\"] == 0).sum())\n",
    "    ad_test = int((test_visits_df[\"label\"] == 1).sum())\n",
    "    print(f\"    CN: {cn_test} ({cn_test/len(test_visits_df)*100:.1f}%)\")\n",
    "    print(f\"    AD: {ad_test} ({ad_test/len(test_visits_df)*100:.1f}%)\")\n",
    "\n",
    "    # Fold balance\n",
    "    print(f\"\\n  Fold balance (train+val slices):\")\n",
    "    trainval_fold_counts = (\n",
    "        slices_merged[slices_merged[\"split\"] == \"trainval\"][\"fold_id\"]\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "    )\n",
    "    for fold_id, count in trainval_fold_counts.items():\n",
    "        pct = count / trainval_slices * 100\n",
    "        print(f\"    Fold {fold_id}: {count} slices ({pct:.1f}%)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Validation checks\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    checks_passed = 0\n",
    "    checks_total = 0\n",
    "\n",
    "    # Check 1: No subject overlap\n",
    "    checks_total += 1\n",
    "    if trainval_subj_ids.isdisjoint(test_subj_ids):\n",
    "        print(\"‚úÖ No subject leakage between train+val and test\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(\"‚ùå CRITICAL: Subject leakage detected!\")\n",
    "\n",
    "    # Check 2: All visits assigned to split\n",
    "    checks_total += 1\n",
    "    unassigned_visits = visits_df[visits_df[\"split\"].isna()]\n",
    "    if len(unassigned_visits) == 0:\n",
    "        print(\"‚úÖ All visits assigned to split\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå {len(unassigned_visits)} visits not assigned to split\")\n",
    "\n",
    "    # Check 3: All trainval visits have fold_id\n",
    "    checks_total += 1\n",
    "    trainval_no_fold = visits_df[\n",
    "        (visits_df[\"split\"] == \"trainval\") & \n",
    "        (visits_df[\"fold_id\"] == -1)\n",
    "    ]\n",
    "    if len(trainval_no_fold) == 0:\n",
    "        print(\"‚úÖ All train+val visits assigned to fold\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå {len(trainval_no_fold)} train+val visits missing fold_id\")\n",
    "\n",
    "    # Check 4: Test visits have fold_id = -1\n",
    "    checks_total += 1\n",
    "    test_with_fold = visits_df[\n",
    "        (visits_df[\"split\"] == \"test\") & \n",
    "        (visits_df[\"fold_id\"] != -1)\n",
    "    ]\n",
    "    if len(test_with_fold) == 0:\n",
    "        print(\"‚úÖ Test visits correctly marked (fold_id=-1)\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå {len(test_with_fold)} test visits have invalid fold_id\")\n",
    "\n",
    "    # Check 5: Slice counts per visit\n",
    "    checks_total += 1\n",
    "    if len(incorrect_visits) == 0:\n",
    "        print(\"‚úÖ All visits have correct slice count (16)\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {len(incorrect_visits)} visits have incorrect slice count\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if checks_passed == checks_total:\n",
    "        print(f\"‚úÖ ALL VALIDATION CHECKS PASSED ({checks_passed}/{checks_total})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  VALIDATION: {checks_passed}/{checks_total} checks passed\")\n",
    "\n",
    "    return visits_df, slices_merged\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visits_df, slices_df = build_s4_splits()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé¨ Ready for downstream tasks:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"  Next: S5 - DSBN backbone + visit-level attention\")\n",
    "    print(\"  Then: S6 - Focal loss training\")\n",
    "    print(\"  Then: S7 - XAI (Grad-CAM, hippocampal Dice)\")\n",
    "    print(\"\\n  Files ready:\")\n",
    "    print(\"    - visits_with_splits.csv\")\n",
    "    print(\"    - slices_metadata_ROI_splits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c735797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:07:49.090864Z",
     "iopub.status.busy": "2025-11-29T20:07:49.090634Z",
     "iopub.status.idle": "2025-11-29T20:08:10.137980Z",
     "shell.execute_reply": "2025-11-29T20:08:10.137156Z"
    },
    "papermill": {
     "duration": 21.0921,
     "end_time": "2025-11-29T20:08:10.139143",
     "exception": false,
     "start_time": "2025-11-29T20:07:49.047043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ Testing S5 Components (Fixed for S6)\n",
      "======================================================================\n",
      "\n",
      "Test 1: Dataset loading...\n",
      "  Train dataset: 370 visits\n",
      "    CN: 319, AD: 51\n",
      "  ‚úì Dataset shape: imgs=torch.Size([16, 3, 224, 224]), label=0.0, domain_id=0\n",
      "  ‚úì Image value range: [0.000, 0.988] (should be ~[0, 1])\n",
      "  ‚úì Dataset test passed\n",
      "\n",
      "Test 2: Domain-Specific BN...\n",
      "  ‚úì DSBN output shape: torch.Size([32, 512, 7, 7])\n",
      "  ‚úì DSBN test passed\n",
      "\n",
      "Test 3: Visit Attention...\n",
      "  ‚úì Attention outputs: v=torch.Size([4, 512]), alpha=torch.Size([4, 16])\n",
      "  ‚úì Attention test passed\n",
      "\n",
      "Test 4: Complete Model...\n",
      "  ‚úì Model outputs: logits=torch.Size([2]), alpha=torch.Size([2, 16])\n",
      "  ‚úì Model test passed\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All S5 tests passed!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üé¨ S5 Ready for S6 Training\n",
      "======================================================================\n",
      "\n",
      "Key change:\n",
      "  - Dataset now returns images in [0, 1] WITHOUT normalization\n",
      "  - S6 training loop will handle normalization + augmentation\n",
      "\n",
      "Next: Run S6 training script\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S5: Dataset + Model Architecture (DSBN + Attention) - FIXED FOR S6\n",
    "\n",
    "KEY CHANGE: Dataset now returns images in [0, 1] WITHOUT normalization\n",
    "(S6 training loop will handle normalization + augmentation)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class S5Config:\n",
    "    \"\"\"Configuration for dataset and model\"\"\"\n",
    "    \n",
    "    # Data\n",
    "    SLICES_CSV = \"/kaggle/working/slices_metadata_ROI_splits.csv\"\n",
    "    \n",
    "    # Model architecture\n",
    "    BACKBONE = \"vgg16_bn\"  # Options: 'vgg16_bn', 'resnet50', 'densenet121'\n",
    "    FEATURE_DIM = 512      # VGG16: 512, ResNet50: 2048, DenseNet121: 1024\n",
    "    ATTENTION_DIM = 128\n",
    "    NUM_DOMAINS = 2        # OASIS2=0, OASIS3=1\n",
    "    USE_DSBN = True\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # Image preprocessing\n",
    "    IMG_SIZE = 224\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET: TriPlanarVisitDataset (FIXED FOR S6)\n",
    "# ============================================================================\n",
    "\n",
    "class TriPlanarVisitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for tri-planar visit-level classification\n",
    "    \n",
    "    FIXED FOR S6: Returns images in [0, 1] WITHOUT normalization\n",
    "    (S6 training loop handles normalization + label-aware augmentation)\n",
    "    \n",
    "    Loads 16 slices per visit (8 axial + 6 coronal + 2 sagittal)\n",
    "    Supports train/val/test splits with fold-based CV\n",
    "    \n",
    "    Args:\n",
    "        csv_path: path to slices_metadata_ROI_splits.csv\n",
    "        mode: 'train', 'val', or 'test'\n",
    "        fold_id: fold number (0-4) for train/val split, ignored for test\n",
    "        transform: NOT USED (kept for compatibility)\n",
    "        config: S5Config instance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, mode, fold_id=None, transform=None, config=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.mode = mode\n",
    "        self.fold_id = fold_id\n",
    "        self.config = config or S5Config()\n",
    "        \n",
    "        # Load metadata\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Filter by split and fold\n",
    "        if mode == \"train\":\n",
    "            assert fold_id is not None, \"fold_id required for train mode\"\n",
    "            self.df = self.df[\n",
    "                (self.df[\"split\"] == \"trainval\") & \n",
    "                (self.df[\"fold_id\"] != fold_id)\n",
    "            ].copy()\n",
    "        elif mode == \"val\":\n",
    "            assert fold_id is not None, \"fold_id required for val mode\"\n",
    "            self.df = self.df[\n",
    "                (self.df[\"split\"] == \"trainval\") & \n",
    "                (self.df[\"fold_id\"] == fold_id)\n",
    "            ].copy()\n",
    "        elif mode == \"test\":\n",
    "            self.df = self.df[self.df[\"split\"] == \"test\"].copy()\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be 'train', 'val', or 'test', got {mode}\")\n",
    "        \n",
    "        # Group by visit_id\n",
    "        self.visits = []\n",
    "        grouped = self.df.groupby(\"visit_id\")\n",
    "        \n",
    "        for visit_id, group in grouped:\n",
    "            group_sorted = group.sort_values(\"global_slice_idx\")\n",
    "            \n",
    "            if len(group_sorted) != 16:\n",
    "                print(f\"Warning: Visit {visit_id} has {len(group_sorted)} slices, skipping\")\n",
    "                continue\n",
    "            \n",
    "            self.visits.append({\n",
    "                \"visit_id\": visit_id,\n",
    "                \"rows\": group_sorted,\n",
    "                \"label\": int(group_sorted[\"label\"].iloc[0]),\n",
    "                \"domain_id\": int(group_sorted[\"domain_id\"].iloc[0]),\n",
    "            })\n",
    "        \n",
    "        print(f\"  {mode.capitalize()} dataset: {len(self.visits)} visits\")\n",
    "        if len(self.visits) > 0:\n",
    "            cn_count = sum(1 for v in self.visits if v[\"label\"] == 0)\n",
    "            ad_count = sum(1 for v in self.visits if v[\"label\"] == 1)\n",
    "            print(f\"    CN: {cn_count}, AD: {ad_count}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.visits)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        CRITICAL CHANGE: Returns images in [0, 1] WITHOUT normalization\n",
    "        \n",
    "        S6 training loop will handle:\n",
    "        - Label-aware augmentation\n",
    "        - ImageNet normalization\n",
    "        \"\"\"\n",
    "        visit = self.visits[idx]\n",
    "        rows = visit[\"rows\"]\n",
    "        \n",
    "        # Load all 16 slices\n",
    "        imgs = []\n",
    "        for _, row in rows.iterrows():\n",
    "            img_path = row[\"img_path\"]\n",
    "            \n",
    "            # Load image and convert to tensor [0, 1]\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transforms.ToTensor()(img)  # Converts to [0, 1]\n",
    "            \n",
    "            imgs.append(img)\n",
    "        \n",
    "        # Stack to (K, C, H, W)\n",
    "        imgs = torch.stack(imgs, dim=0)  # (16, 3, 224, 224) in [0, 1]\n",
    "        \n",
    "        label = torch.tensor(visit[\"label\"], dtype=torch.float32)\n",
    "        domain_id = torch.tensor(visit[\"domain_id\"], dtype=torch.long)\n",
    "        \n",
    "        return imgs, label, domain_id\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DOMAIN-SPECIFIC BATCH NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class DomainSpecificBN2d(nn.Module):\n",
    "    \"\"\"Domain-Specific Batch Normalization for multi-site adaptation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_domains=2):\n",
    "        super().__init__()\n",
    "        self.num_domains = num_domains\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm2d(num_features) for _ in range(num_domains)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, domain_id):\n",
    "        out = torch.zeros_like(x)\n",
    "        \n",
    "        for d in range(self.num_domains):\n",
    "            mask = (domain_id == d)\n",
    "            if mask.any():\n",
    "                out[mask] = self.bns[d](x[mask])\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISIT-LEVEL ATTENTION\n",
    "# ============================================================================\n",
    "\n",
    "class VisitAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism for aggregating K slice features into visit embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, attn_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_a = nn.Linear(in_dim, attn_dim)\n",
    "        self.b_a = nn.Parameter(torch.zeros(attn_dim))\n",
    "        self.w_a = nn.Linear(attn_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, F):\n",
    "        H = torch.tanh(self.W_a(F) + self.b_a)\n",
    "        s = self.w_a(H).squeeze(-1)\n",
    "        alpha = torch.softmax(s, dim=1)\n",
    "        v = torch.sum(alpha.unsqueeze(-1) * F, dim=1)\n",
    "        \n",
    "        return v, alpha\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISIT CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "class VisitClassifier(nn.Module):\n",
    "    \"\"\"Simple linear classifier for visit embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, 1)\n",
    "    \n",
    "    def forward(self, v):\n",
    "        logits = self.fc(v).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE MODEL: TriPlanarADNet\n",
    "# ============================================================================\n",
    "\n",
    "class TriPlanarADNet(nn.Module):\n",
    "    \"\"\"Complete tri-planar AD classification network\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone_name='vgg16_bn', num_domains=2, \n",
    "                 use_dsbn=True, pretrained=True, config=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config or S5Config()\n",
    "        self.backbone_name = backbone_name\n",
    "        self.use_dsbn = use_dsbn\n",
    "        \n",
    "        # Load backbone\n",
    "        self.backbone, self.feature_dim = self._load_backbone(backbone_name, pretrained)\n",
    "        \n",
    "        # Domain-specific BN\n",
    "        if use_dsbn:\n",
    "            self.dsbn = DomainSpecificBN2d(\n",
    "                num_features=self.feature_dim,\n",
    "                num_domains=num_domains\n",
    "            )\n",
    "        else:\n",
    "            self.dsbn = None\n",
    "        \n",
    "        # Visit-level attention\n",
    "        self.attention = VisitAttention(\n",
    "            in_dim=self.feature_dim,\n",
    "            attn_dim=self.config.ATTENTION_DIM\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = VisitClassifier(in_dim=self.feature_dim)\n",
    "    \n",
    "    def _load_backbone(self, backbone_name, pretrained):\n",
    "        if backbone_name == 'vgg16_bn':\n",
    "            vgg = models.vgg16_bn(pretrained=pretrained)\n",
    "            backbone = vgg.features\n",
    "            feature_dim = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "            backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "            feature_dim = 2048\n",
    "        elif backbone_name == 'densenet121':\n",
    "            densenet = models.densenet121(pretrained=pretrained)\n",
    "            backbone = densenet.features\n",
    "            feature_dim = 1024\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "        \n",
    "        return backbone, feature_dim\n",
    "    \n",
    "    def forward(self, imgs, domain_id):\n",
    "        B, K, C, H, W = imgs.shape\n",
    "        \n",
    "        # Reshape to process all slices\n",
    "        x = imgs.view(B * K, C, H, W)\n",
    "        \n",
    "        # Repeat domain_id for each slice\n",
    "        domain_id_slices = domain_id.unsqueeze(1).repeat(1, K).view(-1)\n",
    "        \n",
    "        # Extract conv features\n",
    "        feat = self.backbone(x)\n",
    "        \n",
    "        # Apply DSBN\n",
    "        if self.use_dsbn:\n",
    "            feat = self.dsbn(feat, domain_id_slices)\n",
    "        \n",
    "        # Global average pooling\n",
    "        feat_pooled = feat.mean(dim=[2, 3])\n",
    "        \n",
    "        # Reshape to (B, K, D)\n",
    "        F = feat_pooled.view(B, K, -1)\n",
    "        \n",
    "        # Visit-level attention\n",
    "        v, alpha = self.attention(F)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(v)\n",
    "        \n",
    "        return logits, alpha\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING: Verify Components\n",
    "# ============================================================================\n",
    "\n",
    "def test_s5_components():\n",
    "    \"\"\"Test dataset and model components\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üß™ Testing S5 Components (Fixed for S6)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    config = S5Config()\n",
    "    \n",
    "    # Test 1: Dataset\n",
    "    print(\"\\nTest 1: Dataset loading...\")\n",
    "    try:\n",
    "        dataset = TriPlanarVisitDataset(\n",
    "            csv_path=config.SLICES_CSV,\n",
    "            mode=\"train\",\n",
    "            fold_id=0,\n",
    "            transform=None,  # Not used anymore\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        imgs, label, domain_id = dataset[0]\n",
    "        print(f\"  ‚úì Dataset shape: imgs={imgs.shape}, label={label}, domain_id={domain_id}\")\n",
    "        print(f\"  ‚úì Image value range: [{imgs.min():.3f}, {imgs.max():.3f}] (should be ~[0, 1])\")\n",
    "        assert imgs.shape == (16, 3, 224, 224), f\"Expected (16, 3, 224, 224), got {imgs.shape}\"\n",
    "        assert imgs.min() >= 0 and imgs.max() <= 1, \"Images should be in [0, 1] range\"\n",
    "        print(\"  ‚úì Dataset test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Dataset test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # Test 2: DSBN\n",
    "    print(\"\\nTest 2: Domain-Specific BN...\")\n",
    "    try:\n",
    "        dsbn = DomainSpecificBN2d(num_features=512, num_domains=2)\n",
    "        x = torch.randn(32, 512, 7, 7)\n",
    "        domain_id = torch.randint(0, 2, (32,))\n",
    "        out = dsbn(x, domain_id)\n",
    "        print(f\"  ‚úì DSBN output shape: {out.shape}\")\n",
    "        assert out.shape == x.shape\n",
    "        print(\"  ‚úì DSBN test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå DSBN test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3: Attention\n",
    "    print(\"\\nTest 3: Visit Attention...\")\n",
    "    try:\n",
    "        attention = VisitAttention(in_dim=512, attn_dim=128)\n",
    "        F = torch.randn(4, 16, 512)\n",
    "        v, alpha = attention(F)\n",
    "        print(f\"  ‚úì Attention outputs: v={v.shape}, alpha={alpha.shape}\")\n",
    "        assert v.shape == (4, 512) and alpha.shape == (4, 16)\n",
    "        assert torch.allclose(alpha.sum(dim=1), torch.ones(4), atol=1e-6)\n",
    "        print(\"  ‚úì Attention test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Attention test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 4: Full Model\n",
    "    print(\"\\nTest 4: Complete Model...\")\n",
    "    try:\n",
    "        model = TriPlanarADNet(\n",
    "            backbone_name='vgg16_bn',\n",
    "            num_domains=2,\n",
    "            use_dsbn=True,\n",
    "            pretrained=False,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        imgs = torch.rand(2, 16, 3, 224, 224)  # [0, 1] range\n",
    "        domain_id = torch.randint(0, 2, (2,))\n",
    "        \n",
    "        logits, alpha = model(imgs, domain_id)\n",
    "        print(f\"  ‚úì Model outputs: logits={logits.shape}, alpha={alpha.shape}\")\n",
    "        assert logits.shape == (2,) and alpha.shape == (2, 16)\n",
    "        print(\"  ‚úì Model test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Model test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ All S5 tests passed!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_s5_components()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üé¨ S5 Ready for S6 Training\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nKey change:\")\n",
    "        print(\"  - Dataset now returns images in [0, 1] WITHOUT normalization\")\n",
    "        print(\"  - S6 training loop will handle normalization + augmentation\")\n",
    "        print(\"\\nNext: Run S6 training script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477653ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:08:10.230437Z",
     "iopub.status.busy": "2025-11-29T20:08:10.230154Z",
     "iopub.status.idle": "2025-11-29T20:19:00.298098Z",
     "shell.execute_reply": "2025-11-29T20:19:00.296835Z"
    },
    "papermill": {
     "duration": 650.116027,
     "end_time": "2025-11-29T20:19:00.299626",
     "exception": false,
     "start_time": "2025-11-29T20:08:10.183599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ STEP 1: 1-FOLD VALIDATION (Stable Baseline)\n",
      "======================================================================\n",
      "\n",
      "Goal: Establish stable baseline\n",
      "  Target: AUC ‚â• 0.70, BalAcc ‚â• 0.70\n",
      "  Strategy: Stage 1 only, BCE + pos_weight, AD duplication\n",
      "\n",
      "======================================================================\n",
      "üî• SNIPPET S6: STABLE BASELINE (Stage 1 Only)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Device: cuda\n",
      "  Backbone: vgg16_bn\n",
      "  Batch size: 4\n",
      "  Stage 1 epochs: 10\n",
      "  Stage 2: DISABLED (stable baseline)\n",
      "  AD duplication: 2x\n",
      "  Augmentation: True\n",
      "\n",
      "======================================================================\n",
      "üìÇ FOLD 1/1\n",
      "======================================================================\n",
      "  Train dataset: 370 visits\n",
      "    CN: 319, AD: 51\n",
      "  Val dataset: 93 visits\n",
      "    CN: 80, AD: 13\n",
      "\n",
      "  Class counts: CN=319, AD=51\n",
      "  pos_weight: 6.25\n",
      "  Effective AD samples (with 2x dup): 102\n",
      "  Train: 370 visits\n",
      "  Val:   93 visits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:02<00:00, 192MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STAGE 1: LINEAR PROBING (PRIMARY)\n",
      "======================================================================\n",
      "  Frozen: Backbone\n",
      "  Training: DSBN, Attention, Classifier\n",
      "  Loss: BCEWithLogitsLoss(pos_weight=6.25)\n",
      "  AD duplication: 2x\n",
      "  Augmentation: True\n",
      "\n",
      "Epoch   Train Loss   Val BalAcc    Val AUC  Sensitivity  Specificity  Threshold\n",
      "--------------------------------------------------------------------------------\n",
      "    1       0.5640       0.5000     0.5365       0.0000       1.0000       0.05\n",
      "    2       0.0673       0.5000     0.4644       0.0000       1.0000       0.05\n",
      "    3       0.0231       0.5000     0.5077       0.0000       1.0000       0.05\n",
      "    4       0.0148       0.5000     0.4337       0.0000       1.0000       0.05\n",
      "    5       0.0091       0.5000     0.4433       0.0000       1.0000       0.05\n",
      "    6       0.0119       0.5000     0.5125       0.0000       1.0000       0.05\n",
      "    7       0.0077       0.5000     0.4385       0.0000       1.0000       0.05\n",
      "    8       0.0080       0.5000     0.5692       0.0000       1.0000       0.05\n",
      "    9       0.0043       0.5000     0.5288       0.0000       1.0000       0.05\n",
      "\n",
      "  Early stopping at epoch 9\n",
      "\n",
      "  ‚úì Best BalAcc: 0.5000, AUC: 0.5365\n",
      "\n",
      "‚úì Fold 1 Complete:\n",
      "  Final BalAcc: 0.5000 at thr=0.05\n",
      "  Final AUC:    0.5365\n",
      "  Final Sens:   0.0000\n",
      "  Final Spec:   1.0000\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL RESULTS (VGG16_BN)\n",
      "======================================================================\n",
      "\n",
      " fold backbone  final_bal_acc  final_auc  final_sens  final_spec  final_thr  tp  tn  fp  fn\n",
      "    0 vgg16_bn            0.5   0.536538         0.0         1.0       0.05   0  80   0  13\n",
      "\n",
      "üìà Mean ¬± Std:\n",
      "  Balanced Accuracy: 0.5000 ¬± nan\n",
      "  AUC:              0.5365 ¬± nan\n",
      "  Sensitivity (AD): 0.0000 ¬± nan\n",
      "  Specificity (CN): 1.0000 ¬± nan\n",
      "  Optimal Threshold: 0.05 ¬± nan\n",
      "\n",
      "‚úì Results saved: /kaggle/working/s6_results_baseline.csv\n",
      "\n",
      "======================================================================\n",
      "BASELINE ASSESSMENT\n",
      "======================================================================\n",
      "‚ö†Ô∏è  WEAK BASELINE: AUC=0.5365, BalAcc=0.5000\n",
      "   This suggests data/ROI issues, not just model tuning\n",
      "   Consider:\n",
      "   - Different ROI extraction\n",
      "   - More slices per visit\n",
      "   - Verify S2 preprocessing quality\n",
      "\n",
      "======================================================================\n",
      "‚úÖ 1-FOLD BASELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "If successful (AUC ‚â• 0.70):\n",
      "  ‚Üí Set NUM_FOLDS=5 for full CV\n",
      "  ‚Üí Try Focal Loss as next experiment\n",
      "  ‚Üí Then add ResNet50 + DenseNet121\n",
      "\n",
      "If AUC < 0.70:\n",
      "  ‚Üí Increase AD_DUP_FACTOR to 3\n",
      "  ‚Üí Increase STAGE1_EPOCHS to 15\n",
      "  ‚Üí Check data quality\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SNIPPET S6: STABLE BASELINE (Stage 1 Only + AD Duplication)\n",
    "\n",
    "STRATEGY (Step 1 from roadmap):\n",
    "1. Stage 1 ONLY (skip Stage 2 - unstable on small dataset)\n",
    "2. BCE with pos_weight (stable)\n",
    "3. AD duplication (not sampler)\n",
    "4. Label-aware augmentation ON\n",
    "5. Threshold search for evaluation\n",
    "\n",
    "Goal: Establish stable baseline (AUC ‚â• 0.70, BalAcc ‚â• 0.70) before:\n",
    "- Trying Focal Loss\n",
    "- Adding ResNet50/DenseNet121\n",
    "- Building ensemble\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class S6Config:\n",
    "    \"\"\"Training configuration - Stable Baseline\"\"\"\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SLICES_CSV = \"/kaggle/working/slices_metadata_ROI_splits.csv\"\n",
    "    \n",
    "    # Training (Stage 1 only for now)\n",
    "    STAGE1_EPOCHS = 10\n",
    "    USE_STAGE2 = False  # Disabled for stable baseline\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 2\n",
    "    GRAD_CLIP = 5.0\n",
    "    \n",
    "    STAGE1_LR = 1e-3\n",
    "    STAGE1_WEIGHT_DECAY = 1e-4\n",
    "    STAGE1_PATIENCE = 8\n",
    "    \n",
    "    # AD duplication (replaces sampler)\n",
    "    AD_DUP_FACTOR = 2  # Duplicate each AD visit 2x in training\n",
    "    \n",
    "    # Augmentation\n",
    "    USE_AUG = True\n",
    "    AUG_ROTATION = 15\n",
    "    AUG_TRANSLATE = 0.02\n",
    "    AUG_SCALE = (0.9, 1.1)\n",
    "    AUG_BRIGHTNESS = 0.2\n",
    "    AUG_CONTRAST = 0.2\n",
    "    AUG_NOISE_SIGMA = 0.02\n",
    "    \n",
    "    BACKBONE = \"vgg16_bn\"\n",
    "    NUM_FOLDS = 5\n",
    "    \n",
    "    CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n",
    "    RESULTS_CSV = \"/kaggle/working/s6_results_baseline.csv\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "IMAGENET_NORM = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "def create_ad_augmentation(config):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=config.AUG_ROTATION),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0,\n",
    "            translate=(config.AUG_TRANSLATE, config.AUG_TRANSLATE),\n",
    "            scale=config.AUG_SCALE\n",
    "        ),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=config.AUG_BRIGHTNESS,\n",
    "            contrast=config.AUG_CONTRAST\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_cn_augmentation(config):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=config.AUG_ROTATION // 2),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "    ])\n",
    "\n",
    "\n",
    "def add_gaussian_noise(x, sigma=0.02):\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    return torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def augment_batch_label_aware(imgs, labels, config):\n",
    "    \"\"\"Label-aware augmentation: heavier for AD, lighter for CN\"\"\"\n",
    "    B, K, C, H, W = imgs.shape\n",
    "    imgs_aug = torch.zeros_like(imgs)\n",
    "    \n",
    "    ad_transform = create_ad_augmentation(config)\n",
    "    cn_transform = create_cn_augmentation(config)\n",
    "    \n",
    "    for b in range(B):\n",
    "        label = labels[b].item()\n",
    "        \n",
    "        if label == 1:  # AD\n",
    "            transform = ad_transform\n",
    "            use_noise = True\n",
    "        else:  # CN\n",
    "            transform = cn_transform\n",
    "            use_noise = False\n",
    "        \n",
    "        for k in range(K):\n",
    "            slice_img = imgs[b, k]\n",
    "            pil_img = transforms.functional.to_pil_image(slice_img)\n",
    "            pil_img = transform(pil_img)\n",
    "            slice_tensor = transforms.functional.to_tensor(pil_img)\n",
    "            \n",
    "            if use_noise:\n",
    "                slice_tensor = add_gaussian_noise(slice_tensor, sigma=config.AUG_NOISE_SIGMA)\n",
    "            \n",
    "            slice_tensor = IMAGENET_NORM(slice_tensor)\n",
    "            imgs_aug[b, k] = slice_tensor\n",
    "    \n",
    "    return imgs_aug\n",
    "\n",
    "\n",
    "def normalize_batch(imgs):\n",
    "    B, K, C, H, W = imgs.shape\n",
    "    imgs_norm = torch.zeros_like(imgs)\n",
    "    \n",
    "    for b in range(B):\n",
    "        for k in range(K):\n",
    "            imgs_norm[b, k] = IMAGENET_NORM(imgs[b, k])\n",
    "    \n",
    "    return imgs_norm\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS WITH THRESHOLD SEARCH\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(logits, labels, search_best_thr=True):\n",
    "    \"\"\"Compute metrics with optimal threshold search\"\"\"\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    y = labels.astype(int)\n",
    "    \n",
    "    if search_best_thr:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "        best_bal_acc = -1.0\n",
    "        best_thr = 0.5\n",
    "        best_stats = None\n",
    "        \n",
    "        for thr in thresholds:\n",
    "            preds = (probs >= thr).astype(int)\n",
    "            \n",
    "            TP = ((y == 1) & (preds == 1)).sum()\n",
    "            TN = ((y == 0) & (preds == 0)).sum()\n",
    "            FP = ((y == 0) & (preds == 1)).sum()\n",
    "            FN = ((y == 1) & (preds == 0)).sum()\n",
    "            \n",
    "            sens = TP / (TP + FN + 1e-8)\n",
    "            spec = TN / (TN + FP + 1e-8)\n",
    "            bal_acc = 0.5 * (sens + spec)\n",
    "            \n",
    "            if bal_acc > best_bal_acc:\n",
    "                best_bal_acc = bal_acc\n",
    "                best_thr = thr\n",
    "                best_stats = (TP, TN, FP, FN, sens, spec)\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y, probs)\n",
    "        except ValueError:\n",
    "            auc = 0.5\n",
    "        \n",
    "        TP, TN, FP, FN, sens, spec = best_stats\n",
    "        \n",
    "        return {\n",
    "            'bal_acc': float(best_bal_acc),\n",
    "            'auc': float(auc),\n",
    "            'sens': float(sens),\n",
    "            'spec': float(spec),\n",
    "            'tp': int(TP),\n",
    "            'tn': int(TN),\n",
    "            'fp': int(FP),\n",
    "            'fn': int(FN),\n",
    "            'thr': float(best_thr)\n",
    "        }\n",
    "    \n",
    "    # Fixed 0.5 threshold (for comparison)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    TP = ((y == 1) & (preds == 1)).sum()\n",
    "    TN = ((y == 0) & (preds == 0)).sum()\n",
    "    FP = ((y == 0) & (preds == 1)).sum()\n",
    "    FN = ((y == 1) & (preds == 0)).sum()\n",
    "    \n",
    "    sens = TP / (TP + FN + 1e-8)\n",
    "    spec = TN / (TN + FP + 1e-8)\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y, probs)\n",
    "    except ValueError:\n",
    "        auc = 0.5\n",
    "    \n",
    "    return {\n",
    "        'bal_acc': float(bal_acc),\n",
    "        'auc': float(auc),\n",
    "        'sens': float(sens),\n",
    "        'spec': float(spec),\n",
    "        'tp': int(TP),\n",
    "        'tn': int(TN),\n",
    "        'fp': int(FP),\n",
    "        'fn': int(FN),\n",
    "        'thr': 0.5\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING WITH AD DUPLICATION\n",
    "# ============================================================================\n",
    "\n",
    "def train_one_epoch_with_ad_dup(model, train_loader, criterion, optimizer, device, config):\n",
    "    \"\"\"\n",
    "    Training with AD duplication instead of sampler\n",
    "    \n",
    "    Each batch: if it contains AD samples, duplicate them AD_DUP_FACTOR times\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for imgs, labels, domain_ids in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        domain_ids = domain_ids.to(device)\n",
    "        \n",
    "        # üî• AD DUPLICATION: duplicate AD samples in the batch\n",
    "        if config.AD_DUP_FACTOR > 1:\n",
    "            ad_mask = (labels == 1)\n",
    "            if ad_mask.any():\n",
    "                ad_imgs = imgs[ad_mask]\n",
    "                ad_labels = labels[ad_mask]\n",
    "                ad_domain_ids = domain_ids[ad_mask]\n",
    "                \n",
    "                # Duplicate AD samples\n",
    "                for _ in range(config.AD_DUP_FACTOR - 1):\n",
    "                    imgs = torch.cat([imgs, ad_imgs], dim=0)\n",
    "                    labels = torch.cat([labels, ad_labels], dim=0)\n",
    "                    domain_ids = torch.cat([domain_ids, ad_domain_ids], dim=0)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if config.USE_AUG:\n",
    "            imgs = augment_batch_label_aware(imgs, labels, config)\n",
    "        else:\n",
    "            imgs = normalize_batch(imgs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, alpha = model(imgs, domain_ids)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        if config.GRAD_CLIP is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRAD_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device, config):\n",
    "    model.eval()\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for imgs, labels, domain_ids in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        domain_ids = domain_ids.to(device)\n",
    "        \n",
    "        imgs = normalize_batch(imgs)\n",
    "        \n",
    "        logits, alpha = model(imgs, domain_ids)\n",
    "        \n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    if len(all_logits) == 0:\n",
    "        return {'bal_acc': 0.0, 'auc': 0.0, 'sens': 0.0, 'spec': 0.0, 'thr': 0.5}\n",
    "    \n",
    "    logits_np = np.concatenate(all_logits, axis=0)\n",
    "    labels_np = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(logits_np, labels_np, search_best_thr=True)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 1 TRAINING (PRIMARY)\n",
    "# ============================================================================\n",
    "\n",
    "def freeze_module(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def unfreeze_module(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "def stage1_training(model, train_loader, val_loader, device, config, pos_weight):\n",
    "    \"\"\"\n",
    "    Stage 1: Linear probing (freeze backbone, train DSBN + attention + classifier)\n",
    "    \n",
    "    This is our PRIMARY training strategy (Stage 2 disabled for small dataset)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAGE 1: LINEAR PROBING (PRIMARY)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Frozen: Backbone\")\n",
    "    print(f\"  Training: DSBN, Attention, Classifier\")\n",
    "    print(f\"  Loss: BCEWithLogitsLoss(pos_weight={pos_weight.item():.2f})\")\n",
    "    print(f\"  AD duplication: {config.AD_DUP_FACTOR}x\")\n",
    "    print(f\"  Augmentation: {config.USE_AUG}\")\n",
    "    \n",
    "    freeze_module(model.backbone)\n",
    "    \n",
    "    if hasattr(model, 'dsbn') and model.dsbn is not None:\n",
    "        unfreeze_module(model.dsbn)\n",
    "    unfreeze_module(model.attention)\n",
    "    unfreeze_module(model.classifier)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config.STAGE1_LR,\n",
    "        weight_decay=config.STAGE1_WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    best_bal_acc = 0.0\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    print(f\"\\n{'Epoch':>5} {'Train Loss':>12} {'Val BalAcc':>12} {'Val AUC':>10} {'Sensitivity':>12} {'Specificity':>12} {'Threshold':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for epoch in range(config.STAGE1_EPOCHS):\n",
    "        train_loss = train_one_epoch_with_ad_dup(\n",
    "            model, train_loader, criterion, optimizer, device, config\n",
    "        )\n",
    "        val_metrics = evaluate(model, val_loader, device, config)\n",
    "        \n",
    "        bal_acc = val_metrics['bal_acc']\n",
    "        auc = val_metrics['auc']\n",
    "        sens = val_metrics['sens']\n",
    "        spec = val_metrics['spec']\n",
    "        thr = val_metrics.get('thr', 0.5)\n",
    "        \n",
    "        print(f\"{epoch+1:>5} {train_loss:>12.4f} {bal_acc:>12.4f} {auc:>10.4f} {sens:>12.4f} {spec:>12.4f} {thr:>10.2f}\")\n",
    "        \n",
    "        # Track best by balanced accuracy (primary metric)\n",
    "        if bal_acc > best_bal_acc + 1e-4:\n",
    "            best_bal_acc = bal_acc\n",
    "            best_auc = auc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= config.STAGE1_PATIENCE:\n",
    "                print(f\"\\n  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\n  ‚úì Best BalAcc: {best_bal_acc:.4f}, AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    return best_bal_acc, best_auc\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_s6_training_baseline(config=None):\n",
    "    \"\"\"\n",
    "    Run stable baseline training (Stage 1 only)\n",
    "    \n",
    "    This establishes baseline before:\n",
    "    - Trying Focal Loss\n",
    "    - Adding other backbones\n",
    "    - Building ensemble\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = S6Config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üî• SNIPPET S6: STABLE BASELINE (Stage 1 Only)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    print(f\"  Backbone: {config.BACKBONE}\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Stage 1 epochs: {config.STAGE1_EPOCHS}\")\n",
    "    print(f\"  Stage 2: {'ENABLED' if config.USE_STAGE2 else 'DISABLED (stable baseline)'}\")\n",
    "    print(f\"  AD duplication: {config.AD_DUP_FACTOR}x\")\n",
    "    print(f\"  Augmentation: {config.USE_AUG}\")\n",
    "    \n",
    "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold_id in range(config.NUM_FOLDS):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üìÇ FOLD {fold_id + 1}/{config.NUM_FOLDS}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        train_dataset = TriPlanarVisitDataset(\n",
    "            csv_path=config.SLICES_CSV,\n",
    "            mode='train',\n",
    "            fold_id=fold_id,\n",
    "            transform=None,\n",
    "            config=None\n",
    "        )\n",
    "        \n",
    "        val_dataset = TriPlanarVisitDataset(\n",
    "            csv_path=config.SLICES_CSV,\n",
    "            mode='val',\n",
    "            fold_id=fold_id,\n",
    "            transform=None,\n",
    "            config=None\n",
    "        )\n",
    "        \n",
    "        # Compute pos_weight\n",
    "        labels_train = np.array([v['label'] for v in train_dataset.visits])\n",
    "        class_counts = np.bincount(labels_train.astype(int))\n",
    "        cn_count = class_counts[0]\n",
    "        ad_count = class_counts[1]\n",
    "        \n",
    "        pos_weight_value = cn_count / (ad_count + 1e-8)\n",
    "        pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32, device=config.DEVICE)\n",
    "        \n",
    "        print(f\"\\n  Class counts: CN={cn_count}, AD={ad_count}\")\n",
    "        print(f\"  pos_weight: {pos_weight_value:.2f}\")\n",
    "        print(f\"  Effective AD samples (with {config.AD_DUP_FACTOR}x dup): {ad_count * config.AD_DUP_FACTOR}\")\n",
    "        \n",
    "        # Simple DataLoaders (no sampler, AD duplication happens in training loop)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train: {len(train_dataset)} visits\")\n",
    "        print(f\"  Val:   {len(val_dataset)} visits\")\n",
    "        \n",
    "        model = TriPlanarADNet(\n",
    "            backbone_name=config.BACKBONE,\n",
    "            num_domains=2,\n",
    "            use_dsbn=True,\n",
    "            pretrained=True,\n",
    "            config=None\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Stage 1 (PRIMARY)\n",
    "        best_bal_acc, best_auc = stage1_training(\n",
    "            model, train_loader, val_loader, config.DEVICE, config, pos_weight\n",
    "        )\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_metrics = evaluate(model, val_loader, config.DEVICE, config)\n",
    "        \n",
    "        print(f\"\\n‚úì Fold {fold_id + 1} Complete:\")\n",
    "        print(f\"  Final BalAcc: {final_metrics['bal_acc']:.4f} at thr={final_metrics.get('thr', 0.5):.2f}\")\n",
    "        print(f\"  Final AUC:    {final_metrics['auc']:.4f}\")\n",
    "        print(f\"  Final Sens:   {final_metrics['sens']:.4f}\")\n",
    "        print(f\"  Final Spec:   {final_metrics['spec']:.4f}\")\n",
    "        \n",
    "        checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f\"{config.BACKBONE}_fold{fold_id}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_id,\n",
    "            'backbone': config.BACKBONE,\n",
    "            'final_bal_acc': final_metrics['bal_acc'],\n",
    "            'final_auc': final_metrics['auc'],\n",
    "            'final_sens': final_metrics['sens'],\n",
    "            'final_spec': final_metrics['spec'],\n",
    "            'final_thr': final_metrics.get('thr', 0.5),\n",
    "            'tp': final_metrics['tp'],\n",
    "            'tn': final_metrics['tn'],\n",
    "            'fp': final_metrics['fp'],\n",
    "            'fn': final_metrics['fn']\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(fold_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä FINAL RESULTS ({config.BACKBONE.upper()})\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n\" + results_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìà Mean ¬± Std:\")\n",
    "    print(f\"  Balanced Accuracy: {results_df['final_bal_acc'].mean():.4f} ¬± {results_df['final_bal_acc'].std():.4f}\")\n",
    "    print(f\"  AUC:              {results_df['final_auc'].mean():.4f} ¬± {results_df['final_auc'].std():.4f}\")\n",
    "    print(f\"  Sensitivity (AD): {results_df['final_sens'].mean():.4f} ¬± {results_df['final_sens'].std():.4f}\")\n",
    "    print(f\"  Specificity (CN): {results_df['final_spec'].mean():.4f} ¬± {results_df['final_spec'].std():.4f}\")\n",
    "    print(f\"  Optimal Threshold: {results_df['final_thr'].mean():.2f} ¬± {results_df['final_thr'].std():.2f}\")\n",
    "    \n",
    "    results_df.to_csv(config.RESULTS_CSV, index=False)\n",
    "    print(f\"\\n‚úì Results saved: {config.RESULTS_CSV}\")\n",
    "    \n",
    "    # Assessment\n",
    "    mean_bal_acc = results_df['final_bal_acc'].mean()\n",
    "    mean_auc = results_df['final_auc'].mean()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE ASSESSMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if mean_auc >= 0.75 and mean_bal_acc >= 0.70:\n",
    "        print(f\"‚úÖ STRONG BASELINE: AUC={mean_auc:.4f}, BalAcc={mean_bal_acc:.4f}\")\n",
    "        print(\"   Next steps:\")\n",
    "        print(\"   1. Try Focal Loss (may improve by 2-3%)\")\n",
    "        print(\"   2. Add ResNet50 + DenseNet121\")\n",
    "        print(\"   3. Build 15-model ensemble (target: 0.80-0.85 AUC)\")\n",
    "    elif mean_auc >= 0.70 and mean_bal_acc >= 0.65:\n",
    "        print(f\"‚úÖ GOOD BASELINE: AUC={mean_auc:.4f}, BalAcc={mean_bal_acc:.4f}\")\n",
    "        print(\"   Next steps:\")\n",
    "        print(\"   1. Try increasing AD_DUP_FACTOR to 3\")\n",
    "        print(\"   2. Then proceed to multi-backbone ensemble\")\n",
    "    elif mean_auc >= 0.65:\n",
    "        print(f\"‚ö†Ô∏è  MODERATE BASELINE: AUC={mean_auc:.4f}, BalAcc={mean_bal_acc:.4f}\")\n",
    "        print(\"   Suggestions:\")\n",
    "        print(\"   1. Increase STAGE1_EPOCHS to 15\")\n",
    "        print(\"   2. Try AD_DUP_FACTOR = 3\")\n",
    "        print(\"   3. Verify ROI quality in S3\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WEAK BASELINE: AUC={mean_auc:.4f}, BalAcc={mean_bal_acc:.4f}\")\n",
    "        print(\"   This suggests data/ROI issues, not just model tuning\")\n",
    "        print(\"   Consider:\")\n",
    "        print(\"   - Different ROI extraction\")\n",
    "        print(\"   - More slices per visit\")\n",
    "        print(\"   - Verify S2 preprocessing quality\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # STEP 1: 1-FOLD VALIDATION\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ STEP 1: 1-FOLD VALIDATION (Stable Baseline)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nGoal: Establish stable baseline\")\n",
    "    print(\"  Target: AUC ‚â• 0.70, BalAcc ‚â• 0.70\")\n",
    "    print(\"  Strategy: Stage 1 only, BCE + pos_weight, AD duplication\")\n",
    "    \n",
    "    cfg = S6Config()\n",
    "    cfg.NUM_FOLDS = 1  # Start with 1 fold\n",
    "    cfg.STAGE1_EPOCHS = 10\n",
    "    cfg.AD_DUP_FACTOR = 2\n",
    "    cfg.USE_AUG = True\n",
    "    cfg.USE_STAGE2 = False\n",
    "    \n",
    "    results = run_s6_training_baseline(cfg)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ 1-FOLD BASELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nIf successful (AUC ‚â• 0.70):\")\n",
    "    print(\"  ‚Üí Set NUM_FOLDS=5 for full CV\")\n",
    "    print(\"  ‚Üí Try Focal Loss as next experiment\")\n",
    "    print(\"  ‚Üí Then add ResNet50 + DenseNet121\")\n",
    "    \n",
    "    print(\"\\nIf AUC < 0.70:\")\n",
    "    print(\"  ‚Üí Increase AD_DUP_FACTOR to 3\")\n",
    "    print(\"  ‚Üí Increase STAGE1_EPOCHS to 15\")\n",
    "    print(\"  ‚Üí Check data quality\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1980,
     "sourceId": 3398,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8690252,
     "sourceId": 13667832,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8691044,
     "sourceId": 13668930,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8746096,
     "sourceId": 13745173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8747805,
     "sourceId": 13747487,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3778.708427,
   "end_time": "2025-11-29T20:19:02.825121",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T19:16:04.116694",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
