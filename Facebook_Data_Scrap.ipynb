{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOLKEbRcLsZvNK5b+tRoqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/Facebook_Data_Scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_Xj7sBQlj3X",
        "outputId": "51153bc0-4b72-4622-efd1-0b3c057aa957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install selenium bs4 webdriver-manager pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d9aed9",
        "outputId": "b16fb8b1-f753-4284-a7ab-e656bd341f97"
      },
      "source": [
        "!pip install chromedriver-autoinstaller"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from chromedriver-autoinstaller) (25.0)\n",
            "Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: chromedriver-autoinstaller\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce67c980"
      },
      "source": [
        "import os\n",
        "\n",
        "# On Linux/macOS:\n",
        "os.environ['FB_EMAIL'] = \"chote@jbnote.com\"\n",
        "os.environ['FB_PASSWORD'] = \"Rm(bb7SBuyj6q7s\"\n",
        "\n",
        "# On Windows (PowerShell):\n",
        "# os.environ['FB_EMAIL'] = \"chote@jbnote.com\"\n",
        "# os.environ['FB_PASSWORD'] = \"Rm(bb7SBuyj6q7s\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6df06eb",
        "outputId": "2942e6e5-370f-4fcc-c01a-560776a56eed"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install google-chrome-stable"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,152 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,853 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,771 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Fetched 23.6 MB in 5s (4,959 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package google-chrome-stable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34975367",
        "outputId": "f4ba0318-518e-4d1d-f540-e1f5dc00bc2b"
      },
      "source": [
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -f # Install dependencies"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 06:03:25--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.250.141.93, 142.250.141.136, 142.250.141.190, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.250.141.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118146996 (113M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 112.67M   291MB/s    in 0.4s    \n",
            "\n",
            "2025-07-28 06:03:27 (291 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [118146996/118146996]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (138.0.7204.168-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 2s (5,359 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 126402 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (138.0.7204.168-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium webdriver-manager facebook-scraper pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Zb-TrZqwaS",
        "outputId": "f484d409-1581-49bb-db8e-0902e4ad703a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: facebook-scraper in /usr/local/lib/python3.11/dist-packages (0.2.59)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: dateparser<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from facebook-scraper) (1.2.2)\n",
            "Requirement already satisfied: demjson3<4.0.0,>=3.0.5 in /usr/local/lib/python3.11/dist-packages (from facebook-scraper) (3.0.6)\n",
            "Requirement already satisfied: requests-html<0.11.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from facebook-scraper) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook-scraper) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook-scraper) (5.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.2.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (0.0.25)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: pyee in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (11.1.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (10.4)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.67.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.13.4)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook-scraper) (5.4.0)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "scrape_ghorerbazarbd_requests.py\n",
        "\n",
        "1) Selenium logs into www.facebook.com (desktop) → harvests cookies\n",
        "2) requests.Session() uses those cookies on mbasic.facebook.com\n",
        "3) Follows “Older Posts” links to paginate\n",
        "4) Stops after 400 posts or when no more pages\n",
        "5) Saves ID, URL, timestamp, text, likes, comments, shares → CSV\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
        "\n",
        "FB_EMAIL    = \"chote@jbnote.com\"\n",
        "FB_PASSWORD = \"Rm(bb7SBuyj6q7s\"\n",
        "PAGE_HANDLE = \"Ghorerbazarbd.comm\"\n",
        "POST_LIMIT  = 400\n",
        "OUTPUT_CSV  = \"ghorerbazarbd_posts.csv\"\n",
        "\n",
        "# ─── 1) Use Selenium to log in and grab cookies ────────────────────────────────\n",
        "\n",
        "def fetch_facebook_cookies():\n",
        "    opts = Options()\n",
        "    # You can remove headless if Facebook blocks it:\n",
        "    opts.add_argument(\"--headless\")\n",
        "    opts.add_argument(\"--disable-gpu\")\n",
        "    opts.add_argument(\"--no-sandbox\")\n",
        "\n",
        "    driver = webdriver.Chrome(\n",
        "        service=Service(ChromeDriverManager().install()),\n",
        "        options=opts\n",
        "    )\n",
        "    try:\n",
        "        driver.get(\"https://www.facebook.com/login\")\n",
        "        time.sleep(2)\n",
        "        driver.find_element(By.NAME, \"email\").send_keys(FB_EMAIL)\n",
        "        driver.find_element(By.NAME, \"pass\").send_keys(FB_PASSWORD)\n",
        "        driver.find_element(By.NAME, \"login\").click()\n",
        "        time.sleep(5)  # let post‑login redirect finish\n",
        "\n",
        "        # extract cookies into a dict\n",
        "        ck = {}\n",
        "        for c in driver.get_cookies():\n",
        "            ck[c[\"name\"]] = c[\"value\"]\n",
        "\n",
        "        # sanity check\n",
        "        if \"c_user\" not in ck or \"xs\" not in ck:\n",
        "            raise RuntimeError(\"Login didn’t yield c_user/xs cookies; check credentials or 2FA.\")\n",
        "        return ck\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# ─── 2) Use requests + those cookies to scrape mbasic.facebook.com ────────────\n",
        "\n",
        "def scrape_with_requests(cookies):\n",
        "    session = requests.Session()\n",
        "    # inject selenium cookies into requests\n",
        "    jar = requests.cookies.RequestsCookieJar()\n",
        "    for k, v in cookies.items():\n",
        "        jar.set(k, v, domain=\".facebook.com\", path=\"/\")\n",
        "    session.cookies.update(jar)\n",
        "\n",
        "    url = f\"https://mbasic.facebook.com/{PAGE_HANDLE}\"\n",
        "    posts = []\n",
        "    seen = set()\n",
        "\n",
        "    while url and len(posts) < POST_LIMIT:\n",
        "        resp = session.get(url)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        # 1) Extract all posts on this page\n",
        "        for art in soup.find_all(\"div\", {\"role\": \"article\"}):\n",
        "            # get link → ID & URL\n",
        "            a = art.find(\"a\", href=True)\n",
        "            if not a:\n",
        "                continue\n",
        "            href = a[\"href\"]\n",
        "            post_url = \"https://mbasic.facebook.com\" + href\n",
        "            if \"story_fbid=\" in href:\n",
        "                pid = href.split(\"story_fbid=\")[1].split(\"&\")[0]\n",
        "            else:\n",
        "                pid = href\n",
        "\n",
        "            if pid in seen:\n",
        "                continue\n",
        "            seen.add(pid)\n",
        "\n",
        "            # text: join stripped strings, filter out “See more”\n",
        "            txt = \" \".join(\n",
        "                t for t in art.stripped_strings\n",
        "                if t.lower() not in (\"see more\",)\n",
        "            )\n",
        "\n",
        "            # timestamp\n",
        "            abbr = art.find(\"abbr\")\n",
        "            ts = abbr[\"data-utime\"] if (abbr and abbr.has_attr(\"data-utime\")) else \"\"\n",
        "\n",
        "            # counts (likes/comments/shares) in the next <div>\n",
        "            likes = comments = shares = 0\n",
        "            foot = art.find_next_sibling(\"div\")\n",
        "            if foot:\n",
        "                for link in foot.find_all(\"a\"):\n",
        "                    parts = link.get_text(strip=True).split()\n",
        "                    if len(parts) >= 2 and parts[0].isdigit():\n",
        "                        cnt = int(parts[0])\n",
        "                        if \"Like\" in parts[1]:\n",
        "                            likes = cnt\n",
        "                        elif \"Comment\" in parts[1]:\n",
        "                            comments = cnt\n",
        "                        elif \"Share\" in parts[1]:\n",
        "                            shares = cnt\n",
        "\n",
        "            posts.append({\n",
        "                \"post_id\":  pid,\n",
        "                \"post_url\": post_url,\n",
        "                \"timestamp\": ts,\n",
        "                \"text\":     txt,\n",
        "                \"likes\":    likes,\n",
        "                \"comments\": comments,\n",
        "                \"shares\":   shares\n",
        "            })\n",
        "            print(f\"[{len(posts):3d}/{POST_LIMIT}] post {pid}\")\n",
        "\n",
        "            if len(posts) >= POST_LIMIT:\n",
        "                break\n",
        "\n",
        "        if len(posts) >= POST_LIMIT:\n",
        "            break\n",
        "\n",
        "        # 2) Find “Older Posts” link to paginate\n",
        "        older = soup.find(\"a\", string=lambda s: s and \"older posts\" in s.lower())\n",
        "        if older and older.has_attr(\"href\"):\n",
        "            url = \"https://mbasic.facebook.com\" + older[\"href\"]\n",
        "            time.sleep(1)  # be kind\n",
        "        else:\n",
        "            print(\"↳ no more “Older Posts” link found.\")\n",
        "            break\n",
        "\n",
        "    return posts\n",
        "\n",
        "# ─── 3) Save to CSV ────────────────────────────────────────────────────────────\n",
        "\n",
        "def save_to_csv(posts):\n",
        "    keys = [\"post_id\",\"post_url\",\"timestamp\",\"text\",\"likes\",\"comments\",\"shares\"]\n",
        "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(posts)\n",
        "    print(f\"\\n✅ Saved {len(posts)} posts → {OUTPUT_CSV}\")\n",
        "\n",
        "# ─── MAIN ───────────────────────────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🔐 Logging into Facebook via Selenium…\")\n",
        "    ck = fetch_facebook_cookies()\n",
        "    print(\"🚀 Scraping via requests & mbasic.facebook.com…\")\n",
        "    data = scrape_with_requests(ck)\n",
        "    if data:\n",
        "        save_to_csv(data)\n",
        "    else:\n",
        "        print(\"⚠️  No posts found—check that your account can view that page.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "cdKtpmftlwnj",
        "outputId": "4e04b969-4cef-4bba-f208-1bf11ffce46b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔐 Logging into Facebook via Selenium…\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Login didn’t yield c_user/xs cookies; check credentials or 2FA.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-4150959274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔐 Logging into Facebook via Selenium…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_facebook_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Scraping via requests & mbasic.facebook.com…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_with_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-4150959274.py\u001b[0m in \u001b[0;36mfetch_facebook_cookies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"c_user\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mck\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"xs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mck\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Login didn’t yield c_user/xs cookies; check credentials or 2FA.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Login didn’t yield c_user/xs cookies; check credentials or 2FA."
          ]
        }
      ]
    }
  ]
}