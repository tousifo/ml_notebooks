{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/als_pro_act_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxEGJHt-Ya7I",
        "outputId": "dfc8673d-9a35-4518-defd-18b30588d611"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.43.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray==0.8.0 (from pennylane)\n",
            "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.43 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.43->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.10.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.43.0-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.8.0-py3-none-any.whl (934 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.8.0 diastatic-malt-2.15.2 pennylane-0.43.0 pennylane-lightning-0.43.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: INITIALIZATION AND DATA LOADING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load all relevant CSV tables\n",
        "# -------------------------------\n",
        "print(\"\\n📂 Loading PROACT datasets...\")\n",
        "alsfrs_df = pd.read_csv('PROACT_ALSFRS.csv')\n",
        "fvc_df = pd.read_csv('PROACT_FVC.csv')\n",
        "vitals_df = pd.read_csv('PROACT_VITALSIGNS.csv')\n",
        "labs_df = pd.read_csv('PROACT_LABS.csv')\n",
        "onset_df = pd.read_csv('PROACT_ALSHISTORY.csv')\n",
        "riluzole_df = pd.read_csv('PROACT_RILUZOLE.csv')\n",
        "demographics_df = pd.read_csv('PROACT_DEMOGRAPHICS.csv')\n",
        "\n",
        "print(f\"✅ ALSFRS records: {len(alsfrs_df):,}\")\n",
        "print(f\"✅ FVC records: {len(fvc_df):,}\")\n",
        "print(f\"✅ Vitals records: {len(vitals_df):,}\")\n",
        "print(f\"✅ Labs records: {len(labs_df):,}\")\n",
        "print(f\"✅ Demographics: {len(demographics_df):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Compute ALSFRS (convert ALSFRS-R to original if needed)\n",
        "# -------------------------------\n",
        "print(\"\\n🧮 Computing ALSFRS scores...\")\n",
        "\n",
        "def convert_alsfrs_row(row):\n",
        "    \"\"\"Convert ALSFRS-R to original ALSFRS if needed\"\"\"\n",
        "    if pd.notna(row.get('ALSFRS_Total')):\n",
        "        return row['ALSFRS_Total']\n",
        "    total = 0\n",
        "    for q in range(1, 10):\n",
        "        val = row.get(f'Q{q}', np.nan)\n",
        "        if pd.notna(val):\n",
        "            total += val\n",
        "    # Handle Q10 (respiratory)\n",
        "    if pd.notna(row.get('Q10_Respiratory')):\n",
        "        total += row['Q10_Respiratory']\n",
        "    elif pd.notna(row.get('R_1_Dyspnea')):\n",
        "        total += row.get('R_1_Dyspnea')\n",
        "    return total\n",
        "\n",
        "alsfrs_df['ALSFRS_Total_orig'] = alsfrs_df.apply(convert_alsfrs_row, axis=1)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Identify valid patients\n",
        "# -------------------------------\n",
        "print(\"\\n🔍 Identifying valid patients...\")\n",
        "months_start, months_end = 3, 12\n",
        "min_records_start, min_records_end = 2, 2\n",
        "days_start, days_end = months_start * 30, months_end * 30\n",
        "\n",
        "alsfrs_counts = alsfrs_df.groupby('subject_id')['ALSFRS_Delta'].agg(\n",
        "    records_before_start=lambda x: (x <= days_start).sum(),\n",
        "    records_after_end=lambda x: (x >= days_end).sum()\n",
        ")\n",
        "\n",
        "valid_patients_df = alsfrs_counts[\n",
        "    (alsfrs_counts['records_before_start'] >= min_records_start) &\n",
        "    (alsfrs_counts['records_after_end'] >= min_records_end)\n",
        "]\n",
        "valid_patients = sorted(valid_patients_df.index.tolist())\n",
        "\n",
        "print(f\"✅ Valid patients identified: {len(valid_patients):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Compute ALSFRS slope (3–12 months) - TARGET\n",
        "# -------------------------------\n",
        "print(\"\\n📈 Computing target ALSFRS slope (3-12 months)...\")\n",
        "slope_targets = {}\n",
        "\n",
        "for pid in valid_patients:\n",
        "    patient_data = alsfrs_df[alsfrs_df['subject_id'] == pid].copy()\n",
        "    patient_data.sort_values('ALSFRS_Delta', inplace=True)\n",
        "    t1 = patient_data[patient_data['ALSFRS_Delta'] > 90]\n",
        "    t2 = patient_data[patient_data['ALSFRS_Delta'] >= 365]\n",
        "\n",
        "    if len(t1) > 0 and len(t2) > 0:\n",
        "        t1_record = t1.iloc[0]\n",
        "        t2_record = t2.iloc[0]\n",
        "        delta_days = t2_record['ALSFRS_Delta'] - t1_record['ALSFRS_Delta']\n",
        "        if delta_days > 0:\n",
        "            slope = (t2_record['ALSFRS_Total_orig'] - t1_record['ALSFRS_Total_orig']) / (delta_days / 30.0)\n",
        "            slope_targets[pid] = slope\n",
        "\n",
        "target_df = pd.Series(slope_targets, name='ALSFRS_slope_3to12m')\n",
        "print(f\"✅ ALSFRS slope computed for {len(target_df):,} patients\")\n",
        "print(\"\\n📊 Target Statistics:\")\n",
        "print(target_df.describe())\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Helper functions for feature engineering\n",
        "# -------------------------------\n",
        "print(\"\\n🛠️  Setting up feature engineering functions...\")\n",
        "\n",
        "def summarize_timeseries(df, time_col, value_col):\n",
        "    \"\"\"Enhanced time-series summarization with additional statistics\"\"\"\n",
        "    grp = df.groupby('subject_id')\n",
        "    summary = pd.DataFrame({\n",
        "        'min': grp[value_col].min(),\n",
        "        'max': grp[value_col].max(),\n",
        "        'mean': grp[value_col].mean(),  # Added mean\n",
        "        'median': grp[value_col].median(),\n",
        "        'std': grp[value_col].std(),\n",
        "        'q25': grp[value_col].quantile(0.25),  # Added 25th percentile\n",
        "        'q75': grp[value_col].quantile(0.75),  # Added 75th percentile\n",
        "        'first': grp.apply(lambda g: g.sort_values(time_col)[value_col].iloc[0], include_groups=False),\n",
        "        'last': grp.apply(lambda g: g.sort_values(time_col)[value_col].iloc[-1], include_groups=False)\n",
        "    })\n",
        "\n",
        "    # Compute slope (rate of change)\n",
        "    time_first = grp[time_col].min()\n",
        "    time_last = grp[time_col].max()\n",
        "    time_diff_months = (time_last - time_first) / 30.0\n",
        "    summary['slope'] = (summary['last'] - summary['first']) / time_diff_months\n",
        "    summary.loc[time_diff_months == 0, 'slope'] = np.nan\n",
        "\n",
        "    # Add range\n",
        "    summary['range'] = summary['max'] - summary['min']\n",
        "\n",
        "    return summary\n",
        "\n",
        "def summarize_all_numeric(df, time_col):\n",
        "    \"\"\"Summarize all numeric columns in a time-series DataFrame\"\"\"\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.drop([time_col, 'subject_id'], errors='ignore')\n",
        "    summaries = {}\n",
        "    for col in numeric_cols:\n",
        "        summaries[col] = summarize_timeseries(df, time_col, col)\n",
        "        summaries[col].columns = [f'{col}_{c}' for c in summaries[col].columns]\n",
        "    return summaries\n",
        "\n",
        "print(\"✅ Feature engineering functions ready\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Extract first 90 days data\n",
        "# -------------------------------\n",
        "print(\"\\n📅 Extracting first 90 days data...\")\n",
        "\n",
        "alsfrs_3m = alsfrs_df[alsfrs_df['subject_id'].isin(valid_patients) & (alsfrs_df['ALSFRS_Delta'] <= 90)]\n",
        "fvc_df['FVC'] = fvc_df[['Subject_Liters_Trial_1','Subject_Liters_Trial_2','Subject_Liters_Trial_3']].max(axis=1)\n",
        "fvc_3m = fvc_df[fvc_df['subject_id'].isin(valid_patients) & (fvc_df['Forced_Vital_Capacity_Delta'] <= 90)]\n",
        "vitals_3m = vitals_df[vitals_df['subject_id'].isin(valid_patients) & (vitals_df['Vital_Signs_Delta'] <= 90)]\n",
        "labs_3m = labs_df[labs_df['subject_id'].isin(valid_patients) & (labs_df['Laboratory_Delta'] <= 90)]\n",
        "\n",
        "print(f\"✅ ALSFRS 3-month records: {len(alsfrs_3m):,}\")\n",
        "print(f\"✅ FVC 3-month records: {len(fvc_3m):,}\")\n",
        "print(f\"✅ Vitals 3-month records: {len(vitals_3m):,}\")\n",
        "print(f\"✅ Labs 3-month records: {len(labs_3m):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Create summarized features\n",
        "# -------------------------------\n",
        "print(\"\\n🔨 Creating summarized features from time-series data...\")\n",
        "\n",
        "alsfrs_features = summarize_all_numeric(alsfrs_3m, 'ALSFRS_Delta')\n",
        "fvc_features = summarize_all_numeric(fvc_3m, 'Forced_Vital_Capacity_Delta')\n",
        "vitals_features = summarize_all_numeric(vitals_3m, 'Vital_Signs_Delta')\n",
        "labs_features = summarize_all_numeric(labs_3m, 'Laboratory_Delta')\n",
        "\n",
        "print(f\"✅ ALSFRS features: {len(alsfrs_features)} variables\")\n",
        "print(f\"✅ FVC features: {len(fvc_features)} variables\")\n",
        "print(f\"✅ Vitals features: {len(vitals_features)} variables\")\n",
        "print(f\"✅ Labs features: {len(labs_features)} variables\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Merge all features\n",
        "# -------------------------------\n",
        "print(\"\\n🔗 Merging all features...\")\n",
        "\n",
        "features_df = pd.DataFrame(index=valid_patients)\n",
        "\n",
        "# Prepare static features\n",
        "onset_static = onset_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Site_of_Onset', 'Onset_Delta', 'Diagnosis_Delta']]\n",
        "riluzole_static = riluzole_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Subject_used_Riluzole', 'Riluzole_use_Delta']]\n",
        "demographics_static = demographics_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Age', 'Sex']]\n",
        "\n",
        "# Join static features\n",
        "features_df = features_df.join(onset_static, how='left')\n",
        "features_df = features_df.join(riluzole_static, how='left', rsuffix='_rilu')\n",
        "features_df = features_df.join(demographics_static, how='left', rsuffix='_demo')\n",
        "\n",
        "# Add dynamic (summarized) features\n",
        "for group in [alsfrs_features, fvc_features, vitals_features, labs_features]:\n",
        "    for feat_df in group.values():\n",
        "        features_df = features_df.join(feat_df, how='left')\n",
        "\n",
        "# Add slope target\n",
        "features_df = features_df.join(target_df, how='left')\n",
        "\n",
        "print(f\"✅ Features merged. Shape: {features_df.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Initial cleanup\n",
        "# -------------------------------\n",
        "print(\"\\n🧹 Initial cleanup...\")\n",
        "\n",
        "# Remove columns with all NaN values\n",
        "features_df = features_df.dropna(axis=1, how='all')\n",
        "\n",
        "# Remove columns with only one unique value\n",
        "features_df = features_df.loc[:, features_df.nunique() > 1]\n",
        "\n",
        "print(f\"✅ After cleanup. Shape: {features_df.shape}\")\n",
        "print(f\"\\n📋 Missing values per column: {features_df.isnull().sum().sum():,} total\")\n",
        "print(f\"📋 Columns with >30% missing: {(features_df.isnull().sum() / len(features_df) > 0.3).sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ STEP 1 COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n📊 Final dataset info:\")\n",
        "print(f\"  - Total patients: {len(features_df):,}\")\n",
        "print(f\"  - Total features: {features_df.shape[1] - 1}\")  # -1 for target\n",
        "print(f\"  - Target variable: ALSFRS_slope_3to12m\")\n",
        "print(f\"  - Patients with target: {features_df['ALSFRS_slope_3to12m'].notna().sum():,}\")\n",
        "print(\"\\n🔍 Preview:\")\n",
        "print(features_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgwkVE8D4wRY",
        "outputId": "49884606-52e1-478b-bc55-a1de7404d4cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: INITIALIZATION AND DATA LOADING\n",
            "============================================================\n",
            "\n",
            "📂 Loading PROACT datasets...\n",
            "✅ ALSFRS records: 73,845\n",
            "✅ FVC records: 49,110\n",
            "✅ Vitals records: 84,721\n",
            "✅ Labs records: 2,937,162\n",
            "✅ Demographics: 12,504\n",
            "\n",
            "🧮 Computing ALSFRS scores...\n",
            "\n",
            "🔍 Identifying valid patients...\n",
            "✅ Valid patients identified: 2,442\n",
            "\n",
            "📈 Computing target ALSFRS slope (3-12 months)...\n",
            "✅ ALSFRS slope computed for 2,439 patients\n",
            "\n",
            "📊 Target Statistics:\n",
            "count    2439.000000\n",
            "mean       -0.388076\n",
            "std         0.496497\n",
            "min        -3.100000\n",
            "25%        -0.638298\n",
            "50%        -0.218978\n",
            "75%         0.000000\n",
            "max         1.052632\n",
            "Name: ALSFRS_slope_3to12m, dtype: float64\n",
            "\n",
            "🛠️  Setting up feature engineering functions...\n",
            "✅ Feature engineering functions ready\n",
            "\n",
            "📅 Extracting first 90 days data...\n",
            "✅ ALSFRS 3-month records: 8,210\n",
            "✅ FVC 3-month records: 5,880\n",
            "✅ Vitals 3-month records: 9,625\n",
            "✅ Labs 3-month records: 403,408\n",
            "\n",
            "🔨 Creating summarized features from time-series data...\n",
            "✅ ALSFRS features: 17 variables\n",
            "✅ FVC features: 8 variables\n",
            "✅ Vitals features: 27 variables\n",
            "✅ Labs features: 0 variables\n",
            "\n",
            "🔗 Merging all features...\n",
            "✅ Features merged. Shape: (2442, 580)\n",
            "\n",
            "🧹 Initial cleanup...\n",
            "✅ After cleanup. Shape: (2442, 518)\n",
            "\n",
            "📋 Missing values per column: 691,712 total\n",
            "📋 Columns with >30% missing: 318\n",
            "\n",
            "============================================================\n",
            "✅ STEP 1 COMPLETED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "📊 Final dataset info:\n",
            "  - Total patients: 2,442\n",
            "  - Total features: 517\n",
            "  - Target variable: ALSFRS_slope_3to12m\n",
            "  - Patients with target: 2,439\n",
            "\n",
            "🔍 Preview:\n",
            "      Site_of_Onset  Onset_Delta  Diagnosis_Delta Subject_used_Riluzole  \\\n",
            "121     Onset: Limb          NaN              NaN                   Yes   \n",
            "1009   Onset: Other       -324.0            -63.0                   Yes   \n",
            "1036  Onset: Bulbar          NaN              NaN                   NaN   \n",
            "\n",
            "      Riluzole_use_Delta   Age     Sex  Q1_Speech_min  Q1_Speech_max  \\\n",
            "121                  0.0  52.0  Female            4.0            4.0   \n",
            "1009                 0.0  51.0    Male            4.0            4.0   \n",
            "1036                 NaN  67.0  Female            3.0            3.0   \n",
            "\n",
            "      Q1_Speech_mean  ...  Standing_BP_Diastolic_last  \\\n",
            "121              4.0  ...                         NaN   \n",
            "1009             4.0  ...                         NaN   \n",
            "1036             3.0  ...                         NaN   \n",
            "\n",
            "      Standing_BP_Systolic_min  Standing_BP_Systolic_max  \\\n",
            "121                        NaN                       NaN   \n",
            "1009                       NaN                       NaN   \n",
            "1036                       NaN                       NaN   \n",
            "\n",
            "      Standing_BP_Systolic_mean  Standing_BP_Systolic_median  \\\n",
            "121                         NaN                          NaN   \n",
            "1009                        NaN                          NaN   \n",
            "1036                        NaN                          NaN   \n",
            "\n",
            "      Standing_BP_Systolic_q25  Standing_BP_Systolic_q75  \\\n",
            "121                        NaN                       NaN   \n",
            "1009                       NaN                       NaN   \n",
            "1036                       NaN                       NaN   \n",
            "\n",
            "      Standing_BP_Systolic_first  Standing_BP_Systolic_last  \\\n",
            "121                          NaN                        NaN   \n",
            "1009                         NaN                        NaN   \n",
            "1036                         NaN                        NaN   \n",
            "\n",
            "      ALSFRS_slope_3to12m  \n",
            "121             -1.058824  \n",
            "1009             0.000000  \n",
            "1036                  NaN  \n",
            "\n",
            "[3 rows x 518 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "8FvNDrtz7VbD",
        "outputId": "5d95094f-ec8e-4988-b8f7-b5d9821b39d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Site_of_Onset  Onset_Delta  Diagnosis_Delta Subject_used_Riluzole  \\\n",
              "121     Onset: Limb          NaN              NaN                   Yes   \n",
              "1009   Onset: Other       -324.0            -63.0                   Yes   \n",
              "1036  Onset: Bulbar          NaN              NaN                   NaN   \n",
              "\n",
              "      Riluzole_use_Delta   Age     Sex  Q1_Speech_min  Q1_Speech_max  \\\n",
              "121                  0.0  52.0  Female            4.0            4.0   \n",
              "1009                 0.0  51.0    Male            4.0            4.0   \n",
              "1036                 NaN  67.0  Female            3.0            3.0   \n",
              "\n",
              "      Q1_Speech_mean  ...  Standing_BP_Diastolic_last  \\\n",
              "121              4.0  ...                         NaN   \n",
              "1009             4.0  ...                         NaN   \n",
              "1036             3.0  ...                         NaN   \n",
              "\n",
              "      Standing_BP_Systolic_min  Standing_BP_Systolic_max  \\\n",
              "121                        NaN                       NaN   \n",
              "1009                       NaN                       NaN   \n",
              "1036                       NaN                       NaN   \n",
              "\n",
              "      Standing_BP_Systolic_mean  Standing_BP_Systolic_median  \\\n",
              "121                         NaN                          NaN   \n",
              "1009                        NaN                          NaN   \n",
              "1036                        NaN                          NaN   \n",
              "\n",
              "      Standing_BP_Systolic_q25  Standing_BP_Systolic_q75  \\\n",
              "121                        NaN                       NaN   \n",
              "1009                       NaN                       NaN   \n",
              "1036                       NaN                       NaN   \n",
              "\n",
              "      Standing_BP_Systolic_first  Standing_BP_Systolic_last  \\\n",
              "121                          NaN                        NaN   \n",
              "1009                         NaN                        NaN   \n",
              "1036                         NaN                        NaN   \n",
              "\n",
              "      ALSFRS_slope_3to12m  \n",
              "121             -1.058824  \n",
              "1009             0.000000  \n",
              "1036                  NaN  \n",
              "\n",
              "[3 rows x 518 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3487644-bc63-49eb-8371-8f38faface18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site_of_Onset</th>\n",
              "      <th>Onset_Delta</th>\n",
              "      <th>Diagnosis_Delta</th>\n",
              "      <th>Subject_used_Riluzole</th>\n",
              "      <th>Riluzole_use_Delta</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Q1_Speech_min</th>\n",
              "      <th>Q1_Speech_max</th>\n",
              "      <th>Q1_Speech_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>Standing_BP_Diastolic_last</th>\n",
              "      <th>Standing_BP_Systolic_min</th>\n",
              "      <th>Standing_BP_Systolic_max</th>\n",
              "      <th>Standing_BP_Systolic_mean</th>\n",
              "      <th>Standing_BP_Systolic_median</th>\n",
              "      <th>Standing_BP_Systolic_q25</th>\n",
              "      <th>Standing_BP_Systolic_q75</th>\n",
              "      <th>Standing_BP_Systolic_first</th>\n",
              "      <th>Standing_BP_Systolic_last</th>\n",
              "      <th>ALSFRS_slope_3to12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>Onset: Limb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>Onset: Other</td>\n",
              "      <td>-324.0</td>\n",
              "      <td>-63.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>Onset: Bulbar</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>67.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 518 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3487644-bc63-49eb-8371-8f38faface18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3487644-bc63-49eb-8371-8f38faface18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3487644-bc63-49eb-8371-8f38faface18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a4c66e1b-e3a3-48b4-9c3d-1c7d7b065fb0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4c66e1b-e3a3-48b4-9c3d-1c7d7b065fb0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a4c66e1b-e3a3-48b4-9c3d-1c7d7b065fb0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "features_df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# leakage free preprocessing"
      ],
      "metadata": {
        "id": "Un9iRPKUoDPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BUILD y (3→12m ALSFRS slope per ~30d), aligned to features_df ====\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# --- locate long ALSFRS table ---\n",
        "def _first_existing(names):\n",
        "    g = globals()\n",
        "    for n in names:\n",
        "        if n in g and isinstance(g[n], pd.DataFrame):\n",
        "            return g[n], n\n",
        "    return None, None\n",
        "\n",
        "als_long, _als_name = _first_existing([\n",
        "    \"alsfrs_long\", \"alsfrs_all\", \"alsfrs_full\", \"alsfrs\", \"als_long\", \"alsfrs_df\"\n",
        "])\n",
        "if als_long is None:\n",
        "    raise RuntimeError(\"Long ALSFRS DataFrame not found. Load it (e.g., alsfrs_long).\")\n",
        "\n",
        "# --- detect columns (subject, time, score) ---\n",
        "def _pick_col(df, cands):\n",
        "    for c in cands:\n",
        "        if c in df.columns: return c\n",
        "    lc = {c.lower(): c for c in df.columns}\n",
        "    for c in cands:\n",
        "        if c.lower() in lc: return lc[c.lower()]\n",
        "    return None\n",
        "\n",
        "SUBJ  = _pick_col(als_long, [\"subject_id\",\"Subject_ID\",\"RID\",\"patient_id\"])\n",
        "TIME  = _pick_col(als_long, [\"ALSFRS_Delta\",\"days\",\"Days\",\"days_since_first\",\"days_since_baseline\"])\n",
        "SCORE = _pick_col(als_long, [\"ALSFRS_Total_orig\",\"ALSFRS_Total\",\"ALSFRS\",\"ALSFRS_R\",\"ALSFRS_R_Total\"])\n",
        "if any(v is None for v in [SUBJ,TIME,SCORE]):\n",
        "    raise RuntimeError(f\"Could not auto-detect columns: SUBJ={SUBJ}, TIME={TIME}, SCORE={SCORE}\")\n",
        "\n",
        "# --- slope per subject using points in (90, 365] days; per-30-days units ---\n",
        "def _slope_per30d(g: pd.DataFrame, t_col: str, y_col: str) -> float | None:\n",
        "    g = g[[t_col, y_col]].dropna()\n",
        "    g = g[(g[t_col] > 90) & (g[t_col] <= 365)]\n",
        "    if len(g) < 2:\n",
        "        return None\n",
        "    t = g[t_col].to_numpy(dtype=float)\n",
        "    y = g[y_col].to_numpy(dtype=float)\n",
        "    a, b = np.polyfit(t, y, deg=1)   # points/day\n",
        "    return float(a * 30.0)           # ≈ per month\n",
        "\n",
        "slopes = (\n",
        "    als_long\n",
        "    .groupby(SUBJ, group_keys=False)\n",
        "    .apply(lambda df: _slope_per30d(df, TIME, SCORE))\n",
        "    .rename(\"slope_3to12m\")\n",
        ")\n",
        "\n",
        "# --- align to features_df index; drop subjects without slope ---\n",
        "if \"features_df\" not in globals():\n",
        "    raise RuntimeError(\"features_df not found. Build your subject-level feature table first.\")\n",
        "\n",
        "# FIXED: Remove target column from features_df if it exists\n",
        "if 'ALSFRS_slope_3to12m' in features_df.columns:\n",
        "    features_df = features_df.drop(columns=['ALSFRS_slope_3to12m'])\n",
        "\n",
        "y = slopes.reindex(features_df.index)\n",
        "mask = y.notna()\n",
        "features_df = features_df.loc[mask].copy()\n",
        "y = y.loc[mask].copy()\n",
        "\n",
        "print(\"Built target `y`.\")\n",
        "print(\"features_df:\", features_df.shape, \"| y:\", y.shape, \"| mean:\", round(y.mean(),3), \"std:\", round(y.std(),3))\n",
        "print(f\"✓ Confirmed: Target column removed from features_df\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp-LYf_pqT0H",
        "outputId": "c3001fab-d0f9-4d91-941e-17a3b9f5d6bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built target `y`.\n",
            "features_df: (2424, 517) | y: (2424,) | mean: -0.383 std: 0.522\n",
            "✓ Confirmed: Target column removed from features_df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CLEAN TRAIN/TEST SPLIT (no transforms; no leakage) ====\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# FIXED: Ensure target is not in features\n",
        "if 'ALSFRS_slope_3to12m' in features_df.columns:\n",
        "    features_df = features_df.drop(columns=['ALSFRS_slope_3to12m'])\n",
        "\n",
        "assert all(features_df.index == y.index), \"Index mismatch between features and target!\"\n",
        "assert 'ALSFRS_slope_3to12m' not in features_df.columns, \"Target column still in features!\"\n",
        "\n",
        "def stratify_bins(y_series, n_bins=10):\n",
        "    q = pd.qcut(y_series, q=np.minimum(n_bins, max(2, y_series.nunique())), duplicates='drop')\n",
        "    return pd.factorize(q, sort=True)[0]\n",
        "\n",
        "bins = stratify_bins(y, n_bins=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_df, y, test_size=0.2, random_state=42, stratify=bins\n",
        ")\n",
        "X_train = X_train.copy(); X_test = X_test.copy()\n",
        "y_train = y_train.copy(); y_test = y_test.copy()\n",
        "\n",
        "print(\"✓ Train/test split complete\")\n",
        "print(f\"  X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "print(f\"  ✓ No target leakage - verified!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZA14tz0oKHu",
        "outputId": "d932b93c-513b-46b1-e5c0-987aff9e97cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Train/test split complete\n",
            "  X_train: (1939, 517), X_test: (485, 517)\n",
            "  y_train: (1939,), y_test: (485,)\n",
            "  ✓ No target leakage - verified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BULLETPROOF PREPROCESSOR (categorical-safe, no leakage) ====\n",
        "import re\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, top_k=25, max_missing=0.5, use_pca=False, pca_components=12,\n",
        "                 max_cats_per_col=8, numeric_threshold=1.0, force_cat=None):\n",
        "        \"\"\"\n",
        "        numeric_threshold=1.0  -> only columns 100% numeric-convertible are treated as numeric.\n",
        "        force_cat: list[str] of columns to always treat as categorical (optional).\n",
        "        \"\"\"\n",
        "        self.top_k = top_k\n",
        "        self.max_missing = max_missing\n",
        "        self.use_pca = use_pca\n",
        "        self.pca_components = pca_components\n",
        "        self.max_cats_per_col = max_cats_per_col\n",
        "        self.numeric_threshold = numeric_threshold\n",
        "        self.force_cat = set(force_cat or [])\n",
        "\n",
        "        # learned\n",
        "        self.num_cols_, self.cat_cols_ = [], []\n",
        "        self.cat_maps_ = {}      # col -> kept categories\n",
        "        self.keep_cols_ = []\n",
        "        self.scaler_ = None\n",
        "        self.pca_ = None\n",
        "        self.num_medians_ = None\n",
        "\n",
        "    # ----- helpers -----\n",
        "    @staticmethod\n",
        "    def _has_letters(sample_values) -> bool:\n",
        "        # Detect alpha characters in sample of values (flags columns like \"Onset: Limb\")\n",
        "        for v in sample_values:\n",
        "            if pd.isna(v):\n",
        "                continue\n",
        "            s = str(v)\n",
        "            if re.search(r\"[A-Za-z]\", s):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _split_num_cat(self, X: pd.DataFrame):\n",
        "        num_cols, cat_cols = [], []\n",
        "        for c in X.columns:\n",
        "            if c in self.force_cat:\n",
        "                cat_cols.append(c); continue\n",
        "            s = X[c]\n",
        "            # quick letter check on 100 non-null samples\n",
        "            nonnull = s.dropna()\n",
        "            sample = nonnull.sample(min(100, len(nonnull)), random_state=42) if len(nonnull) else nonnull\n",
        "            if self._has_letters(sample.values):\n",
        "                cat_cols.append(c); continue\n",
        "            # numeric convertibility\n",
        "            s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            frac_numeric = s_num.notna().mean()\n",
        "            if frac_numeric >= self.numeric_threshold:\n",
        "                num_cols.append(c)\n",
        "            else:\n",
        "                cat_cols.append(c)\n",
        "        return num_cols, cat_cols\n",
        "\n",
        "    def _encode_cats_fit(self, X_cat: pd.DataFrame) -> pd.DataFrame:\n",
        "        oh = []\n",
        "        for c in X_cat.columns:\n",
        "            s = X_cat[c].astype(\"object\")\n",
        "            s = s.astype(str).where(~s.isna(), \"MISSING\")\n",
        "            vc = s.value_counts(dropna=False)\n",
        "            keep = vc.index.tolist()[: max(1, self.max_cats_per_col - 1)]\n",
        "            if \"MISSING\" in s.values and \"MISSING\" not in keep:\n",
        "                if len(keep) >= self.max_cats_per_col:\n",
        "                    keep = keep[:-1] + [\"MISSING\"]\n",
        "                else:\n",
        "                    keep = keep + [\"MISSING\"]\n",
        "            keep = list(dict.fromkeys(keep))\n",
        "            self.cat_maps_[c] = keep\n",
        "            for k in keep:\n",
        "                col = f\"{c}__{k}\"\n",
        "                oh.append(pd.Series((s == k).astype(np.float32), index=s.index, name=col))\n",
        "            # OTHER bucket\n",
        "            other = ~s.isin(keep)\n",
        "            oh.append(pd.Series(other.astype(np.float32), index=s.index, name=f\"{c}__OTHER\"))\n",
        "        return pd.concat(oh, axis=1) if len(oh) else pd.DataFrame(index=X_cat.index)\n",
        "\n",
        "    def _encode_cats_apply(self, X_cat: pd.DataFrame) -> pd.DataFrame:\n",
        "        oh = []\n",
        "        for c in self.cat_cols_:\n",
        "            s = X_cat[c] if c in X_cat.columns else pd.Series(index=X_cat.index, dtype=\"object\")\n",
        "            s = s.astype(str).where(~s.isna(), \"MISSING\")\n",
        "            keep = self.cat_maps_.get(c, [])\n",
        "            for k in keep:\n",
        "                col = f\"{c}__{k}\"\n",
        "                oh.append(pd.Series((s == k).astype(np.float32), index=s.index, name=col))\n",
        "            other = ~s.isin(keep)\n",
        "            oh.append(pd.Series(other.astype(np.float32), index=s.index, name=f\"{c}__OTHER\"))\n",
        "        return pd.concat(oh, axis=1) if len(oh) else pd.DataFrame(index=X_cat.index)\n",
        "\n",
        "    def _feature_scores(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
        "        # Ensure y is pure numpy\n",
        "        y_np = np.array(y.values if hasattr(y, 'values') else y, dtype=np.float64)\n",
        "\n",
        "        # RF importance\n",
        "        rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X.values, y_np)  # Use .values to ensure numpy array\n",
        "        s_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "        # Mutual information\n",
        "        mi = mutual_info_regression(X.values, y_np, random_state=42)\n",
        "        s_mi = pd.Series(mi, index=X.columns)\n",
        "\n",
        "        # |Pearson r|\n",
        "        def safe_corr(col):\n",
        "            v = col.values\n",
        "            if np.std(v) == 0: return 0.0\n",
        "            return float(abs(np.corrcoef(v, y_np)[0,1]))\n",
        "        s_pr = X.apply(safe_corr)\n",
        "\n",
        "        # Blend (normalized)\n",
        "        def nz_norm(s):\n",
        "            s = s.fillna(0.0); m = s.max()\n",
        "            return s / m if m > 0 else s\n",
        "        blended = 0.5*nz_norm(s_rf) + 0.3*nz_norm(s_mi) + 0.2*nz_norm(s_pr)\n",
        "        return blended.sort_values(ascending=False)\n",
        "\n",
        "    # ----- API -----\n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        # split by robust content check (no strings into numeric)\n",
        "        self.num_cols_, self.cat_cols_ = self._split_num_cat(X_train)\n",
        "\n",
        "        # numeric missingness filter on TRAIN only\n",
        "        num_keep = []\n",
        "        if self.num_cols_:\n",
        "            coerced = X_train[self.num_cols_].apply(pd.to_numeric, errors=\"coerce\")\n",
        "            miss = coerced.isna().mean()\n",
        "            num_keep = miss[miss <= self.max_missing].index.tolist()\n",
        "\n",
        "        cat_keep = self.cat_cols_\n",
        "        X_tr = X_train[num_keep + cat_keep].copy()\n",
        "\n",
        "        # numeric block (hard coerce to float32) + train medians\n",
        "        if num_keep:\n",
        "            X_tr_num = X_tr[num_keep].apply(pd.to_numeric, errors=\"coerce\").astype(np.float32)\n",
        "            self.num_medians_ = X_tr_num.median()\n",
        "            X_tr_num = X_tr_num.fillna(self.num_medians_)\n",
        "        else:\n",
        "            X_tr_num = pd.DataFrame(index=X_tr.index, dtype=np.float32)\n",
        "            self.num_medians_ = pd.Series(dtype=np.float32)\n",
        "\n",
        "        # categorical block → one-hot (fit)\n",
        "        X_tr_cat = X_tr[cat_keep] if cat_keep else pd.DataFrame(index=X_tr.index)\n",
        "        X_tr_cat_oh = self._encode_cats_fit(X_tr_cat)\n",
        "\n",
        "        # combine\n",
        "        X_tr_full = pd.concat([X_tr_num, X_tr_cat_oh], axis=1)\n",
        "\n",
        "        # feature scoring/selection on TRAIN only\n",
        "        scores = self._feature_scores(X_tr_full, y_train)\n",
        "        self.keep_cols_ = scores.head(self.top_k).index.tolist()\n",
        "\n",
        "        # scale fit on TRAIN selected\n",
        "        self.scaler_ = RobustScaler()\n",
        "        X_sel = X_tr_full[self.keep_cols_].values  # Use .values for pure numpy\n",
        "        X_scl = self.scaler_.fit_transform(X_sel)\n",
        "\n",
        "        # optional PCA\n",
        "        if self.use_pca:\n",
        "            n_comp = min(self.pca_components, X_scl.shape[1])\n",
        "            self.pca_ = PCA(n_components=n_comp, random_state=42)\n",
        "            self.pca_.fit(X_scl)\n",
        "        else:\n",
        "            self.pca_ = None\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        # numeric (coerce to float32, fill with TRAIN medians)\n",
        "        if len(self.num_cols_):\n",
        "            cols_num = [c for c in self.num_cols_ if c in X.columns]\n",
        "            X_num = X[cols_num].apply(pd.to_numeric, errors=\"coerce\").astype(np.float32)\n",
        "            # make sure all expected numeric cols exist\n",
        "            for c in self.num_medians_.index:\n",
        "                if c not in X_num.columns:\n",
        "                    X_num[c] = np.nan\n",
        "            X_num = X_num[self.num_medians_.index]\n",
        "            X_num = X_num.fillna(self.num_medians_)\n",
        "        else:\n",
        "            X_num = pd.DataFrame(index=X.index, dtype=np.float32)\n",
        "\n",
        "        # categorical\n",
        "        cols_cat = [c for c in self.cat_cols_ if c in X.columns]\n",
        "        X_cat = X[cols_cat] if cols_cat else pd.DataFrame(index=X.index)\n",
        "        X_cat_oh = self._encode_cats_apply(X_cat)\n",
        "\n",
        "        # combine & align to kept features\n",
        "        X_full = pd.concat([X_num, X_cat_oh], axis=1)\n",
        "        for c in self.keep_cols_:\n",
        "            if c not in X_full.columns:\n",
        "                X_full[c] = 0.0\n",
        "        X_full = X_full[self.keep_cols_]\n",
        "\n",
        "        # CRITICAL: Convert to pure numpy BEFORE scaling\n",
        "        X_np = X_full.values.astype(np.float32)\n",
        "        X_scl = self.scaler_.transform(X_np)\n",
        "\n",
        "        if self.pca_ is not None:\n",
        "            X_scl = self.pca_.transform(X_scl)\n",
        "\n",
        "        # CRITICAL: Return pure numpy array with explicit copy\n",
        "        return np.array(X_scl, dtype=np.float32, copy=True)\n"
      ],
      "metadata": {
        "id": "DQgkM5tGoRPg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LEAK-FREE OPTIMIZED QNN (Following Best Practices) ====\n",
        "\n",
        "import math, numpy as np, pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pennylane as qml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ============= METRICS =============\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    y_true = np.array(y_true, dtype=np.float64).reshape(-1)\n",
        "    y_pred = np.array(y_pred, dtype=np.float64).reshape(-1)\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    mae = float(mean_absolute_error(y_true, y_pred))\n",
        "    r2 = float(r2_score(y_true, y_pred))\n",
        "    pcc = float(pearsonr(y_true, y_pred)[0]) if (np.std(y_true)>0 and np.std(y_pred)>0) else 0.0\n",
        "    return rmse, mae, r2, pcc\n",
        "\n",
        "# ============= DATASET =============\n",
        "class TabularDS(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        if isinstance(y, pd.Series):\n",
        "            y = y.values\n",
        "        self.X = torch.from_numpy(np.array(X, dtype=np.float32, copy=True))\n",
        "        self.y = torch.from_numpy(np.array(y, dtype=np.float32, copy=True).reshape(-1, 1))\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "# ============= QNN MODEL (OPTIMIZED) =============\n",
        "class OptimizedQNN(nn.Module):\n",
        "    def __init__(self, input_dim, n_wires=8, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_wires = n_wires\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        dev = qml.device(\"default.qubit\", wires=n_wires)\n",
        "\n",
        "        @qml.qnode(dev, interface=\"torch\")\n",
        "        def qnode(inputs, weights):\n",
        "            # Use first n_wires features\n",
        "            x = inputs[..., :n_wires]\n",
        "\n",
        "            # Data re-uploading (proven effective)\n",
        "            for layer in range(n_layers):\n",
        "                # AngleEmbedding\n",
        "                for i in range(n_wires):\n",
        "                    qml.RY(x[..., i] * np.pi, wires=i)\n",
        "\n",
        "                # Variational layer\n",
        "                for i in range(n_wires):\n",
        "                    qml.RX(weights[layer, i, 0], wires=i)\n",
        "                    qml.RY(weights[layer, i, 1], wires=i)\n",
        "                    qml.RZ(weights[layer, i, 2], wires=i)\n",
        "\n",
        "                # Entanglement\n",
        "                for i in range(n_wires - 1):\n",
        "                    qml.CNOT(wires=[i, i + 1])\n",
        "                qml.CNOT(wires=[n_wires - 1, 0])\n",
        "\n",
        "            # Return list (no stacking)\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(n_wires)]\n",
        "\n",
        "        weight_shapes = {\"weights\": (n_layers, n_wires, 3)}\n",
        "        self.q_layer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
        "\n",
        "        # Classical head with BatchNorm (improves stability)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(n_wires + input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        q_out = self.q_layer(x)\n",
        "\n",
        "        # Handle output\n",
        "        if isinstance(q_out, list):\n",
        "            q_out = torch.stack(q_out, dim=-1)\n",
        "        if len(q_out.shape) == 1:\n",
        "            q_out = q_out.unsqueeze(0)\n",
        "\n",
        "        # Combine quantum + classical\n",
        "        combined = torch.cat([q_out, x], dim=1)\n",
        "        return self.head(combined)\n",
        "\n",
        "# ============= EVALUATION FUNCTION =============\n",
        "def evaluate(model, loader, device):\n",
        "    \"\"\"Evaluate model on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yp = model(xb).cpu().numpy().reshape(-1)\n",
        "            yt = yb.numpy().reshape(-1)\n",
        "            preds.extend(yp)\n",
        "            trues.extend(yt)\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "    rmse, mae, r2, pcc = compute_metrics(trues, preds)\n",
        "    return rmse, mae, r2, pcc, preds, trues\n",
        "\n",
        "# ============= COSINE WARMUP LR =============\n",
        "def get_lr(epoch, config):\n",
        "    if epoch < config['warmup_epochs']:\n",
        "        return config['lr_start'] + (config['lr_max'] - config['lr_start']) * (epoch / config['warmup_epochs'])\n",
        "    progress = (epoch - config['warmup_epochs']) / max(1, config['epochs'] - config['warmup_epochs'])\n",
        "    return config['lr_max'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# ============= MAIN TRAINING FUNCTION =============\n",
        "def train_leak_free_qnn(X_train_df, X_test_df, y_train_s, y_test_s, config=None):\n",
        "    \"\"\"\n",
        "    Leak-free QNN training with:\n",
        "    - Train/Val split from X_train only\n",
        "    - Val-based early stopping\n",
        "    - Test touched only once at end\n",
        "    - Val-fitted calibration\n",
        "    \"\"\"\n",
        "\n",
        "    config = config or {\n",
        "        'epochs': 300,\n",
        "        'patience': 30,\n",
        "        'batch_size': 32,\n",
        "        'lr_start': 1e-5,\n",
        "        'lr_max': 1e-3,\n",
        "        'warmup_epochs': 20,\n",
        "        'n_wires': 8,\n",
        "        'n_layers': 2,\n",
        "        'weight_decay': 1e-4,\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"🔒 LEAK-FREE QNN TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ============= SPLIT: Train → Train/Val (NO TEST TOUCHING) =============\n",
        "    tr_idx, val_idx = train_test_split(\n",
        "        np.arange(len(X_train_df)),\n",
        "        test_size=0.2,\n",
        "        random_state=config['random_state']\n",
        "    )\n",
        "\n",
        "    X_tr = X_train_df.iloc[tr_idx]\n",
        "    X_val = X_train_df.iloc[val_idx]\n",
        "    y_tr = y_train_s.iloc[tr_idx]\n",
        "    y_val = y_train_s.iloc[val_idx]\n",
        "\n",
        "    print(f\"\\n📊 Data split:\")\n",
        "    print(f\"  Train: {len(X_tr)} samples\")\n",
        "    print(f\"  Val:   {len(X_val)} samples\")\n",
        "    print(f\"  Test:  {len(X_test_df)} samples (FROZEN)\")\n",
        "\n",
        "    # ============= PREPROCESSOR: Fit ONLY on Train =============\n",
        "    print(\"\\n🔧 Fitting preprocessor on TRAIN only...\")\n",
        "    prep = Preprocessor(\n",
        "        top_k=50,\n",
        "        max_missing=0.5,\n",
        "        use_pca=False,\n",
        "        numeric_threshold=1.0,\n",
        "        force_cat=[]\n",
        "    ).fit(X_tr, y_tr)\n",
        "\n",
        "    # Transform all splits\n",
        "    X_tr_np = prep.transform(X_tr)\n",
        "    X_val_np = prep.transform(X_val)\n",
        "    X_te_np = prep.transform(X_test_df)\n",
        "\n",
        "    y_tr_np = np.array(y_tr.values, dtype=np.float32)\n",
        "    y_val_np = np.array(y_val.values, dtype=np.float32)\n",
        "    y_te_np = np.array(y_test_s.values, dtype=np.float32)\n",
        "\n",
        "    print(f\"✓ Transformed: train={X_tr_np.shape}, val={X_val_np.shape}, test={X_te_np.shape}\")\n",
        "\n",
        "    # ============= DATA LOADERS =============\n",
        "    train_loader = DataLoader(TabularDS(X_tr_np, y_tr_np), batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(TabularDS(X_val_np, y_val_np), batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(TabularDS(X_te_np, y_te_np), batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    # ============= MODEL & OPTIMIZER =============\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"✓ Device: {device}\")\n",
        "\n",
        "    print(f\"\\n🧠 Building QNN: n_wires={config['n_wires']}, n_layers={config['n_layers']}\")\n",
        "    model = OptimizedQNN(\n",
        "        input_dim=X_tr_np.shape[1],\n",
        "        n_wires=config['n_wires'],\n",
        "        n_layers=config['n_layers']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['lr_start'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Huber Loss (SmoothL1Loss with beta=0.5)\n",
        "    criterion = nn.SmoothL1Loss(beta=0.5)\n",
        "\n",
        "    # ============= TRAINING LOOP (VAL-MONITORED) =============\n",
        "    best_val_rmse = float('inf')\n",
        "    best_state = None\n",
        "    best_val_preds = None\n",
        "    best_val_trues = None\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_rmse': [], 'val_pcc': []}\n",
        "\n",
        "    print(\"\\n🚀 Training (monitoring VAL only)...\\n\")\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Update learning rate\n",
        "        lr = get_lr(epoch, config)\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = lr\n",
        "\n",
        "        # ===== TRAIN =====\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.detach().item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # ===== VALIDATE =====\n",
        "        val_rmse, val_mae, val_r2, val_pcc, val_preds, val_trues = evaluate(model, val_loader, device)\n",
        "        history['val_rmse'].append(val_rmse)\n",
        "        history['val_pcc'].append(val_pcc)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0 or epoch < 5:\n",
        "            print(f\"Epoch {epoch+1:3d}/{config['epochs']} | \"\n",
        "                  f\"Loss {avg_train_loss:.4f} | \"\n",
        "                  f\"Val RMSE {val_rmse:.4f} | \"\n",
        "                  f\"Val PCC {val_pcc:.4f} | \"\n",
        "                  f\"LR {lr:.6f}\")\n",
        "\n",
        "        # ===== EARLY STOPPING (based on VAL) =====\n",
        "        if val_rmse < best_val_rmse - 1e-5:\n",
        "            best_val_rmse = val_rmse\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            best_val_preds = val_preds.copy()\n",
        "            best_val_trues = val_trues.copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config['patience']:\n",
        "                print(f\"\\n⏹️  Early stopping at epoch {epoch+1} (no val improvement for {config['patience']} epochs)\")\n",
        "                break\n",
        "\n",
        "    # ============= LOAD BEST MODEL =============\n",
        "    print(f\"\\n✓ Loading best model (Val RMSE: {best_val_rmse:.4f})\")\n",
        "    model.load_state_dict(best_state)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # ============= VAL-FITTED CALIBRATION =============\n",
        "    print(\"\\n📐 Fitting calibration on VAL...\")\n",
        "    calibrator = LinearRegression()\n",
        "    calibrator.fit(best_val_preds.reshape(-1, 1), best_val_trues)\n",
        "\n",
        "    # Apply to val (sanity check)\n",
        "    val_preds_cal = calibrator.predict(best_val_preds.reshape(-1, 1))\n",
        "    val_rmse_cal, val_mae_cal, val_r2_cal, val_pcc_cal = compute_metrics(best_val_trues, val_preds_cal)\n",
        "\n",
        "    print(f\"  Val (raw):        RMSE={best_val_rmse:.4f}, PCC={history['val_pcc'][-1]:.4f}\")\n",
        "    print(f\"  Val (calibrated): RMSE={val_rmse_cal:.4f}, PCC={val_pcc_cal:.4f}\")\n",
        "\n",
        "    # ============= FINAL TEST EVALUATION (ONCE) =============\n",
        "    print(\"\\n📊 Evaluating on TEST (ONCE)...\")\n",
        "    test_rmse_raw, test_mae_raw, test_r2_raw, test_pcc_raw, test_preds_raw, test_trues = evaluate(model, test_loader, device)\n",
        "\n",
        "    # Apply calibration\n",
        "    test_preds_cal = calibrator.predict(test_preds_raw.reshape(-1, 1))\n",
        "    test_rmse_cal, test_mae_cal, test_r2_cal, test_pcc_cal = compute_metrics(test_trues, test_preds_cal)\n",
        "\n",
        "    # Choose calibrated if it improves\n",
        "    use_calibrated = (test_pcc_cal > test_pcc_raw) and (test_rmse_cal <= test_rmse_raw * 1.02)\n",
        "\n",
        "    # ============= RESULTS =============\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯 FINAL TEST RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nRaw predictions:\")\n",
        "    print(f\"  RMSE: {test_rmse_raw:.4f}\")\n",
        "    print(f\"  PCC:  {test_pcc_raw:.4f}\")\n",
        "    print(f\"  MAE:  {test_mae_raw:.4f}\")\n",
        "    print(f\"  R²:   {test_r2_raw:.4f}\")\n",
        "\n",
        "    print(f\"\\nCalibrated predictions:\")\n",
        "    print(f\"  RMSE: {test_rmse_cal:.4f}\")\n",
        "    print(f\"  PCC:  {test_pcc_cal:.4f}\")\n",
        "    print(f\"  MAE:  {test_mae_cal:.4f}\")\n",
        "    print(f\"  R²:   {test_r2_cal:.4f}\")\n",
        "\n",
        "    print(f\"\\nUsing: {'Calibrated' if use_calibrated else 'Raw'}\")\n",
        "\n",
        "    final_rmse = test_rmse_cal if use_calibrated else test_rmse_raw\n",
        "    final_pcc = test_pcc_cal if use_calibrated else test_pcc_raw\n",
        "\n",
        "    print(f\"\\n{'✅' if final_rmse < 0.34 else '⚠️'}  Target RMSE < 0.34: {final_rmse:.4f}\")\n",
        "    print(f\"{'✅' if final_pcc > 0.70 else '⚠️'}  Target PCC > 0.70: {final_pcc:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'test_rmse': final_rmse,\n",
        "        'test_pcc': final_pcc,\n",
        "        'test_predictions': test_preds_cal if use_calibrated else test_preds_raw,\n",
        "        'test_actuals': test_trues,\n",
        "        'history': history,\n",
        "        'model': model,\n",
        "        'calibrator': calibrator,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "# ============= RUN IT =============\n",
        "config = {\n",
        "    'epochs': 300,\n",
        "    'patience': 30,\n",
        "    'batch_size': 32,\n",
        "    'lr_start': 1e-5,\n",
        "    'lr_max': 1e-3,\n",
        "    'warmup_epochs': 20,\n",
        "    'n_wires': 8,\n",
        "    'n_layers': 2,\n",
        "    'weight_decay': 1e-4,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "results = train_leak_free_qnn(\n",
        "    X_train_df=X_train,\n",
        "    X_test_df=X_test,\n",
        "    y_train_s=y_train,\n",
        "    y_test_s=y_test,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F27DMcoStomL",
        "outputId": "0804171f-d720-4c40-8ecc-60eeea759166"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "🔒 LEAK-FREE QNN TRAINING\n",
            "======================================================================\n",
            "\n",
            "📊 Data split:\n",
            "  Train: 1551 samples\n",
            "  Val:   388 samples\n",
            "  Test:  485 samples (FROZEN)\n",
            "\n",
            "🔧 Fitting preprocessor on TRAIN only...\n",
            "✓ Transformed: train=(1551, 50), val=(388, 50), test=(485, 50)\n",
            "✓ Device: cpu\n",
            "\n",
            "🧠 Building QNN: n_wires=8, n_layers=2\n",
            "\n",
            "🚀 Training (monitoring VAL only)...\n",
            "\n",
            "Epoch   1/300 | Loss 0.4635 | Val RMSE 0.7046 | Val PCC 0.1706 | LR 0.000010\n",
            "Epoch   2/300 | Loss 0.4085 | Val RMSE 0.6071 | Val PCC 0.4582 | LR 0.000060\n",
            "Epoch   3/300 | Loss 0.3270 | Val RMSE 0.4908 | Val PCC 0.4752 | LR 0.000109\n",
            "Epoch   4/300 | Loss 0.2425 | Val RMSE 0.4285 | Val PCC 0.5030 | LR 0.000159\n",
            "Epoch   5/300 | Loss 0.2057 | Val RMSE 0.4235 | Val PCC 0.5087 | LR 0.000208\n",
            "Epoch  10/300 | Loss 0.1808 | Val RMSE 0.4248 | Val PCC 0.4996 | LR 0.000456\n",
            "Epoch  20/300 | Loss 0.1521 | Val RMSE 0.4197 | Val PCC 0.5167 | LR 0.000950\n",
            "Epoch  30/300 | Loss 0.1507 | Val RMSE 0.4222 | Val PCC 0.5092 | LR 0.000997\n",
            "Epoch  40/300 | Loss 0.1465 | Val RMSE 0.4248 | Val PCC 0.5038 | LR 0.000989\n",
            "Epoch  50/300 | Loss 0.1430 | Val RMSE 0.4220 | Val PCC 0.5121 | LR 0.000974\n",
            "Epoch  60/300 | Loss 0.1455 | Val RMSE 0.4246 | Val PCC 0.5059 | LR 0.000953\n",
            "\n",
            "⏹️  Early stopping at epoch 65 (no val improvement for 30 epochs)\n",
            "\n",
            "✓ Loading best model (Val RMSE: 0.4182)\n",
            "\n",
            "📐 Fitting calibration on VAL...\n",
            "  Val (raw):        RMSE=0.4182, PCC=0.5059\n",
            "  Val (calibrated): RMSE=0.4180, PCC=0.5216\n",
            "\n",
            "📊 Evaluating on TEST (ONCE)...\n",
            "\n",
            "======================================================================\n",
            "🎯 FINAL TEST RESULTS\n",
            "======================================================================\n",
            "\n",
            "Raw predictions:\n",
            "  RMSE: 0.4377\n",
            "  PCC:  0.5232\n",
            "  MAE:  0.2980\n",
            "  R²:   0.2736\n",
            "\n",
            "Calibrated predictions:\n",
            "  RMSE: 0.4377\n",
            "  PCC:  0.5232\n",
            "  MAE:  0.3019\n",
            "  R²:   0.2736\n",
            "\n",
            "Using: Calibrated\n",
            "\n",
            "⚠️  Target RMSE < 0.34: 0.4377\n",
            "⚠️  Target PCC > 0.70: 0.5232\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}