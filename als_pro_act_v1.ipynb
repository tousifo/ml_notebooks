{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousifo/ml_notebooks/blob/main/als_pro_act_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxEGJHt-Ya7I",
        "outputId": "50406f3d-4069-41e6-98ea-b40da751a2dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.43.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray==0.8.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.8.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.43 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.43.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.43->pennylane) (0.3.30.0.4)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.10.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: INITIALIZATION AND DATA LOADING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load all relevant CSV tables\n",
        "# -------------------------------\n",
        "print(\"\\nüìÇ Loading PROACT datasets...\")\n",
        "alsfrs_df = pd.read_csv('PROACT_ALSFRS.csv')\n",
        "fvc_df = pd.read_csv('PROACT_FVC.csv')\n",
        "vitals_df = pd.read_csv('PROACT_VITALSIGNS.csv')\n",
        "labs_df = pd.read_csv('PROACT_LABS.csv')\n",
        "onset_df = pd.read_csv('PROACT_ALSHISTORY.csv')\n",
        "riluzole_df = pd.read_csv('PROACT_RILUZOLE.csv')\n",
        "demographics_df = pd.read_csv('PROACT_DEMOGRAPHICS.csv')\n",
        "\n",
        "print(f\"‚úÖ ALSFRS records: {len(alsfrs_df):,}\")\n",
        "print(f\"‚úÖ FVC records: {len(fvc_df):,}\")\n",
        "print(f\"‚úÖ Vitals records: {len(vitals_df):,}\")\n",
        "print(f\"‚úÖ Labs records: {len(labs_df):,}\")\n",
        "print(f\"‚úÖ Demographics: {len(demographics_df):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Compute ALSFRS (convert ALSFRS-R to original if needed)\n",
        "# -------------------------------\n",
        "print(\"\\nüßÆ Computing ALSFRS scores...\")\n",
        "\n",
        "def convert_alsfrs_row(row):\n",
        "    \"\"\"Convert ALSFRS-R to original ALSFRS if needed\"\"\"\n",
        "    if pd.notna(row.get('ALSFRS_Total')):\n",
        "        return row['ALSFRS_Total']\n",
        "    total = 0\n",
        "    for q in range(1, 10):\n",
        "        val = row.get(f'Q{q}', np.nan)\n",
        "        if pd.notna(val):\n",
        "            total += val\n",
        "    # Handle Q10 (respiratory)\n",
        "    if pd.notna(row.get('Q10_Respiratory')):\n",
        "        total += row['Q10_Respiratory']\n",
        "    elif pd.notna(row.get('R_1_Dyspnea')):\n",
        "        total += row.get('R_1_Dyspnea')\n",
        "    return total\n",
        "\n",
        "alsfrs_df['ALSFRS_Total_orig'] = alsfrs_df.apply(convert_alsfrs_row, axis=1)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Identify valid patients\n",
        "# -------------------------------\n",
        "print(\"\\nüîç Identifying valid patients...\")\n",
        "months_start, months_end = 3, 12\n",
        "min_records_start, min_records_end = 2, 2\n",
        "days_start, days_end = months_start * 30, months_end * 30\n",
        "\n",
        "alsfrs_counts = alsfrs_df.groupby('subject_id')['ALSFRS_Delta'].agg(\n",
        "    records_before_start=lambda x: (x <= days_start).sum(),\n",
        "    records_after_end=lambda x: (x >= days_end).sum()\n",
        ")\n",
        "\n",
        "valid_patients_df = alsfrs_counts[\n",
        "    (alsfrs_counts['records_before_start'] >= min_records_start) &\n",
        "    (alsfrs_counts['records_after_end'] >= min_records_end)\n",
        "]\n",
        "valid_patients = sorted(valid_patients_df.index.tolist())\n",
        "\n",
        "print(f\"‚úÖ Valid patients identified: {len(valid_patients):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Compute ALSFRS slope (3‚Äì12 months) - TARGET\n",
        "# -------------------------------\n",
        "print(\"\\nüìà Computing target ALSFRS slope (3-12 months)...\")\n",
        "slope_targets = {}\n",
        "\n",
        "for pid in valid_patients:\n",
        "    patient_data = alsfrs_df[alsfrs_df['subject_id'] == pid].copy()\n",
        "    patient_data.sort_values('ALSFRS_Delta', inplace=True)\n",
        "    t1 = patient_data[patient_data['ALSFRS_Delta'] > 90]\n",
        "    t2 = patient_data[patient_data['ALSFRS_Delta'] >= 365]\n",
        "\n",
        "    if len(t1) > 0 and len(t2) > 0:\n",
        "        t1_record = t1.iloc[0]\n",
        "        t2_record = t2.iloc[0]\n",
        "        delta_days = t2_record['ALSFRS_Delta'] - t1_record['ALSFRS_Delta']\n",
        "        if delta_days > 0:\n",
        "            slope = (t2_record['ALSFRS_Total_orig'] - t1_record['ALSFRS_Total_orig']) / (delta_days / 30.0)\n",
        "            slope_targets[pid] = slope\n",
        "\n",
        "target_df = pd.Series(slope_targets, name='ALSFRS_slope_3to12m')\n",
        "print(f\"‚úÖ ALSFRS slope computed for {len(target_df):,} patients\")\n",
        "print(\"\\nüìä Target Statistics:\")\n",
        "print(target_df.describe())\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Helper functions for feature engineering\n",
        "# -------------------------------\n",
        "print(\"\\nüõ†Ô∏è  Setting up feature engineering functions...\")\n",
        "\n",
        "def summarize_timeseries(df, time_col, value_col):\n",
        "    \"\"\"Enhanced time-series summarization with additional statistics\"\"\"\n",
        "    grp = df.groupby('subject_id')\n",
        "    summary = pd.DataFrame({\n",
        "        'min': grp[value_col].min(),\n",
        "        'max': grp[value_col].max(),\n",
        "        'mean': grp[value_col].mean(),  # Added mean\n",
        "        'median': grp[value_col].median(),\n",
        "        'std': grp[value_col].std(),\n",
        "        'q25': grp[value_col].quantile(0.25),  # Added 25th percentile\n",
        "        'q75': grp[value_col].quantile(0.75),  # Added 75th percentile\n",
        "        'first': grp.apply(lambda g: g.sort_values(time_col)[value_col].iloc[0], include_groups=False),\n",
        "        'last': grp.apply(lambda g: g.sort_values(time_col)[value_col].iloc[-1], include_groups=False)\n",
        "    })\n",
        "\n",
        "    # Compute slope (rate of change)\n",
        "    time_first = grp[time_col].min()\n",
        "    time_last = grp[time_col].max()\n",
        "    time_diff_months = (time_last - time_first) / 30.0\n",
        "    summary['slope'] = (summary['last'] - summary['first']) / time_diff_months\n",
        "    summary.loc[time_diff_months == 0, 'slope'] = np.nan\n",
        "\n",
        "    # Add range\n",
        "    summary['range'] = summary['max'] - summary['min']\n",
        "\n",
        "    return summary\n",
        "\n",
        "def summarize_all_numeric(df, time_col):\n",
        "    \"\"\"Summarize all numeric columns in a time-series DataFrame\"\"\"\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.drop([time_col, 'subject_id'], errors='ignore')\n",
        "    summaries = {}\n",
        "    for col in numeric_cols:\n",
        "        summaries[col] = summarize_timeseries(df, time_col, col)\n",
        "        summaries[col].columns = [f'{col}_{c}' for c in summaries[col].columns]\n",
        "    return summaries\n",
        "\n",
        "print(\"‚úÖ Feature engineering functions ready\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Extract first 90 days data\n",
        "# -------------------------------\n",
        "print(\"\\nüìÖ Extracting first 90 days data...\")\n",
        "\n",
        "alsfrs_3m = alsfrs_df[alsfrs_df['subject_id'].isin(valid_patients) & (alsfrs_df['ALSFRS_Delta'] <= 90)]\n",
        "fvc_df['FVC'] = fvc_df[['Subject_Liters_Trial_1','Subject_Liters_Trial_2','Subject_Liters_Trial_3']].max(axis=1)\n",
        "fvc_3m = fvc_df[fvc_df['subject_id'].isin(valid_patients) & (fvc_df['Forced_Vital_Capacity_Delta'] <= 90)]\n",
        "vitals_3m = vitals_df[vitals_df['subject_id'].isin(valid_patients) & (vitals_df['Vital_Signs_Delta'] <= 90)]\n",
        "labs_3m = labs_df[labs_df['subject_id'].isin(valid_patients) & (labs_df['Laboratory_Delta'] <= 90)]\n",
        "\n",
        "print(f\"‚úÖ ALSFRS 3-month records: {len(alsfrs_3m):,}\")\n",
        "print(f\"‚úÖ FVC 3-month records: {len(fvc_3m):,}\")\n",
        "print(f\"‚úÖ Vitals 3-month records: {len(vitals_3m):,}\")\n",
        "print(f\"‚úÖ Labs 3-month records: {len(labs_3m):,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Create summarized features\n",
        "# -------------------------------\n",
        "print(\"\\nüî® Creating summarized features from time-series data...\")\n",
        "\n",
        "alsfrs_features = summarize_all_numeric(alsfrs_3m, 'ALSFRS_Delta')\n",
        "fvc_features = summarize_all_numeric(fvc_3m, 'Forced_Vital_Capacity_Delta')\n",
        "vitals_features = summarize_all_numeric(vitals_3m, 'Vital_Signs_Delta')\n",
        "labs_features = summarize_all_numeric(labs_3m, 'Laboratory_Delta')\n",
        "\n",
        "print(f\"‚úÖ ALSFRS features: {len(alsfrs_features)} variables\")\n",
        "print(f\"‚úÖ FVC features: {len(fvc_features)} variables\")\n",
        "print(f\"‚úÖ Vitals features: {len(vitals_features)} variables\")\n",
        "print(f\"‚úÖ Labs features: {len(labs_features)} variables\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Merge all features\n",
        "# -------------------------------\n",
        "print(\"\\nüîó Merging all features...\")\n",
        "\n",
        "features_df = pd.DataFrame(index=valid_patients)\n",
        "\n",
        "# Prepare static features\n",
        "onset_static = onset_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Site_of_Onset', 'Onset_Delta', 'Diagnosis_Delta']]\n",
        "riluzole_static = riluzole_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Subject_used_Riluzole', 'Riluzole_use_Delta']]\n",
        "demographics_static = demographics_df.drop_duplicates(subset='subject_id', keep='first').set_index('subject_id')[['Age', 'Sex']]\n",
        "\n",
        "# Join static features\n",
        "features_df = features_df.join(onset_static, how='left')\n",
        "features_df = features_df.join(riluzole_static, how='left', rsuffix='_rilu')\n",
        "features_df = features_df.join(demographics_static, how='left', rsuffix='_demo')\n",
        "\n",
        "# Add dynamic (summarized) features\n",
        "for group in [alsfrs_features, fvc_features, vitals_features, labs_features]:\n",
        "    for feat_df in group.values():\n",
        "        features_df = features_df.join(feat_df, how='left')\n",
        "\n",
        "# Add slope target\n",
        "features_df = features_df.join(target_df, how='left')\n",
        "\n",
        "print(f\"‚úÖ Features merged. Shape: {features_df.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Initial cleanup\n",
        "# -------------------------------\n",
        "print(\"\\nüßπ Initial cleanup...\")\n",
        "\n",
        "# Remove columns with all NaN values\n",
        "features_df = features_df.dropna(axis=1, how='all')\n",
        "\n",
        "# Remove columns with only one unique value\n",
        "features_df = features_df.loc[:, features_df.nunique() > 1]\n",
        "\n",
        "print(f\"‚úÖ After cleanup. Shape: {features_df.shape}\")\n",
        "print(f\"\\nüìã Missing values per column: {features_df.isnull().sum().sum():,} total\")\n",
        "print(f\"üìã Columns with >30% missing: {(features_df.isnull().sum() / len(features_df) > 0.3).sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ STEP 1 COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Final dataset info:\")\n",
        "print(f\"  - Total patients: {len(features_df):,}\")\n",
        "print(f\"  - Total features: {features_df.shape[1] - 1}\")  # -1 for target\n",
        "print(f\"  - Target variable: ALSFRS_slope_3to12m\")\n",
        "print(f\"  - Patients with target: {features_df['ALSFRS_slope_3to12m'].notna().sum():,}\")\n",
        "print(\"\\nüîç Preview:\")\n",
        "print(features_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgwkVE8D4wRY",
        "outputId": "7575327e-e583-4077-bdd2-1eb4e491cce0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: INITIALIZATION AND DATA LOADING\n",
            "============================================================\n",
            "\n",
            "üìÇ Loading PROACT datasets...\n",
            "‚úÖ ALSFRS records: 73,845\n",
            "‚úÖ FVC records: 49,110\n",
            "‚úÖ Vitals records: 84,721\n",
            "‚úÖ Labs records: 2,937,162\n",
            "‚úÖ Demographics: 12,504\n",
            "\n",
            "üßÆ Computing ALSFRS scores...\n",
            "\n",
            "üîç Identifying valid patients...\n",
            "‚úÖ Valid patients identified: 2,442\n",
            "\n",
            "üìà Computing target ALSFRS slope (3-12 months)...\n",
            "‚úÖ ALSFRS slope computed for 2,439 patients\n",
            "\n",
            "üìä Target Statistics:\n",
            "count    2439.000000\n",
            "mean       -0.388076\n",
            "std         0.496497\n",
            "min        -3.100000\n",
            "25%        -0.638298\n",
            "50%        -0.218978\n",
            "75%         0.000000\n",
            "max         1.052632\n",
            "Name: ALSFRS_slope_3to12m, dtype: float64\n",
            "\n",
            "üõ†Ô∏è  Setting up feature engineering functions...\n",
            "‚úÖ Feature engineering functions ready\n",
            "\n",
            "üìÖ Extracting first 90 days data...\n",
            "‚úÖ ALSFRS 3-month records: 8,210\n",
            "‚úÖ FVC 3-month records: 5,880\n",
            "‚úÖ Vitals 3-month records: 9,625\n",
            "‚úÖ Labs 3-month records: 403,408\n",
            "\n",
            "üî® Creating summarized features from time-series data...\n",
            "‚úÖ ALSFRS features: 17 variables\n",
            "‚úÖ FVC features: 8 variables\n",
            "‚úÖ Vitals features: 27 variables\n",
            "‚úÖ Labs features: 0 variables\n",
            "\n",
            "üîó Merging all features...\n",
            "‚úÖ Features merged. Shape: (2442, 580)\n",
            "\n",
            "üßπ Initial cleanup...\n",
            "‚úÖ After cleanup. Shape: (2442, 518)\n",
            "\n",
            "üìã Missing values per column: 691,712 total\n",
            "üìã Columns with >30% missing: 318\n",
            "\n",
            "============================================================\n",
            "‚úÖ STEP 1 COMPLETED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "üìä Final dataset info:\n",
            "  - Total patients: 2,442\n",
            "  - Total features: 517\n",
            "  - Target variable: ALSFRS_slope_3to12m\n",
            "  - Patients with target: 2,439\n",
            "\n",
            "üîç Preview:\n",
            "      Site_of_Onset  Onset_Delta  Diagnosis_Delta Subject_used_Riluzole  \\\n",
            "121     Onset: Limb          NaN              NaN                   Yes   \n",
            "1009   Onset: Other       -324.0            -63.0                   Yes   \n",
            "1036  Onset: Bulbar          NaN              NaN                   NaN   \n",
            "\n",
            "      Riluzole_use_Delta   Age     Sex  Q1_Speech_min  Q1_Speech_max  \\\n",
            "121                  0.0  52.0  Female            4.0            4.0   \n",
            "1009                 0.0  51.0    Male            4.0            4.0   \n",
            "1036                 NaN  67.0  Female            3.0            3.0   \n",
            "\n",
            "      Q1_Speech_mean  ...  Standing_BP_Diastolic_last  \\\n",
            "121              4.0  ...                         NaN   \n",
            "1009             4.0  ...                         NaN   \n",
            "1036             3.0  ...                         NaN   \n",
            "\n",
            "      Standing_BP_Systolic_min  Standing_BP_Systolic_max  \\\n",
            "121                        NaN                       NaN   \n",
            "1009                       NaN                       NaN   \n",
            "1036                       NaN                       NaN   \n",
            "\n",
            "      Standing_BP_Systolic_mean  Standing_BP_Systolic_median  \\\n",
            "121                         NaN                          NaN   \n",
            "1009                        NaN                          NaN   \n",
            "1036                        NaN                          NaN   \n",
            "\n",
            "      Standing_BP_Systolic_q25  Standing_BP_Systolic_q75  \\\n",
            "121                        NaN                       NaN   \n",
            "1009                       NaN                       NaN   \n",
            "1036                       NaN                       NaN   \n",
            "\n",
            "      Standing_BP_Systolic_first  Standing_BP_Systolic_last  \\\n",
            "121                          NaN                        NaN   \n",
            "1009                         NaN                        NaN   \n",
            "1036                         NaN                        NaN   \n",
            "\n",
            "      ALSFRS_slope_3to12m  \n",
            "121             -1.058824  \n",
            "1009             0.000000  \n",
            "1036                  NaN  \n",
            "\n",
            "[3 rows x 518 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "8FvNDrtz7VbD",
        "outputId": "51b8c8b4-d6e0-43f6-e135-51252d605a87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Site_of_Onset  Onset_Delta  Diagnosis_Delta Subject_used_Riluzole  \\\n",
              "121     Onset: Limb          NaN              NaN                   Yes   \n",
              "1009   Onset: Other       -324.0            -63.0                   Yes   \n",
              "1036  Onset: Bulbar          NaN              NaN                   NaN   \n",
              "\n",
              "      Riluzole_use_Delta   Age     Sex  Q1_Speech_min  Q1_Speech_max  \\\n",
              "121                  0.0  52.0  Female            4.0            4.0   \n",
              "1009                 0.0  51.0    Male            4.0            4.0   \n",
              "1036                 NaN  67.0  Female            3.0            3.0   \n",
              "\n",
              "      Q1_Speech_mean  ...  Standing_BP_Diastolic_last  \\\n",
              "121              4.0  ...                         NaN   \n",
              "1009             4.0  ...                         NaN   \n",
              "1036             3.0  ...                         NaN   \n",
              "\n",
              "      Standing_BP_Systolic_min  Standing_BP_Systolic_max  \\\n",
              "121                        NaN                       NaN   \n",
              "1009                       NaN                       NaN   \n",
              "1036                       NaN                       NaN   \n",
              "\n",
              "      Standing_BP_Systolic_mean  Standing_BP_Systolic_median  \\\n",
              "121                         NaN                          NaN   \n",
              "1009                        NaN                          NaN   \n",
              "1036                        NaN                          NaN   \n",
              "\n",
              "      Standing_BP_Systolic_q25  Standing_BP_Systolic_q75  \\\n",
              "121                        NaN                       NaN   \n",
              "1009                       NaN                       NaN   \n",
              "1036                       NaN                       NaN   \n",
              "\n",
              "      Standing_BP_Systolic_first  Standing_BP_Systolic_last  \\\n",
              "121                          NaN                        NaN   \n",
              "1009                         NaN                        NaN   \n",
              "1036                         NaN                        NaN   \n",
              "\n",
              "      ALSFRS_slope_3to12m  \n",
              "121             -1.058824  \n",
              "1009             0.000000  \n",
              "1036                  NaN  \n",
              "\n",
              "[3 rows x 518 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a204bb38-404a-42e4-8d49-1e85b8eb673a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Site_of_Onset</th>\n",
              "      <th>Onset_Delta</th>\n",
              "      <th>Diagnosis_Delta</th>\n",
              "      <th>Subject_used_Riluzole</th>\n",
              "      <th>Riluzole_use_Delta</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Q1_Speech_min</th>\n",
              "      <th>Q1_Speech_max</th>\n",
              "      <th>Q1_Speech_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>Standing_BP_Diastolic_last</th>\n",
              "      <th>Standing_BP_Systolic_min</th>\n",
              "      <th>Standing_BP_Systolic_max</th>\n",
              "      <th>Standing_BP_Systolic_mean</th>\n",
              "      <th>Standing_BP_Systolic_median</th>\n",
              "      <th>Standing_BP_Systolic_q25</th>\n",
              "      <th>Standing_BP_Systolic_q75</th>\n",
              "      <th>Standing_BP_Systolic_first</th>\n",
              "      <th>Standing_BP_Systolic_last</th>\n",
              "      <th>ALSFRS_slope_3to12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>Onset: Limb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>Onset: Other</td>\n",
              "      <td>-324.0</td>\n",
              "      <td>-63.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>Onset: Bulbar</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>67.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows √ó 518 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a204bb38-404a-42e4-8d49-1e85b8eb673a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a204bb38-404a-42e4-8d49-1e85b8eb673a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a204bb38-404a-42e4-8d49-1e85b8eb673a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aa248461-a285-4427-bec0-4cb6659871df\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa248461-a285-4427-bec0-4cb6659871df')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aa248461-a285-4427-bec0-4cb6659871df button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "features_df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# leakage free preprocessing"
      ],
      "metadata": {
        "id": "Un9iRPKUoDPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BUILD y (3‚Üí12m ALSFRS slope per ~30d), aligned to features_df ====\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# --- locate long ALSFRS table ---\n",
        "def _first_existing(names):\n",
        "    g = globals()\n",
        "    for n in names:\n",
        "        if n in g and isinstance(g[n], pd.DataFrame):\n",
        "            return g[n], n\n",
        "    return None, None\n",
        "\n",
        "als_long, _als_name = _first_existing([\n",
        "    \"alsfrs_long\", \"alsfrs_all\", \"alsfrs_full\", \"alsfrs\", \"als_long\", \"alsfrs_df\"\n",
        "])\n",
        "if als_long is None:\n",
        "    raise RuntimeError(\"Long ALSFRS DataFrame not found. Load it (e.g., alsfrs_long).\")\n",
        "\n",
        "# --- detect columns (subject, time, score) ---\n",
        "def _pick_col(df, cands):\n",
        "    for c in cands:\n",
        "        if c in df.columns: return c\n",
        "    lc = {c.lower(): c for c in df.columns}\n",
        "    for c in cands:\n",
        "        if c.lower() in lc: return lc[c.lower()]\n",
        "    return None\n",
        "\n",
        "SUBJ  = _pick_col(als_long, [\"subject_id\",\"Subject_ID\",\"RID\",\"patient_id\"])\n",
        "TIME  = _pick_col(als_long, [\"ALSFRS_Delta\",\"days\",\"Days\",\"days_since_first\",\"days_since_baseline\"])\n",
        "SCORE = _pick_col(als_long, [\"ALSFRS_Total_orig\",\"ALSFRS_Total\",\"ALSFRS\",\"ALSFRS_R\",\"ALSFRS_R_Total\"])\n",
        "if any(v is None for v in [SUBJ,TIME,SCORE]):\n",
        "    raise RuntimeError(f\"Could not auto-detect columns: SUBJ={SUBJ}, TIME={TIME}, SCORE={SCORE}\")\n",
        "\n",
        "# --- slope per subject using points in (90, 365] days; per-30-days units ---\n",
        "def _slope_per30d(g: pd.DataFrame, t_col: str, y_col: str) -> float | None:\n",
        "    g = g[[t_col, y_col]].dropna()\n",
        "    g = g[(g[t_col] > 90) & (g[t_col] <= 365)]\n",
        "    if len(g) < 2:\n",
        "        return None\n",
        "    t = g[t_col].to_numpy(dtype=float)\n",
        "    y = g[y_col].to_numpy(dtype=float)\n",
        "    a, b = np.polyfit(t, y, deg=1)   # points/day\n",
        "    return float(a * 30.0)           # ‚âà per month\n",
        "\n",
        "slopes = (\n",
        "    als_long\n",
        "    .groupby(SUBJ, group_keys=False)\n",
        "    .apply(lambda df: _slope_per30d(df, TIME, SCORE))\n",
        "    .rename(\"slope_3to12m\")\n",
        ")\n",
        "\n",
        "# --- align to features_df index; drop subjects without slope ---\n",
        "if \"features_df\" not in globals():\n",
        "    raise RuntimeError(\"features_df not found. Build your subject-level feature table first.\")\n",
        "\n",
        "# FIXED: Remove target column from features_df if it exists\n",
        "if 'ALSFRS_slope_3to12m' in features_df.columns:\n",
        "    features_df = features_df.drop(columns=['ALSFRS_slope_3to12m'])\n",
        "\n",
        "y = slopes.reindex(features_df.index)\n",
        "mask = y.notna()\n",
        "features_df = features_df.loc[mask].copy()\n",
        "y = y.loc[mask].copy()\n",
        "\n",
        "print(\"Built target `y`.\")\n",
        "print(\"features_df:\", features_df.shape, \"| y:\", y.shape, \"| mean:\", round(y.mean(),3), \"std:\", round(y.std(),3))\n",
        "print(f\"‚úì Confirmed: Target column removed from features_df\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp-LYf_pqT0H",
        "outputId": "bf452e62-c791-4d9d-f3a7-94fcda6e5ca1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built target `y`.\n",
            "features_df: (2424, 517) | y: (2424,) | mean: -0.383 std: 0.522\n",
            "‚úì Confirmed: Target column removed from features_df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CLEAN TRAIN/TEST SPLIT (no transforms; no leakage) ====\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# FIXED: Ensure target is not in features\n",
        "if 'ALSFRS_slope_3to12m' in features_df.columns:\n",
        "    features_df = features_df.drop(columns=['ALSFRS_slope_3to12m'])\n",
        "\n",
        "assert all(features_df.index == y.index), \"Index mismatch between features and target!\"\n",
        "assert 'ALSFRS_slope_3to12m' not in features_df.columns, \"Target column still in features!\"\n",
        "\n",
        "def stratify_bins(y_series, n_bins=10):\n",
        "    q = pd.qcut(y_series, q=np.minimum(n_bins, max(2, y_series.nunique())), duplicates='drop')\n",
        "    return pd.factorize(q, sort=True)[0]\n",
        "\n",
        "bins = stratify_bins(y, n_bins=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_df, y, test_size=0.2, random_state=42, stratify=bins\n",
        ")\n",
        "X_train = X_train.copy(); X_test = X_test.copy()\n",
        "y_train = y_train.copy(); y_test = y_test.copy()\n",
        "\n",
        "print(\"‚úì Train/test split complete\")\n",
        "print(f\"  X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "print(f\"  ‚úì No target leakage - verified!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZA14tz0oKHu",
        "outputId": "6eeab6d5-d071-4bf8-bece-85d53d5841b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Train/test split complete\n",
            "  X_train: (1939, 517), X_test: (485, 517)\n",
            "  y_train: (1939,), y_test: (485,)\n",
            "  ‚úì No target leakage - verified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BULLETPROOF PREPROCESSOR (categorical-safe, no leakage) ====\n",
        "import re\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, top_k=25, max_missing=0.5, use_pca=False, pca_components=12,\n",
        "                 max_cats_per_col=8, numeric_threshold=1.0, force_cat=None):\n",
        "        \"\"\"\n",
        "        numeric_threshold=1.0  -> only columns 100% numeric-convertible are treated as numeric.\n",
        "        force_cat: list[str] of columns to always treat as categorical (optional).\n",
        "        \"\"\"\n",
        "        self.top_k = top_k\n",
        "        self.max_missing = max_missing\n",
        "        self.use_pca = use_pca\n",
        "        self.pca_components = pca_components\n",
        "        self.max_cats_per_col = max_cats_per_col\n",
        "        self.numeric_threshold = numeric_threshold\n",
        "        self.force_cat = set(force_cat or [])\n",
        "\n",
        "        # learned\n",
        "        self.num_cols_, self.cat_cols_ = [], []\n",
        "        self.cat_maps_ = {}      # col -> kept categories\n",
        "        self.keep_cols_ = []\n",
        "        self.scaler_ = None\n",
        "        self.pca_ = None\n",
        "        self.num_medians_ = None\n",
        "\n",
        "    # ----- helpers -----\n",
        "    @staticmethod\n",
        "    def _has_letters(sample_values) -> bool:\n",
        "        # Detect alpha characters in sample of values (flags columns like \"Onset: Limb\")\n",
        "        for v in sample_values:\n",
        "            if pd.isna(v):\n",
        "                continue\n",
        "            s = str(v)\n",
        "            if re.search(r\"[A-Za-z]\", s):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _split_num_cat(self, X: pd.DataFrame):\n",
        "        num_cols, cat_cols = [], []\n",
        "        for c in X.columns:\n",
        "            if c in self.force_cat:\n",
        "                cat_cols.append(c); continue\n",
        "            s = X[c]\n",
        "            # quick letter check on 100 non-null samples\n",
        "            nonnull = s.dropna()\n",
        "            sample = nonnull.sample(min(100, len(nonnull)), random_state=42) if len(nonnull) else nonnull\n",
        "            if self._has_letters(sample.values):\n",
        "                cat_cols.append(c); continue\n",
        "            # numeric convertibility\n",
        "            s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            frac_numeric = s_num.notna().mean()\n",
        "            if frac_numeric >= self.numeric_threshold:\n",
        "                num_cols.append(c)\n",
        "            else:\n",
        "                cat_cols.append(c)\n",
        "        return num_cols, cat_cols\n",
        "\n",
        "    def _encode_cats_fit(self, X_cat: pd.DataFrame) -> pd.DataFrame:\n",
        "        oh = []\n",
        "        for c in X_cat.columns:\n",
        "            s = X_cat[c].astype(\"object\")\n",
        "            s = s.astype(str).where(~s.isna(), \"MISSING\")\n",
        "            vc = s.value_counts(dropna=False)\n",
        "            keep = vc.index.tolist()[: max(1, self.max_cats_per_col - 1)]\n",
        "            if \"MISSING\" in s.values and \"MISSING\" not in keep:\n",
        "                if len(keep) >= self.max_cats_per_col:\n",
        "                    keep = keep[:-1] + [\"MISSING\"]\n",
        "                else:\n",
        "                    keep = keep + [\"MISSING\"]\n",
        "            keep = list(dict.fromkeys(keep))\n",
        "            self.cat_maps_[c] = keep\n",
        "            for k in keep:\n",
        "                col = f\"{c}__{k}\"\n",
        "                oh.append(pd.Series((s == k).astype(np.float32), index=s.index, name=col))\n",
        "            # OTHER bucket\n",
        "            other = ~s.isin(keep)\n",
        "            oh.append(pd.Series(other.astype(np.float32), index=s.index, name=f\"{c}__OTHER\"))\n",
        "        return pd.concat(oh, axis=1) if len(oh) else pd.DataFrame(index=X_cat.index)\n",
        "\n",
        "    def _encode_cats_apply(self, X_cat: pd.DataFrame) -> pd.DataFrame:\n",
        "        oh = []\n",
        "        for c in self.cat_cols_:\n",
        "            s = X_cat[c] if c in X_cat.columns else pd.Series(index=X_cat.index, dtype=\"object\")\n",
        "            s = s.astype(str).where(~s.isna(), \"MISSING\")\n",
        "            keep = self.cat_maps_.get(c, [])\n",
        "            for k in keep:\n",
        "                col = f\"{c}__{k}\"\n",
        "                oh.append(pd.Series((s == k).astype(np.float32), index=s.index, name=col))\n",
        "            other = ~s.isin(keep)\n",
        "            oh.append(pd.Series(other.astype(np.float32), index=s.index, name=f\"{c}__OTHER\"))\n",
        "        return pd.concat(oh, axis=1) if len(oh) else pd.DataFrame(index=X_cat.index)\n",
        "\n",
        "    def _feature_scores(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
        "        # Ensure y is pure numpy\n",
        "        y_np = np.array(y.values if hasattr(y, 'values') else y, dtype=np.float64)\n",
        "\n",
        "        # RF importance\n",
        "        rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X.values, y_np)  # Use .values to ensure numpy array\n",
        "        s_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "        # Mutual information\n",
        "        mi = mutual_info_regression(X.values, y_np, random_state=42)\n",
        "        s_mi = pd.Series(mi, index=X.columns)\n",
        "\n",
        "        # |Pearson r|\n",
        "        def safe_corr(col):\n",
        "            v = col.values\n",
        "            if np.std(v) == 0: return 0.0\n",
        "            return float(abs(np.corrcoef(v, y_np)[0,1]))\n",
        "        s_pr = X.apply(safe_corr)\n",
        "\n",
        "        # Blend (normalized)\n",
        "        def nz_norm(s):\n",
        "            s = s.fillna(0.0); m = s.max()\n",
        "            return s / m if m > 0 else s\n",
        "        blended = 0.5*nz_norm(s_rf) + 0.3*nz_norm(s_mi) + 0.2*nz_norm(s_pr)\n",
        "        return blended.sort_values(ascending=False)\n",
        "\n",
        "    # ----- API -----\n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        # split by robust content check (no strings into numeric)\n",
        "        self.num_cols_, self.cat_cols_ = self._split_num_cat(X_train)\n",
        "\n",
        "        # numeric missingness filter on TRAIN only\n",
        "        num_keep = []\n",
        "        if self.num_cols_:\n",
        "            coerced = X_train[self.num_cols_].apply(pd.to_numeric, errors=\"coerce\")\n",
        "            miss = coerced.isna().mean()\n",
        "            num_keep = miss[miss <= self.max_missing].index.tolist()\n",
        "\n",
        "        cat_keep = self.cat_cols_\n",
        "        X_tr = X_train[num_keep + cat_keep].copy()\n",
        "\n",
        "        # numeric block (hard coerce to float32) + train medians\n",
        "        if num_keep:\n",
        "            X_tr_num = X_tr[num_keep].apply(pd.to_numeric, errors=\"coerce\").astype(np.float32)\n",
        "            self.num_medians_ = X_tr_num.median()\n",
        "            X_tr_num = X_tr_num.fillna(self.num_medians_)\n",
        "        else:\n",
        "            X_tr_num = pd.DataFrame(index=X_tr.index, dtype=np.float32)\n",
        "            self.num_medians_ = pd.Series(dtype=np.float32)\n",
        "\n",
        "        # categorical block ‚Üí one-hot (fit)\n",
        "        X_tr_cat = X_tr[cat_keep] if cat_keep else pd.DataFrame(index=X_tr.index)\n",
        "        X_tr_cat_oh = self._encode_cats_fit(X_tr_cat)\n",
        "\n",
        "        # combine\n",
        "        X_tr_full = pd.concat([X_tr_num, X_tr_cat_oh], axis=1)\n",
        "\n",
        "        # feature scoring/selection on TRAIN only\n",
        "        scores = self._feature_scores(X_tr_full, y_train)\n",
        "        self.keep_cols_ = scores.head(self.top_k).index.tolist()\n",
        "\n",
        "        # scale fit on TRAIN selected\n",
        "        self.scaler_ = RobustScaler()\n",
        "        X_sel = X_tr_full[self.keep_cols_].values  # Use .values for pure numpy\n",
        "        X_scl = self.scaler_.fit_transform(X_sel)\n",
        "\n",
        "        # optional PCA\n",
        "        if self.use_pca:\n",
        "            n_comp = min(self.pca_components, X_scl.shape[1])\n",
        "            self.pca_ = PCA(n_components=n_comp, random_state=42)\n",
        "            self.pca_.fit(X_scl)\n",
        "        else:\n",
        "            self.pca_ = None\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        # numeric (coerce to float32, fill with TRAIN medians)\n",
        "        if len(self.num_cols_):\n",
        "            cols_num = [c for c in self.num_cols_ if c in X.columns]\n",
        "            X_num = X[cols_num].apply(pd.to_numeric, errors=\"coerce\").astype(np.float32)\n",
        "            # make sure all expected numeric cols exist\n",
        "            for c in self.num_medians_.index:\n",
        "                if c not in X_num.columns:\n",
        "                    X_num[c] = np.nan\n",
        "            X_num = X_num[self.num_medians_.index]\n",
        "            X_num = X_num.fillna(self.num_medians_)\n",
        "        else:\n",
        "            X_num = pd.DataFrame(index=X.index, dtype=np.float32)\n",
        "\n",
        "        # categorical\n",
        "        cols_cat = [c for c in self.cat_cols_ if c in X.columns]\n",
        "        X_cat = X[cols_cat] if cols_cat else pd.DataFrame(index=X.index)\n",
        "        X_cat_oh = self._encode_cats_apply(X_cat)\n",
        "\n",
        "        # combine & align to kept features\n",
        "        X_full = pd.concat([X_num, X_cat_oh], axis=1)\n",
        "        for c in self.keep_cols_:\n",
        "            if c not in X_full.columns:\n",
        "                X_full[c] = 0.0\n",
        "        X_full = X_full[self.keep_cols_]\n",
        "\n",
        "        # CRITICAL: Convert to pure numpy BEFORE scaling\n",
        "        X_np = X_full.values.astype(np.float32)\n",
        "        X_scl = self.scaler_.transform(X_np)\n",
        "\n",
        "        if self.pca_ is not None:\n",
        "            X_scl = self.pca_.transform(X_scl)\n",
        "\n",
        "        # CRITICAL: Return pure numpy array with explicit copy\n",
        "        return np.array(X_scl, dtype=np.float32, copy=True)\n"
      ],
      "metadata": {
        "id": "DQgkM5tGoRPg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== QNN TRAINING - ABSOLUTE FINAL WORKING VERSION ====\n",
        "\n",
        "import math, time, gc\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pennylane as qml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ---------------- metrics ----------------\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Completely tensor-safe metric computation\"\"\"\n",
        "    y_true = np.array(y_true, dtype=np.float64, copy=True).reshape(-1)\n",
        "    y_pred = np.array(y_pred, dtype=np.float64, copy=True).reshape(-1)\n",
        "\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
        "    r2   = float(r2_score(y_true, y_pred))\n",
        "    pcc  = float(pearsonr(y_true, y_pred)[0]) if (np.std(y_true)>0 and np.std(y_pred)>0) else float(\"nan\")\n",
        "    return rmse, mae, r2, pcc\n",
        "\n",
        "# --------------- dataset -----------------\n",
        "class NPDataset(Dataset):\n",
        "    def __init__(self, X_np, y_np):\n",
        "        if isinstance(X_np, pd.DataFrame):\n",
        "            X_np = X_np.values\n",
        "        if isinstance(y_np, pd.Series):\n",
        "            y_np = y_np.values\n",
        "\n",
        "        X_np = np.array(X_np, dtype=np.float32, copy=True)\n",
        "        y_np = np.array(y_np, dtype=np.float32, copy=True).reshape(-1, 1)\n",
        "\n",
        "        self.X = torch.from_numpy(X_np)\n",
        "        self.y = torch.from_numpy(y_np)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --------------- QNN model ----------------\n",
        "class QNNRegressor(nn.Module):\n",
        "    def __init__(self, input_dim: int, n_wires: int = 8, n_layers: int = 2, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_wires = int(min(n_wires, max(1, input_dim)))\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Use default.qubit with shots=None for exact simulation\n",
        "        dev = qml.device(\"default.qubit\", wires=self.n_wires)\n",
        "\n",
        "        @qml.qnode(dev, interface=\"torch\")  # No diff_method specified - let PennyLane choose\n",
        "        def qnode(inputs, weights):\n",
        "            # Use only the first n_wires features\n",
        "            inputs_use = inputs[..., :self.n_wires]\n",
        "\n",
        "            qml.AngleEmbedding(inputs_use, wires=range(self.n_wires), rotation=\"Y\")\n",
        "            qml.StronglyEntanglingLayers(weights, wires=range(self.n_wires))\n",
        "\n",
        "            # Return list of expectations\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_wires)]\n",
        "\n",
        "        weight_shapes = {\"weights\": (self.n_layers, self.n_wires, 3)}\n",
        "        self.q_layer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(self.n_wires + self.input_dim, hidden), nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get quantum layer output\n",
        "        q_out = self.q_layer(x)\n",
        "\n",
        "        # Handle different output shapes from PennyLane\n",
        "        if isinstance(q_out, (list, tuple)):\n",
        "            q_out = torch.stack(q_out, dim=-1)\n",
        "        if len(q_out.shape) == 1:\n",
        "            q_out = q_out.unsqueeze(0)\n",
        "        if q_out.shape[-1] != self.n_wires:\n",
        "            q_out = q_out.reshape(-1, self.n_wires)\n",
        "\n",
        "        # Concatenate quantum output with classical features\n",
        "        z = torch.cat([q_out, x], dim=1)\n",
        "        return self.head(z)\n",
        "\n",
        "# ----------- train utils -------------\n",
        "def cosine_warmup_lr(epoch, cfg):\n",
        "    if epoch < cfg['warmup_epochs']:\n",
        "        return cfg['lr_start'] + (cfg['lr_max'] - cfg['lr_start']) * (epoch / cfg['warmup_epochs'])\n",
        "    progress = (epoch - cfg['warmup_epochs']) / max(1, (cfg['epochs'] - cfg['warmup_epochs']))\n",
        "    return cfg['lr_max'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "def corr_loss(y_hat, y):\n",
        "    \"\"\"Correlation loss function\"\"\"\n",
        "    x = y_hat.view(-1)\n",
        "    t = y.view(-1)\n",
        "    x_centered = x - x.mean()\n",
        "    t_centered = t - t.mean()\n",
        "    numerator = (x_centered * t_centered).mean()\n",
        "    denominator = torch.sqrt((x_centered ** 2).mean() * (t_centered ** 2).mean()) + 1e-8\n",
        "    corr = numerator / denominator\n",
        "    return 1.0 - corr\n",
        "\n",
        "# ------------- main runner ---------------\n",
        "def run_qnn_training(\n",
        "    X_train_df, X_test_df, y_train_s, y_test_s,\n",
        "    cfg=None,\n",
        "    pre_top_k=25, use_pca=False, pca_components=12,\n",
        "    force_cat_cols=None\n",
        "):\n",
        "    \"\"\"Train strictly on train/val; report test once.\"\"\"\n",
        "    cfg = cfg or {\n",
        "        \"epochs\": 200, \"patience\": 25,\n",
        "        \"batch\": 64, \"lr_start\": 1e-5, \"lr_max\": 1e-3, \"warmup_epochs\": 10,\n",
        "        \"n_wires\": 8, \"n_layers\": 2, \"hidden\": 64, \"corr_lambda\": 0.10,\n",
        "        \"val_size\": 0.2, \"random_state\": 42\n",
        "    }\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"QNN TRAINING (leak-free)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Inner train/val split\n",
        "    X_tr_df, X_val_df, y_tr, y_val = train_test_split(\n",
        "        X_train_df, y_train_s, test_size=cfg[\"val_size\"],\n",
        "        random_state=cfg[\"random_state\"], stratify=None\n",
        "    )\n",
        "\n",
        "    # Fit preprocessor\n",
        "    print(\"\\nüîß Fitting preprocessor...\")\n",
        "    prep = Preprocessor(\n",
        "        top_k=pre_top_k, use_pca=use_pca, pca_components=pca_components,\n",
        "        numeric_threshold=1.0, force_cat=force_cat_cols or []\n",
        "    ).fit(X_tr_df, y_tr)\n",
        "\n",
        "    # Transform data\n",
        "    print(\"üîÑ Transforming data...\")\n",
        "    X_tr_np  = prep.transform(X_tr_df)\n",
        "    X_val_np = prep.transform(X_val_df)\n",
        "    X_te_np  = prep.transform(X_test_df)\n",
        "\n",
        "    y_tr_np  = np.array(y_tr.values if hasattr(y_tr, 'values') else y_tr, dtype=np.float32, copy=True)\n",
        "    y_val_np = np.array(y_val.values if hasattr(y_val, 'values') else y_val, dtype=np.float32, copy=True)\n",
        "    y_te_np  = np.array(y_test_s.values if hasattr(y_test_s, 'values') else y_test_s, dtype=np.float32, copy=True)\n",
        "\n",
        "    print(f\"‚úì Data shapes: X_train={X_tr_np.shape}, X_val={X_val_np.shape}, X_test={X_te_np.shape}\")\n",
        "\n",
        "    # Setup device and loaders\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"‚úì Using device: {device}\")\n",
        "\n",
        "    train_loader = DataLoader(NPDataset(X_tr_np, y_tr_np), batch_size=cfg[\"batch\"], shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(NPDataset(X_val_np, y_val_np), batch_size=cfg[\"batch\"], shuffle=False, drop_last=False)\n",
        "    test_loader  = DataLoader(NPDataset(X_te_np, y_te_np),   batch_size=cfg[\"batch\"], shuffle=False, drop_last=False)\n",
        "\n",
        "    # Build model\n",
        "    print(f\"\\nüß† Building QNN model (input_dim={X_tr_np.shape[1]}, n_wires={cfg['n_wires']}, n_layers={cfg['n_layers']})...\")\n",
        "    model = QNNRegressor(input_dim=X_tr_np.shape[1], n_wires=cfg[\"n_wires\"], n_layers=cfg[\"n_layers\"], hidden=cfg[\"hidden\"]).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr_start\"])\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    # Training state\n",
        "    best = {\"val_rmse\": float(\"inf\"), \"state\": None, \"val_pred\": None}\n",
        "    patience = 0\n",
        "    history = {\"train_loss\": [], \"val_rmse\": [], \"val_pcc\": [], \"lrs\": []}\n",
        "\n",
        "    print(\"\\nüöÄ Starting training...\\n\")\n",
        "\n",
        "    for epoch in range(cfg[\"epochs\"]):\n",
        "        # Adjust learning rate\n",
        "        lr = cosine_warmup_lr(epoch, cfg)\n",
        "        for g in opt.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        try:\n",
        "            for xb, yb in train_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Forward pass\n",
        "                pred = model(xb)\n",
        "\n",
        "                # Compute combined loss\n",
        "                loss_mse = mse(pred, yb)\n",
        "                loss_corr = corr_loss(pred, yb)\n",
        "                loss = loss_mse + cfg[\"corr_lambda\"] * loss_corr\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "                opt.step()\n",
        "\n",
        "                # Accumulate loss (detach to prevent memory buildup)\n",
        "                epoch_loss += loss.detach().item()\n",
        "                n_batches += 1\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n‚ùå Error during training epoch {epoch+1}: {str(e)}\")\n",
        "            print(\"This might be due to PennyLane/PyTorch compatibility issues.\")\n",
        "            raise\n",
        "\n",
        "        avg_loss = epoch_loss / max(1, n_batches)\n",
        "        history[\"lrs\"].append(lr)\n",
        "        history[\"train_loss\"].append(avg_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        vpred_list, vtrue_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yp = model(xb)\n",
        "\n",
        "                # Safe conversion to numpy\n",
        "                yp_np = yp.cpu().numpy().copy().reshape(-1)\n",
        "                yt_np = yb.cpu().numpy().copy().reshape(-1)\n",
        "\n",
        "                # Convert to Python floats\n",
        "                vpred_list.extend([float(v) for v in yp_np])\n",
        "                vtrue_list.extend([float(v) for v in yt_np])\n",
        "\n",
        "        # Build pure numpy arrays\n",
        "        vpred = np.array(vpred_list, dtype=np.float64)\n",
        "        vtrue = np.array(vtrue_list, dtype=np.float64)\n",
        "        vrmse, vmae, vr2, vpcc = compute_metrics(vtrue, vpred)\n",
        "        history[\"val_rmse\"].append(vrmse)\n",
        "        history[\"val_pcc\"].append(vpcc)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{cfg['epochs']} | TrainLoss {avg_loss:.4f} | \"\n",
        "                  f\"Val RMSE {vrmse:.4f} | Val PCC {vpcc:.4f} | LR {lr:.6f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        improved = vrmse < best[\"val_rmse\"] - 1e-5\n",
        "        if improved:\n",
        "            best[\"val_rmse\"] = vrmse\n",
        "            best[\"state\"] = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            best[\"val_pred\"] = (vtrue.copy(), vpred.copy())\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= cfg[\"patience\"]:\n",
        "                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best[\"state\"])\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    # Test evaluation\n",
        "    print(\"\\nüìä Evaluating on test set...\")\n",
        "    tpred_list, ttrue_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            yp = model(xb)\n",
        "\n",
        "            yp_np = yp.cpu().numpy().copy().reshape(-1)\n",
        "            yt_np = yb.cpu().numpy().copy().reshape(-1)\n",
        "\n",
        "            tpred_list.extend([float(v) for v in yp_np])\n",
        "            ttrue_list.extend([float(v) for v in yt_np])\n",
        "\n",
        "    tpred = np.array(tpred_list, dtype=np.float64)\n",
        "    ttrue = np.array(ttrue_list, dtype=np.float64)\n",
        "    rmse_pre, mae_pre, r2_pre, pcc_pre = compute_metrics(ttrue, tpred)\n",
        "\n",
        "    # Calibration\n",
        "    vtrue, vpred = best[\"val_pred\"]\n",
        "    A = np.vstack([vpred, np.ones_like(vpred)]).T\n",
        "    alpha, beta = np.linalg.lstsq(A, vtrue, rcond=None)[0]\n",
        "    tpred_cal = alpha * tpred + beta\n",
        "\n",
        "    rmse_post, mae_post, r2_post, pcc_post = compute_metrics(ttrue, tpred_cal)\n",
        "\n",
        "    vrmse_pre, _, _, vpcc_pre = compute_metrics(vtrue, vpred)\n",
        "    vpred_cal = alpha * vpred + beta\n",
        "    vrmse_post, _, _, vpcc_post = compute_metrics(vtrue, vpred_cal)\n",
        "    use_cal = (vpcc_post > vpcc_pre) and (vrmse_post <= vrmse_pre + 0.005)\n",
        "\n",
        "    final = {\n",
        "        \"pre\":  {\"rmse\": rmse_pre,  \"mae\": mae_pre,  \"r2\": r2_pre,  \"pcc\": pcc_pre,  \"pred\": tpred,     \"true\": ttrue},\n",
        "        \"post\": {\"rmse\": rmse_post, \"mae\": mae_post, \"r2\": r2_post, \"pcc\": pcc_post, \"pred\": tpred_cal, \"true\": ttrue},\n",
        "        \"use_calibrated\": use_cal,\n",
        "        \"val_pre\":  {\"rmse\": vrmse_pre,  \"pcc\": vpcc_pre},\n",
        "        \"val_post\": {\"rmse\": vrmse_post, \"pcc\": vpcc_post},\n",
        "        \"history\": history,\n",
        "        \"prep\": prep,\n",
        "        \"cfg\": cfg\n",
        "    }\n",
        "\n",
        "    choice = \"Calibrated\" if use_cal else \"Raw\"\n",
        "    chosen = final[\"post\"] if use_cal else final[\"pre\"]\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üéØ QNN TEST ‚Äî Using: {choice}\")\n",
        "    print(f\"RMSE: {chosen['rmse']:.4f} | MAE: {chosen['mae']:.4f} | R¬≤: {chosen['r2']:.4f} | PCC: {chosen['pcc']:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "    return final\n",
        "\n",
        "# ---- config & run ----\n",
        "cfg = {\n",
        "    \"epochs\": 200, \"patience\": 25, \"batch\": 64,\n",
        "    \"lr_start\": 1e-5, \"lr_max\": 1e-3, \"warmup_epochs\": 10,\n",
        "    \"n_wires\": 8, \"n_layers\": 2, \"hidden\": 64,\n",
        "    \"corr_lambda\": 0.10, \"val_size\": 0.2, \"random_state\": 42\n",
        "}\n",
        "\n",
        "force_cat_cols = [c for c in X_train.columns if \"onset\" in c.lower() or \"type\" in c.lower()]\n",
        "\n",
        "artifacts_qnn = run_qnn_training(\n",
        "    X_train_df=X_train, X_test_df=X_test,\n",
        "    y_train_s=y_train, y_test_s=y_test,\n",
        "    cfg=cfg, pre_top_k=25, use_pca=False, pca_components=12,\n",
        "    force_cat_cols=force_cat_cols\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F27DMcoStomL",
        "outputId": "713b14f6-c0c7-40a1-c31e-cf2328116232"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "QNN TRAINING (leak-free)\n",
            "============================================================\n",
            "\n",
            "üîß Fitting preprocessor...\n",
            "üîÑ Transforming data...\n",
            "‚úì Data shapes: X_train=(1551, 25), X_val=(388, 25), X_test=(485, 25)\n",
            "‚úì Using device: cpu\n",
            "\n",
            "üß† Building QNN model (input_dim=25, n_wires=8, n_layers=2)...\n",
            "\n",
            "üöÄ Starting training...\n",
            "\n",
            "Epoch   1/200 | TrainLoss 0.5234 | Val RMSE 0.5718 | Val PCC -0.3305 | LR 0.000010\n",
            "Epoch   5/200 | TrainLoss 0.2908 | Val RMSE 0.4489 | Val PCC 0.4406 | LR 0.000406\n",
            "Epoch  10/200 | TrainLoss 0.2440 | Val RMSE 0.4309 | Val PCC 0.4850 | LR 0.000901\n",
            "Epoch  15/200 | TrainLoss 0.2450 | Val RMSE 0.4289 | Val PCC 0.4974 | LR 0.000999\n",
            "Epoch  20/200 | TrainLoss 0.2331 | Val RMSE 0.4248 | Val PCC 0.5066 | LR 0.000994\n",
            "Epoch  25/200 | TrainLoss 0.2382 | Val RMSE 0.4237 | Val PCC 0.5084 | LR 0.000987\n",
            "Epoch  30/200 | TrainLoss 0.2427 | Val RMSE 0.4247 | Val PCC 0.5095 | LR 0.000976\n",
            "Epoch  35/200 | TrainLoss 0.2316 | Val RMSE 0.4240 | Val PCC 0.5123 | LR 0.000961\n",
            "Epoch  40/200 | TrainLoss 0.2318 | Val RMSE 0.4226 | Val PCC 0.5121 | LR 0.000944\n",
            "Epoch  45/200 | TrainLoss 0.2393 | Val RMSE 0.4236 | Val PCC 0.5116 | LR 0.000923\n",
            "Epoch  50/200 | TrainLoss 0.2272 | Val RMSE 0.4224 | Val PCC 0.5145 | LR 0.000900\n",
            "Epoch  55/200 | TrainLoss 0.2318 | Val RMSE 0.4240 | Val PCC 0.5105 | LR 0.000873\n",
            "Epoch  60/200 | TrainLoss 0.2291 | Val RMSE 0.4220 | Val PCC 0.5131 | LR 0.000845\n",
            "Epoch  65/200 | TrainLoss 0.2275 | Val RMSE 0.4238 | Val PCC 0.5133 | LR 0.000814\n",
            "Epoch  70/200 | TrainLoss 0.2319 | Val RMSE 0.4244 | Val PCC 0.5109 | LR 0.000780\n",
            "Epoch  75/200 | TrainLoss 0.2369 | Val RMSE 0.4236 | Val PCC 0.5131 | LR 0.000745\n",
            "Epoch  80/200 | TrainLoss 0.2329 | Val RMSE 0.4218 | Val PCC 0.5137 | LR 0.000708\n",
            "Epoch  85/200 | TrainLoss 0.2259 | Val RMSE 0.4224 | Val PCC 0.5142 | LR 0.000670\n",
            "Epoch  90/200 | TrainLoss 0.2294 | Val RMSE 0.4228 | Val PCC 0.5141 | LR 0.000631\n",
            "Epoch  95/200 | TrainLoss 0.2421 | Val RMSE 0.4217 | Val PCC 0.5136 | LR 0.000590\n",
            "\n",
            "‚èπÔ∏è  Early stopping at epoch 97\n",
            "\n",
            "üìä Evaluating on test set...\n",
            "\n",
            "============================================================\n",
            "üéØ QNN TEST ‚Äî Using: Raw\n",
            "RMSE: 0.4445 | MAE: 0.3055 | R¬≤: 0.2511 | PCC: 0.5120\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 Preprocessing"
      ],
      "metadata": {
        "id": "6hjgx2oFbnkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua5B00YFOE8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725e23fe-8a53-472e-d97b-5b3795903fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 2: BREAKTHROUGH PREPROCESSING\n",
            "============================================================\n",
            "\n",
            "üéØ Filtering samples with valid target...\n",
            "‚úÖ Removed 3 samples without target\n",
            "‚úÖ Remaining samples: 2,439\n",
            "\n",
            "üìä Target statistics (ORIGINAL SCALE):\n",
            "  - Mean: -0.3881\n",
            "  - Std: 0.4965\n",
            "  - Range: [-3.1000, 1.0526]\n",
            "\n",
            "üè∑Ô∏è  Encoding categorical variables...\n",
            "‚úÖ Found 3 categorical columns: ['Site_of_Onset', 'Subject_used_Riluzole', 'Sex']\n",
            "\n",
            "üîß Handling missing values...\n",
            "  üìâ Dropping 295 columns with >50% missing\n",
            "  üîπ Applying median imputation...\n",
            "‚úÖ Missing value handling complete. Shape: (2439, 222)\n",
            "\n",
            "üîÄ Creating interaction features...\n",
            "‚úÖ Created 2 interaction features\n",
            "\n",
            "üìä Removing low-variance features...\n",
            "‚úÖ After variance threshold: 223 features\n",
            "\n",
            "üéØ Multi-method feature selection (Top 25)...\n",
            "  üå≤ Random Forest importance...\n",
            "  üìä Mutual Information...\n",
            "  üìà Pearson correlation...\n",
            "\n",
            "  ‚úÖ Selected top 25 features\n",
            "  üèÜ Top 10 features:\n",
            "      1. ALSFRS_Total_orig_first\n",
            "      2. ALSFRS_Total_orig_q25\n",
            "      3. ALSFRS_Total_orig_last\n",
            "      4. ALSFRS_Total_orig_max\n",
            "      5. ALSFRS_Total_orig_mean\n",
            "      6. ALSFRS_Total_orig_q75\n",
            "      7. ALSFRS_Total_slope\n",
            "      8. ALSFRS_Total_orig_slope\n",
            "      9. ALSFRS_Total_orig_min\n",
            "     10. FVC_slope\n",
            "\n",
            "‚öñÔ∏è  Applying Robust Scaling to features...\n",
            "‚úÖ Features scaled. Shape: (2439, 25)\n",
            "   Mean range: [-0.5012, 0.4892]\n",
            "\n",
            "‚úÇÔ∏è  Creating train-test split...\n",
            "‚úÖ Train set: 1951 samples\n",
            "‚úÖ Test set: 488 samples\n",
            "‚úÖ Features: 25\n",
            "\n",
            "============================================================\n",
            "‚úÖ STEP 2 COMPLETED\n",
            "============================================================\n",
            "\n",
            "üìä Preprocessing Summary:\n",
            "  - Training samples: 1,951\n",
            "  - Test samples: 488\n",
            "  - Selected features: 25\n",
            "  - Target: ORIGINAL SCALE (no scaling!)\n",
            "  - Target mean: -0.3881, std: 0.4965\n",
            "  - Target range: [-3.1000, 1.0526]\n",
            "\n",
            "üîç Feature preview:\n",
            "        ALSFRS_Total_orig_first  ALSFRS_Total_orig_q25  \\\n",
            "152744                 0.068966               0.035714   \n",
            "485527                -0.758621              -0.732143   \n",
            "412103                -0.758621              -0.767857   \n",
            "\n",
            "        ALSFRS_Total_orig_last  ALSFRS_Total_orig_max  ALSFRS_Total_orig_mean  \\\n",
            "152744                0.071429               0.033333                0.023392   \n",
            "485527               -0.714286              -0.766667               -0.748538   \n",
            "412103               -0.750000              -0.766667               -0.776608   \n",
            "\n",
            "        ALSFRS_Total_orig_q75  ALSFRS_Total_slope  ALSFRS_Total_orig_slope  \\\n",
            "152744               0.017241           -1.698133                -1.139535   \n",
            "485527              -0.758621            0.000000                 0.000000   \n",
            "412103              -0.793103            0.000000                -0.576471   \n",
            "\n",
            "        ALSFRS_Total_orig_min  FVC_slope  ...  ALSFRS_Total_orig_std  \\\n",
            "152744               0.037037   0.381576  ...               0.817697   \n",
            "485527              -0.703704   0.000000  ...              -0.447214   \n",
            "412103              -0.740741   0.000000  ...              -0.100803   \n",
            "\n",
            "        Q5a_Cutting_without_Gastrostomy_std   FVC_std  Site_of_Onset  \\\n",
            "152744                             1.054093 -0.687966            0.0   \n",
            "485527                             1.000000  0.000000            0.0   \n",
            "412103                             0.000000  0.000000            0.0   \n",
            "\n",
            "        Blood_Pressure_Diastolic_last  Blood_Pressure_Diastolic_min  \\\n",
            "152744                      -0.434783                          -0.4   \n",
            "485527                       0.000000                           0.0   \n",
            "412103                       0.000000                           0.0   \n",
            "\n",
            "        Blood_Pressure_Diastolic_max  Q1_Speech_mean_x_Q3_Swallowing_mean  \\\n",
            "152744                          -1.3                            -0.095238   \n",
            "485527                           0.0                            -0.906667   \n",
            "412103                           0.0                            -1.238095   \n",
            "\n",
            "        Q9_Climbing_Stairs_mean  Blood_Pressure_Systolic_last  \n",
            "152744                    -0.40                     -0.555556  \n",
            "485527                     0.08                      0.000000  \n",
            "412103                    -0.40                      0.000000  \n",
            "\n",
            "[3 rows x 25 columns]\n",
            "\n",
            "üéØ Target preview:\n",
            "[ 0.29900332  0.         -0.23076923 -1.07142857  0.10714286  0.\n",
            "  0.         -0.3125     -0.21505376  0.34482759]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import mutual_info_regression, VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: BREAKTHROUGH PREPROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Filter valid samples\n",
        "# -------------------------------\n",
        "print(\"\\nüéØ Filtering samples with valid target...\")\n",
        "initial_count = len(features_df)\n",
        "features_df = features_df[features_df['ALSFRS_slope_3to12m'].notna()].copy()\n",
        "print(f\"‚úÖ Removed {initial_count - len(features_df)} samples without target\")\n",
        "print(f\"‚úÖ Remaining samples: {len(features_df):,}\")\n",
        "\n",
        "# Separate features and target (ORIGINAL SCALE!)\n",
        "y = features_df['ALSFRS_slope_3to12m'].copy()\n",
        "X = features_df.drop('ALSFRS_slope_3to12m', axis=1)\n",
        "\n",
        "print(f\"\\nüìä Target statistics (ORIGINAL SCALE):\")\n",
        "print(f\"  - Mean: {y.mean():.4f}\")\n",
        "print(f\"  - Std: {y.std():.4f}\")\n",
        "print(f\"  - Range: [{y.min():.4f}, {y.max():.4f}]\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Handle categorical variables\n",
        "# -------------------------------\n",
        "print(\"\\nüè∑Ô∏è  Encoding categorical variables...\")\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"‚úÖ Found {len(categorical_cols)} categorical columns: {categorical_cols}\")\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = X[col].fillna('Missing')\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Quick missing value handling\n",
        "# -------------------------------\n",
        "print(\"\\nüîß Handling missing values...\")\n",
        "\n",
        "# Drop >50% missing\n",
        "missing_pct = X.isnull().sum() / len(X)\n",
        "cols_to_drop = missing_pct[missing_pct > 0.5].index.tolist()\n",
        "print(f\"  üìâ Dropping {len(cols_to_drop)} columns with >50% missing\")\n",
        "X = X.drop(columns=cols_to_drop)\n",
        "\n",
        "# Simple median imputation (fast, memory-efficient)\n",
        "print(f\"  üîπ Applying median imputation...\")\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().any():\n",
        "        X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "X = X.fillna(0)\n",
        "print(f\"‚úÖ Missing value handling complete. Shape: {X.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Create critical interaction features\n",
        "# -------------------------------\n",
        "print(\"\\nüîÄ Creating interaction features...\")\n",
        "interaction_pairs = [\n",
        "    ('Q5_Cutting_mean', 'Weight_median'),\n",
        "    ('FVC_mean', 'Respiratory_Rate_mean'),\n",
        "    ('Q1_Speech_mean', 'Q3_Swallowing_mean'),\n",
        "]\n",
        "\n",
        "created = 0\n",
        "for col1, col2 in interaction_pairs:\n",
        "    if col1 in X.columns and col2 in X.columns:\n",
        "        X[f'{col1}_x_{col2}'] = X[col1] * X[col2]\n",
        "        created += 1\n",
        "print(f\"‚úÖ Created {created} interaction features\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Variance threshold\n",
        "# -------------------------------\n",
        "print(\"\\nüìä Removing low-variance features...\")\n",
        "var_selector = VarianceThreshold(threshold=0.01)\n",
        "X = X[X.columns[var_selector.fit(X).get_support()]]\n",
        "print(f\"‚úÖ After variance threshold: {X.shape[1]} features\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Multi-method feature selection (Top 25)\n",
        "# -------------------------------\n",
        "print(\"\\nüéØ Multi-method feature selection (Top 25)...\")\n",
        "\n",
        "# Method 1: Random Forest (50% weight)\n",
        "print(\"  üå≤ Random Forest importance...\")\n",
        "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
        "rf.fit(X, y)\n",
        "rf_importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Method 2: Mutual Information (30% weight)\n",
        "print(\"  üìä Mutual Information...\")\n",
        "mi_scores = mutual_info_regression(X, y, random_state=42, n_neighbors=5)\n",
        "mi_importance = pd.Series(mi_scores, index=X.columns)\n",
        "\n",
        "# Method 3: Pearson Correlation (20% weight)\n",
        "print(\"  üìà Pearson correlation...\")\n",
        "corr_importance = X.corrwith(y).abs()\n",
        "\n",
        "# Weighted ensemble ranking\n",
        "rank_rf = rf_importance.rank(ascending=False)\n",
        "rank_mi = mi_importance.rank(ascending=False)\n",
        "rank_corr = corr_importance.rank(ascending=False)\n",
        "\n",
        "combined_rank = (0.5 * rank_rf + 0.3 * rank_mi + 0.2 * rank_corr)\n",
        "combined_rank = combined_rank.sort_values()\n",
        "\n",
        "# Select top 25 features\n",
        "n_features_select = 25\n",
        "top_25_features = combined_rank.head(n_features_select).index.tolist()\n",
        "\n",
        "print(f\"\\n  ‚úÖ Selected top {n_features_select} features\")\n",
        "print(f\"  üèÜ Top 10 features:\")\n",
        "for i, feat in enumerate(top_25_features[:10], 1):\n",
        "    print(f\"     {i:2d}. {feat}\")\n",
        "\n",
        "X_selected = X[top_25_features].copy()\n",
        "\n",
        "# Clear memory\n",
        "del X, rf\n",
        "gc.collect()\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Robust scaling for features ONLY\n",
        "# -------------------------------\n",
        "print(\"\\n‚öñÔ∏è  Applying Robust Scaling to features...\")\n",
        "scaler = RobustScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Features scaled. Shape: {X_scaled.shape}\")\n",
        "print(f\"   Mean range: [{X_scaled.mean().min():.4f}, {X_scaled.mean().max():.4f}]\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Train-test split (NO TARGET SCALING!)\n",
        "# -------------------------------\n",
        "print(\"\\n‚úÇÔ∏è  Creating train-test split...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"‚úÖ Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"‚úÖ Features: {X_train.shape[1]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Save preprocessing artifacts\n",
        "# -------------------------------\n",
        "preprocessing_artifacts = {\n",
        "    'scaler': scaler,\n",
        "    'selected_features': top_25_features,\n",
        "    'label_encoders': label_encoders\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ STEP 2 COMPLETED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Preprocessing Summary:\")\n",
        "print(f\"  - Training samples: {len(X_train):,}\")\n",
        "print(f\"  - Test samples: {len(X_test):,}\")\n",
        "print(f\"  - Selected features: {n_features_select}\")\n",
        "print(f\"  - Target: ORIGINAL SCALE (no scaling!)\")\n",
        "print(f\"  - Target mean: {y.mean():.4f}, std: {y.std():.4f}\")\n",
        "print(f\"  - Target range: [{y.min():.4f}, {y.max():.4f}]\")\n",
        "\n",
        "print(\"\\nüîç Feature preview:\")\n",
        "print(X_train.head(3))\n",
        "print(\"\\nüéØ Target preview:\")\n",
        "print(y_train.head(10).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qnn Model"
      ],
      "metadata": {
        "id": "H0vlPbHhbvb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.decomposition import PCA\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: BREAKTHROUGH QNN MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Configuration (Optimized for breakthrough)\n",
        "# -------------------------------\n",
        "print(\"\\n‚öôÔ∏è  Breakthrough Configuration...\")\n",
        "\n",
        "config = {\n",
        "    'n_features': X_train.shape[1],  # 25 features\n",
        "    'n_qubits': 12,  # More than 10, less RAM than 18\n",
        "    'n_layers': 4,   # Deeper than 3 for more expressiveness\n",
        "    'batch_size': 24,  # Balance between speed and memory\n",
        "    'learning_rate_start': 0.0001,  # Very low start for warmup\n",
        "    'learning_rate_max': 0.003,     # Peak learning rate\n",
        "    'n_epochs': 100,\n",
        "    'warmup_epochs': 10,\n",
        "    'early_stopping_patience': 20,\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. PCA with Better Variance Retention\n",
        "# -------------------------------\n",
        "print(f\"\\nüî¨ Applying PCA: {config['n_features']} ‚Üí {config['n_qubits']} dimensions\")\n",
        "\n",
        "pca = PCA(n_components=config['n_qubits'], random_state=42)\n",
        "X_train_qnn = pca.fit_transform(X_train)\n",
        "X_test_qnn = pca.transform(X_test)\n",
        "\n",
        "explained_var = pca.explained_variance_ratio_.sum()\n",
        "print(f\"‚úÖ Explained variance: {explained_var:.1%}\")\n",
        "print(f\"‚úÖ Quantum input shape: {X_train_qnn.shape}\")\n",
        "\n",
        "# Add to preprocessing artifacts\n",
        "preprocessing_artifacts['pca'] = pca\n",
        "\n",
        "# Clear memory\n",
        "del X_train, X_test\n",
        "gc.collect()\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Enhanced Quantum Circuit\n",
        "# -------------------------------\n",
        "print(\"\\nüîÆ Building enhanced quantum circuit...\")\n",
        "\n",
        "dev = qml.device('default.qubit', wires=config['n_qubits'])\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    \"\"\"\n",
        "    Enhanced quantum circuit with:\n",
        "    - Double rotation encoding\n",
        "    - 3-parameter rotations (RX, RY, RZ)\n",
        "    - Ring + Pairwise entanglement\n",
        "    \"\"\"\n",
        "    n_wires = len(inputs)\n",
        "\n",
        "    # Normalize input\n",
        "    norm = torch.sqrt(torch.sum(inputs**2)) + 1e-8\n",
        "    normalized_inputs = inputs / norm\n",
        "\n",
        "    # Enhanced data encoding (2 rotations per qubit)\n",
        "    for i in range(n_wires):\n",
        "        qml.RY(normalized_inputs[i] * np.pi, wires=i)\n",
        "        qml.RZ(normalized_inputs[i] * np.pi / 2, wires=i)\n",
        "\n",
        "    # Apply parameterized layers\n",
        "    for layer_idx in range(weights.shape[0]):\n",
        "        # 3-parameter rotations for maximum expressiveness\n",
        "        for i in range(n_wires):\n",
        "            qml.RX(weights[layer_idx, i, 0], wires=i)\n",
        "            qml.RY(weights[layer_idx, i, 1], wires=i)\n",
        "            qml.RZ(weights[layer_idx, i, 2], wires=i)\n",
        "\n",
        "        # Strong entanglement: Ring topology\n",
        "        for i in range(n_wires):\n",
        "            qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
        "\n",
        "        # Additional pairwise entanglement\n",
        "        for i in range(0, n_wires - 1, 2):\n",
        "            qml.CNOT(wires=[i, i + 1])\n",
        "\n",
        "    # Measure all qubits\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_wires)]\n",
        "\n",
        "print(f\"‚úÖ Quantum circuit created:\")\n",
        "print(f\"   - Qubits: {config['n_qubits']}\")\n",
        "print(f\"   - Layers: {config['n_layers']}\")\n",
        "print(f\"   - Encoding: Double rotation (RY + RZ)\")\n",
        "print(f\"   - Rotations per layer: 3 (RX, RY, RZ)\")\n",
        "print(f\"   - Entanglement: Ring + Pairwise\")\n",
        "print(f\"   - Total quantum params: {config['n_layers'] * config['n_qubits'] * 3}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Hybrid QNN Model with Deeper Classical Head\n",
        "# -------------------------------\n",
        "print(\"\\nüß¨ Building hybrid quantum-classical model...\")\n",
        "\n",
        "class BreakthroughQNN(nn.Module):\n",
        "    \"\"\"Enhanced hybrid model with deeper classical processing\"\"\"\n",
        "\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Quantum weights (3 parameters per qubit per layer)\n",
        "        self.q_weights = nn.Parameter(\n",
        "            torch.randn(n_layers, n_qubits, 3, requires_grad=True) * 0.05\n",
        "        )\n",
        "\n",
        "        # Deeper classical post-processing with BatchNorm\n",
        "        self.fc1 = nn.Linear(n_qubits, 32)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc3 = nn.Linear(16, 8)\n",
        "        self.bn3 = nn.BatchNorm1d(8)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc4 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Quantum processing\n",
        "        quantum_outputs = []\n",
        "        for i in range(batch_size):\n",
        "            q_out = quantum_circuit(x[i], self.q_weights)\n",
        "            quantum_outputs.append(torch.stack(q_out))\n",
        "\n",
        "        quantum_output = torch.stack(quantum_outputs).float()\n",
        "\n",
        "        # Deep classical post-processing\n",
        "        x = torch.relu(self.bn1(self.fc1(quantum_output)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x.squeeze()\n",
        "\n",
        "# Initialize model\n",
        "model = BreakthroughQNN(\n",
        "    n_qubits=config['n_qubits'],\n",
        "    n_layers=config['n_layers']\n",
        ").float()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "quantum_params = config['n_layers'] * config['n_qubits'] * 3\n",
        "classical_params = total_params - quantum_params\n",
        "\n",
        "print(f\"‚úÖ Hybrid QNN model created\")\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"   - Quantum parameters: {quantum_params:,}\")\n",
        "print(f\"   - Classical parameters: {classical_params:,}\")\n",
        "print(f\"   - Total trainable parameters: {total_params:,}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Training Setup\n",
        "# -------------------------------\n",
        "print(\"\\n‚öôÔ∏è  Training setup...\")\n",
        "\n",
        "# Huber loss (robust to outliers in ALS data)\n",
        "criterion = nn.HuberLoss(delta=0.5)\n",
        "print(f\"‚úÖ Loss function: Huber Loss (delta=0.5, robust to outliers)\")\n",
        "\n",
        "# AdamW optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['learning_rate_start'],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "print(f\"‚úÖ Optimizer: AdamW (weight_decay=1e-4)\")\n",
        "\n",
        "# Learning rate scheduler (will be applied in training)\n",
        "print(f\"‚úÖ LR Schedule: Warmup ({config['warmup_epochs']} epochs) + Cosine decay\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Prepare Data Loaders\n",
        "# -------------------------------\n",
        "print(\"\\nüì¶ Preparing data loaders...\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_qnn)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values)\n",
        "X_test_tensor = torch.FloatTensor(X_test_qnn)\n",
        "y_test_tensor = torch.FloatTensor(y_test.values)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create dataloaders with moderate batch size\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train batches: {len(train_loader)}\")\n",
        "print(f\"‚úÖ Test batches: {len(test_loader)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Test Forward Pass\n",
        "# -------------------------------\n",
        "print(\"\\nüß™ Testing model forward pass...\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_batch = X_train_tensor[:2]\n",
        "    try:\n",
        "        output = model(sample_batch)\n",
        "        print(f\"‚úÖ Forward pass successful!\")\n",
        "        print(f\"   Input shape: {sample_batch.shape}\")\n",
        "        print(f\"   Output shape: {output.shape}\")\n",
        "        print(f\"   Output dtype: {output.dtype}\")\n",
        "        print(f\"   Sample outputs: {output.numpy()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in forward pass: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ STEP 3 COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìã Model Summary:\")\n",
        "print(f\"  - Architecture: Hybrid Quantum-Classical\")\n",
        "print(f\"  - Quantum: {config['n_qubits']} qubits, {config['n_layers']} layers\")\n",
        "print(f\"  - Encoding: Double rotation (RY + RZ)\")\n",
        "print(f\"  - Entanglement: Ring + Pairwise\")\n",
        "print(f\"  - Classical: 4 layers (32‚Üí16‚Üí8‚Üí1) with BatchNorm & Dropout\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Data: 1,951 train, 488 test\")\n",
        "print(f\"  - Ready for training: ‚úÖ\")"
      ],
      "metadata": {
        "id": "w9Oh2f19s2m2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74c3a92-39a0-4aad-dcfb-0669461f3210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 3: BREAKTHROUGH QNN MODEL\n",
            "============================================================\n",
            "\n",
            "‚öôÔ∏è  Breakthrough Configuration...\n",
            "‚úÖ Configuration:\n",
            "   n_features: 25\n",
            "   n_qubits: 12\n",
            "   n_layers: 4\n",
            "   batch_size: 24\n",
            "   learning_rate_start: 0.0001\n",
            "   learning_rate_max: 0.003\n",
            "   n_epochs: 100\n",
            "   warmup_epochs: 10\n",
            "   early_stopping_patience: 20\n",
            "\n",
            "üî¨ Applying PCA: 25 ‚Üí 12 dimensions\n",
            "‚úÖ Explained variance: 98.8%\n",
            "‚úÖ Quantum input shape: (1951, 12)\n",
            "\n",
            "üîÆ Building enhanced quantum circuit...\n",
            "‚úÖ Quantum circuit created:\n",
            "   - Qubits: 12\n",
            "   - Layers: 4\n",
            "   - Encoding: Double rotation (RY + RZ)\n",
            "   - Rotations per layer: 3 (RX, RY, RZ)\n",
            "   - Entanglement: Ring + Pairwise\n",
            "   - Total quantum params: 144\n",
            "\n",
            "üß¨ Building hybrid quantum-classical model...\n",
            "‚úÖ Hybrid QNN model created\n",
            "\n",
            "üìä Model Parameters:\n",
            "   - Quantum parameters: 144\n",
            "   - Classical parameters: 1,201\n",
            "   - Total trainable parameters: 1,345\n",
            "\n",
            "‚öôÔ∏è  Training setup...\n",
            "‚úÖ Loss function: Huber Loss (delta=0.5, robust to outliers)\n",
            "‚úÖ Optimizer: AdamW (weight_decay=1e-4)\n",
            "‚úÖ LR Schedule: Warmup (10 epochs) + Cosine decay\n",
            "\n",
            "üì¶ Preparing data loaders...\n",
            "‚úÖ Train batches: 81\n",
            "‚úÖ Test batches: 21\n",
            "\n",
            "üß™ Testing model forward pass...\n",
            "‚úÖ Forward pass successful!\n",
            "   Input shape: torch.Size([2, 12])\n",
            "   Output shape: torch.Size([2])\n",
            "   Output dtype: torch.float32\n",
            "   Sample outputs: [-0.07392714 -0.06184057]\n",
            "\n",
            "============================================================\n",
            "‚úÖ STEP 3 COMPLETED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "üìã Model Summary:\n",
            "  - Architecture: Hybrid Quantum-Classical\n",
            "  - Quantum: 12 qubits, 4 layers\n",
            "  - Encoding: Double rotation (RY + RZ)\n",
            "  - Entanglement: Ring + Pairwise\n",
            "  - Classical: 4 layers (32‚Üí16‚Üí8‚Üí1) with BatchNorm & Dropout\n",
            "  - Total parameters: 1,345\n",
            "  - Data: 1,951 train, 488 test\n",
            "  - Ready for training: ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qnn Training"
      ],
      "metadata": {
        "id": "IR0DUBRKb2pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: BREAKTHROUGH TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Learning Rate Scheduler Function\n",
        "# -------------------------------\n",
        "def get_lr(epoch, config):\n",
        "    \"\"\"Warmup + Cosine annealing learning rate schedule\"\"\"\n",
        "    if epoch < config['warmup_epochs']:\n",
        "        # Linear warmup\n",
        "        return config['learning_rate_start'] + \\\n",
        "               (config['learning_rate_max'] - config['learning_rate_start']) * \\\n",
        "               (epoch / config['warmup_epochs'])\n",
        "    else:\n",
        "        # Cosine annealing\n",
        "        progress = (epoch - config['warmup_epochs']) / \\\n",
        "                   (config['n_epochs'] - config['warmup_epochs'])\n",
        "        return config['learning_rate_max'] * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Training and Evaluation Functions\n",
        "# -------------------------------\n",
        "def train_epoch(model, loader, criterion, optimizer, epoch, config):\n",
        "    \"\"\"Train for one epoch with dynamic learning rate\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Update learning rate\n",
        "    lr = get_lr(epoch, config)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch_x)\n",
        "        loss = criterion(pred, batch_y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (tighter for stability)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Clear memory\n",
        "        del pred, loss\n",
        "\n",
        "    gc.collect()\n",
        "    return total_loss / len(loader), lr\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    \"\"\"Evaluate model on dataset\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in loader:\n",
        "            pred = model(batch_x)\n",
        "            predictions.extend(pred.numpy())\n",
        "            actuals.extend(batch_y.numpy())\n",
        "            del pred\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    actuals = np.array(actuals)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "    pcc, _ = pearsonr(actuals, predictions)\n",
        "\n",
        "    return rmse, mae, r2, pcc, predictions, actuals\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Training Loop with Advanced Strategy\n",
        "# -------------------------------\n",
        "print(\"\\nüöÄ Starting breakthrough training...\")\n",
        "print(f\"   Target: RMSE < 0.35, PCC > 0.65\")\n",
        "print(f\"   Strategy: Warmup + Cosine decay + Heavy RMSE focus\")\n",
        "print(f\"   Epochs: {config['n_epochs']}\")\n",
        "print(f\"   Patience: {config['early_stopping_patience']}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Tracking\n",
        "best_score = -float('inf')\n",
        "best_rmse = float('inf')\n",
        "best_pcc = -1\n",
        "best_mae = float('inf')\n",
        "best_r2 = -1\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'test_rmse': [],\n",
        "    'test_pcc': [],\n",
        "    'learning_rates': []\n",
        "}\n",
        "\n",
        "for epoch in range(config['n_epochs']):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss, lr = train_epoch(model, train_loader, criterion, optimizer, epoch, config)\n",
        "\n",
        "    # Evaluate\n",
        "    rmse, mae, r2, pcc, _, _ = evaluate(model, test_loader)\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['test_rmse'].append(rmse)\n",
        "    history['test_pcc'].append(pcc)\n",
        "    history['learning_rates'].append(lr)\n",
        "\n",
        "    # Combined score with HEAVY weight on RMSE (2x)\n",
        "    # Lower RMSE is better, higher PCC is better\n",
        "    combined_score = -2.0 * rmse + pcc\n",
        "\n",
        "    # Track best model\n",
        "    if combined_score > best_score:\n",
        "        best_score = combined_score\n",
        "        best_rmse = rmse\n",
        "        best_pcc = pcc\n",
        "        best_mae = mae\n",
        "        best_r2 = r2\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{config['n_epochs']} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"RMSE: {rmse:.4f} | \"\n",
        "              f\"MAE: {mae:.4f} | \"\n",
        "              f\"PCC: {pcc:.4f} | \"\n",
        "              f\"R¬≤: {r2:.4f} | \"\n",
        "              f\"LR: {lr:.6f} | \"\n",
        "              f\"Time: {epoch_time:.1f}s\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config['early_stopping_patience']:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "        print(f\"   No improvement for {config['early_stopping_patience']} epochs\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"-\" * 60)\n",
        "print(f\"‚úÖ Training completed in {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Load Best Model and Final Evaluation\n",
        "# -------------------------------\n",
        "print(\"\\nüìä Loading best model and final evaluation...\")\n",
        "\n",
        "model.load_state_dict(best_model_state)\n",
        "final_rmse, final_mae, final_r2, final_pcc, predictions, actuals = evaluate(model, test_loader)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üèÜ BREAKTHROUGH RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä Final Test Performance:\")\n",
        "print(f\"   RMSE: {final_rmse:.4f}\")\n",
        "print(f\"   MAE: {final_mae:.4f}\")\n",
        "print(f\"   R¬≤ Score: {final_r2:.4f}\")\n",
        "print(f\"   Pearson Correlation (PCC): {final_pcc:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Compare with Targets\n",
        "# -------------------------------\n",
        "print(f\"\\nüìà Performance vs Targets:\")\n",
        "print(f\"   Target RMSE: < 0.35\")\n",
        "print(f\"   Achieved RMSE: {final_rmse:.4f}\")\n",
        "\n",
        "if final_rmse < 0.35:\n",
        "    improvement = ((0.58 - final_rmse) / 0.58) * 100\n",
        "    print(f\"   ‚úÖ RMSE TARGET ACHIEVED!\")\n",
        "    print(f\"   üéâ {improvement:.1f}% improvement from original (0.58)\")\n",
        "else:\n",
        "    gap = final_rmse - 0.35\n",
        "    reduction_from_041 = ((0.41 - final_rmse) / 0.41) * 100\n",
        "    print(f\"   üìç Gap to target: {gap:.4f}\")\n",
        "    print(f\"   üìâ Reduced from 0.41: {reduction_from_041:.1f}%\")\n",
        "\n",
        "print(f\"\\n   Target PCC: > 0.65\")\n",
        "print(f\"   Achieved PCC: {final_pcc:.4f}\")\n",
        "\n",
        "if final_pcc > 0.65:\n",
        "    print(f\"   ‚úÖ PCC TARGET ACHIEVED!\")\n",
        "else:\n",
        "    gap = 0.65 - final_pcc\n",
        "    print(f\"   üìç Gap to target: {gap:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Training History Analysis\n",
        "# -------------------------------\n",
        "print(f\"\\nüìä Training Statistics:\")\n",
        "print(f\"   Total epochs: {len(history['train_loss'])}\")\n",
        "print(f\"   Best RMSE: {best_rmse:.4f}\")\n",
        "print(f\"   Best PCC: {best_pcc:.4f}\")\n",
        "print(f\"   Best MAE: {best_mae:.4f}\")\n",
        "print(f\"   Best R¬≤: {best_r2:.4f}\")\n",
        "print(f\"   Final LR: {history['learning_rates'][-1]:.6f}\")\n",
        "print(f\"   Training time: {total_time/60:.1f} minutes\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Sample Predictions Analysis\n",
        "# -------------------------------\n",
        "print(\"\\nüîç Detailed Prediction Analysis (first 15 samples):\")\n",
        "print(f\"{'Actual':>10} | {'Predicted':>10} | {'Error':>10} | {'% Error':>10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i in range(min(15, len(actuals))):\n",
        "    actual = actuals[i]\n",
        "    pred = predictions[i]\n",
        "    error = actual - pred\n",
        "    pct_error = (error / actual * 100) if actual != 0 else 0\n",
        "    print(f\"{actual:>10.4f} | {pred:>10.4f} | {error:>10.4f} | {pct_error:>9.1f}%\")\n",
        "\n",
        "# Error statistics\n",
        "errors = actuals - predictions\n",
        "print(f\"\\nüìä Error Statistics:\")\n",
        "print(f\"   Mean error: {errors.mean():.4f}\")\n",
        "print(f\"   Std error: {errors.std():.4f}\")\n",
        "print(f\"   Max overestimation: {errors.max():.4f}\")\n",
        "print(f\"   Max underestimation: {errors.min():.4f}\")\n",
        "print(f\"   Median absolute error: {np.median(np.abs(errors)):.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Save Model and Artifacts\n",
        "# -------------------------------\n",
        "print(\"\\nüíæ Saving model and artifacts...\")\n",
        "\n",
        "final_artifacts = {\n",
        "    'model_state': best_model_state,\n",
        "    'config': config,\n",
        "    'preprocessing': preprocessing_artifacts,\n",
        "    'history': history,\n",
        "    'best_metrics': {\n",
        "        'rmse': best_rmse,\n",
        "        'mae': best_mae,\n",
        "        'r2': best_r2,\n",
        "        'pcc': best_pcc\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Artifacts ready for saving\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ STEP 4 COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüéâ BREAKTHROUGH TRAINING COMPLETE!\")\n",
        "print(f\"\\nüéØ Final Summary:\")\n",
        "print(f\"   Original Model:  RMSE=0.58, PCC=0.706\")\n",
        "print(f\"   Previous Best:   RMSE=0.41, PCC=0.51\")\n",
        "print(f\"   This Model:      RMSE={final_rmse:.4f}, PCC={final_pcc:.4f}\")\n",
        "\n",
        "if final_rmse < 0.35 and final_pcc > 0.65:\n",
        "    print(f\"\\nüèÜ BOTH TARGETS ACHIEVED! üèÜ\")\n",
        "elif final_rmse < 0.35:\n",
        "    print(f\"\\n‚úÖ RMSE TARGET ACHIEVED!\")\n",
        "    print(f\"üìç PCC needs {0.65 - final_pcc:.4f} more improvement\")\n",
        "elif final_pcc > 0.65:\n",
        "    print(f\"\\n‚úÖ PCC TARGET ACHIEVED!\")\n",
        "    print(f\"üìç RMSE needs {final_rmse - 0.35:.4f} more reduction\")\n",
        "else:\n",
        "    print(f\"\\nüìç Close but not yet achieved:\")\n",
        "    print(f\"   RMSE gap: {final_rmse - 0.35:.4f}\")\n",
        "    print(f\"   PCC gap: {0.65 - final_pcc:.4f}\")\n",
        "    print(f\"\\nüí° Next steps for improvement:\")\n",
        "    print(f\"   - Try ensemble of multiple QNN models\")\n",
        "    print(f\"   - Experiment with different quantum encodings\")\n",
        "    print(f\"   - Adjust hyperparameters (layers, qubits)\")\n",
        "    print(f\"   - Consider data augmentation techniques\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ],
      "metadata": {
        "id": "fQuVa6F4OM9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c236480-01d2-4992-bc2f-f6c024bfad4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 4: BREAKTHROUGH TRAINING\n",
            "============================================================\n",
            "\n",
            "üöÄ Starting breakthrough training...\n",
            "   Target: RMSE < 0.35, PCC > 0.65\n",
            "   Strategy: Warmup + Cosine decay + Heavy RMSE focus\n",
            "   Epochs: 100\n",
            "   Patience: 20\n",
            "------------------------------------------------------------\n",
            "Epoch   1/100 | Loss: 0.1545 | RMSE: 0.5604 | MAE: 0.3861 | PCC: 0.1872 | R¬≤: -0.3337 | LR: 0.000100 | Time: 479.4s\n",
            "Epoch   5/100 | Loss: 0.0830 | RMSE: 0.4295 | MAE: 0.3015 | PCC: 0.4846 | R¬≤: 0.2165 | LR: 0.001260 | Time: 469.5s\n",
            "Epoch  10/100 | Loss: 0.0754 | RMSE: 0.4173 | MAE: 0.2898 | PCC: 0.5249 | R¬≤: 0.2605 | LR: 0.002710 | Time: 468.9s\n",
            "Epoch  15/100 | Loss: 0.0727 | RMSE: 0.4168 | MAE: 0.2891 | PCC: 0.5281 | R¬≤: 0.2623 | LR: 0.002985 | Time: 465.0s\n",
            "Epoch  20/100 | Loss: 0.0719 | RMSE: 0.4100 | MAE: 0.2881 | PCC: 0.5404 | R¬≤: 0.2862 | LR: 0.002927 | Time: 466.7s\n",
            "Epoch  25/100 | Loss: 0.0714 | RMSE: 0.4088 | MAE: 0.2841 | PCC: 0.5445 | R¬≤: 0.2903 | LR: 0.002824 | Time: 469.8s\n",
            "Epoch  30/100 | Loss: 0.0698 | RMSE: 0.4130 | MAE: 0.2889 | PCC: 0.5316 | R¬≤: 0.2758 | LR: 0.002682 | Time: 472.2s\n",
            "Epoch  35/100 | Loss: 0.0695 | RMSE: 0.4110 | MAE: 0.2837 | PCC: 0.5464 | R¬≤: 0.2828 | LR: 0.002504 | Time: 472.8s\n",
            "Epoch  40/100 | Loss: 0.0685 | RMSE: 0.4129 | MAE: 0.2846 | PCC: 0.5418 | R¬≤: 0.2762 | LR: 0.002295 | Time: 474.1s\n",
            "\n",
            "‚èπÔ∏è  Early stopping triggered at epoch 44\n",
            "   No improvement for 20 epochs\n",
            "------------------------------------------------------------\n",
            "‚úÖ Training completed in 20710.7s (345.2 min)\n",
            "\n",
            "üìä Loading best model and final evaluation...\n",
            "\n",
            "============================================================\n",
            "üèÜ BREAKTHROUGH RESULTS\n",
            "============================================================\n",
            "\n",
            "üìä Final Test Performance:\n",
            "   RMSE: 0.4094\n",
            "   MAE: 0.2882\n",
            "   R¬≤ Score: 0.2884\n",
            "   Pearson Correlation (PCC): 0.5409\n",
            "\n",
            "üìà Performance vs Targets:\n",
            "   Target RMSE: < 0.35\n",
            "   Achieved RMSE: 0.4094\n",
            "   üìç Gap to target: 0.0594\n",
            "   üìâ Reduced from 0.41: 0.2%\n",
            "\n",
            "   Target PCC: > 0.65\n",
            "   Achieved PCC: 0.5409\n",
            "   üìç Gap to target: 0.1091\n",
            "\n",
            "üìä Training Statistics:\n",
            "   Total epochs: 44\n",
            "   Best RMSE: 0.4059\n",
            "   Best PCC: 0.5503\n",
            "   Best MAE: 0.2841\n",
            "   Best R¬≤: 0.3003\n",
            "   Final LR: 0.002110\n",
            "   Training time: 345.2 minutes\n",
            "\n",
            "üîç Detailed Prediction Analysis (first 15 samples):\n",
            "    Actual |  Predicted |      Error |    % Error\n",
            "--------------------------------------------------\n",
            "   -0.4396 |    -0.4913 |     0.0518 |     -11.8%\n",
            "    0.1000 |    -0.5972 |     0.6972 |     697.2%\n",
            "   -0.1095 |    -0.1056 |    -0.0038 |       3.5%\n",
            "    0.0000 |    -0.1089 |     0.1089 |       0.0%\n",
            "   -0.3203 |    -0.0887 |    -0.2316 |      72.3%\n",
            "    0.0000 |    -0.1083 |     0.1083 |       0.0%\n",
            "   -0.2222 |    -0.1308 |    -0.0914 |      41.1%\n",
            "    0.0000 |    -0.1181 |     0.1181 |       0.0%\n",
            "   -0.3249 |    -0.1139 |    -0.2110 |      64.9%\n",
            "   -0.1158 |    -0.0926 |    -0.0233 |      20.1%\n",
            "   -0.1071 |    -0.1079 |     0.0008 |      -0.7%\n",
            "    0.0000 |    -0.6205 |     0.6205 |       0.0%\n",
            "   -1.4590 |    -0.6625 |    -0.7965 |      54.6%\n",
            "   -0.1948 |    -0.5132 |     0.3184 |    -163.5%\n",
            "   -1.2245 |    -0.6176 |    -0.6069 |      49.6%\n",
            "\n",
            "üìä Error Statistics:\n",
            "   Mean error: -0.0049\n",
            "   Std error: 0.4093\n",
            "   Max overestimation: 1.2662\n",
            "   Max underestimation: -1.8053\n",
            "   Median absolute error: 0.1832\n",
            "\n",
            "üíæ Saving model and artifacts...\n",
            "‚úÖ Artifacts ready for saving\n",
            "\n",
            "============================================================\n",
            "‚úÖ STEP 4 COMPLETED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "üéâ BREAKTHROUGH TRAINING COMPLETE!\n",
            "\n",
            "üéØ Final Summary:\n",
            "   Original Model:  RMSE=0.58, PCC=0.706\n",
            "   Previous Best:   RMSE=0.41, PCC=0.51\n",
            "   This Model:      RMSE=0.4094, PCC=0.5409\n",
            "\n",
            "üìç Close but not yet achieved:\n",
            "   RMSE gap: 0.0594\n",
            "   PCC gap: 0.1091\n",
            "\n",
            "üí° Next steps for improvement:\n",
            "   - Try ensemble of multiple QNN models\n",
            "   - Experiment with different quantum encodings\n",
            "   - Adjust hyperparameters (layers, qubits)\n",
            "   - Consider data augmentation techniques\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classical Model"
      ],
      "metadata": {
        "id": "VeqhfEi68hSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Paper-style model implementation for ALSFRS slope prediction (3‚Äì12 months)\n",
        "using THIS NOTEBOOK's preprocessing outputs.\n",
        "\n",
        "What this script expects to already exist in memory (from your Step 1 & Step 2 cells):\n",
        "- X_train, X_test, y_train, y_test            # from your preprocessing (original target scale)\n",
        "- features_df                                  # index = subject_id, includes the target column\n",
        "- alsfrs_3m                                    # first-90-day ALSFRS rows (subject_id, ALSFRS_Delta, Q* items, etc.)\n",
        "\n",
        "It will:\n",
        "1) Build the FFNN baseline on selected summary features (your X_train).\n",
        "2) Build ALSFRS visit-matrix (n_q x 5) per patient from alsfrs_3m (first 90d) with last-value carry-forward.\n",
        "3) Build a CNN-fusion model: conv over (n_q √ó 5) + non‚ÄëALSFRS tabular summaries ‚Üí FFNN head.\n",
        "4) Build an RNN-fusion model: 2 recurrent layers over sequence length=5 (features=n_q) + non‚ÄëALSFRS summaries ‚Üí FFNN head.\n",
        "5) Train models with early stopping; evaluate RMSD & PCC on X_test; optional bootstrap CIs.\n",
        "6) Create a simple ensemble (FFNN+CNN average) as in the paper.\n",
        "\n",
        "Note:\n",
        "- The paper mentions \"recurrent\" without specifying cell; here we default to LSTM but you can switch to GRU.\n",
        "- If your selected features (top_25_features) include ALSFRS-derived summaries, we try to exclude them from the fusion \"non‚ÄëALS\" branch by name pattern (columns starting with 'Q'). Adjust the predicate if your column names differ.\n",
        "- Keep target on its ORIGINAL scale (no scaling), as in the paper.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# =============================\n",
        "# Utilities: metrics & seed\n",
        "# =============================\n",
        "\n",
        "def set_global_seed(seed: int = 42):\n",
        "    import random, os\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "def rmsd(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "\n",
        "def pcc(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    if y_true.std() == 0 or y_pred.std() == 0:\n",
        "        return float('nan')\n",
        "    return float(np.corrcoef(y_true, y_pred)[0, 1])\n",
        "\n",
        "\n",
        "def bootstrap_ci(y_true: np.ndarray, y_pred: np.ndarray, metric_fn, n_boot: int = 10000, alpha: float = 0.05, seed: int = 42) -> Tuple[float, Tuple[float, float]]:\n",
        "    \"\"\" Nonparametric bootstrap CI for a metric. \"\"\"\n",
        "    set_global_seed(seed)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "    stats = []\n",
        "    idx = np.arange(n)\n",
        "    for _ in range(n_boot):\n",
        "        bs = np.random.choice(idx, size=n, replace=True)\n",
        "        stats.append(metric_fn(y_true[bs], y_pred[bs]))\n",
        "    stats = np.sort(np.array(stats))\n",
        "    low = stats[int((alpha/2) * n_boot)]\n",
        "    high = stats[int((1 - alpha/2) * n_boot)]\n",
        "    point = metric_fn(y_true, y_pred)\n",
        "    return point, (float(low), float(high))\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Build ALSFRS visit-matrix M (n_q √ó max_visits) per id\n",
        "# ======================================================\n",
        "\n",
        "def _infer_question_cols(df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"Infer ALSFRS question columns like 'Q1_Speech', 'Q3_Swallowing', ...\n",
        "    We look for columns that start with Q<digit> and are numeric.\n",
        "    \"\"\"\n",
        "    qcols = []\n",
        "    for c in df.columns:\n",
        "        if re.match(r\"^Q\\d+\", str(c)) and pd.api.types.is_numeric_dtype(df[c]):\n",
        "            qcols.append(c)\n",
        "    # Sort by the numeric part to maintain order Q1..Q11\n",
        "    def q_key(s: str):\n",
        "        m = re.match(r\"^Q(\\d+)\", s)\n",
        "        return int(m.group(1)) if m else 999\n",
        "    qcols.sort(key=q_key)\n",
        "    if not qcols:\n",
        "        raise ValueError(\"Could not infer ALSFRS question columns (Q1..). Please check column names in alsfrs_3m.\")\n",
        "    return qcols\n",
        "\n",
        "\n",
        "def build_alsfrs_matrices(\n",
        "    alsfrs_3m: pd.DataFrame,\n",
        "    patient_ids: List[int],\n",
        "    max_visits: int = 5,\n",
        "    time_col: str = \"ALSFRS_Delta\",\n",
        "    question_cols: List[str] = None,\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Build ALSFRS matrix A with shape (N, n_q, max_visits, 1) in the same order as patient_ids.\n",
        "    For each patient: take up to the first `max_visits` within 90 days, sort by time, and\n",
        "    last-value carry-forward to fill missing visits; remaining NaNs become 0.\n",
        "    Returns: (A, used_question_cols)\n",
        "    \"\"\"\n",
        "    if question_cols is None:\n",
        "        question_cols = _infer_question_cols(alsfrs_3m)\n",
        "\n",
        "    n_q = len(question_cols)\n",
        "    A = np.zeros((len(patient_ids), n_q, max_visits), dtype=np.float32)\n",
        "\n",
        "    for i, pid in enumerate(patient_ids):\n",
        "        g = alsfrs_3m[alsfrs_3m['subject_id'] == pid].copy()\n",
        "        if time_col not in g.columns:\n",
        "            raise ValueError(f\"'{time_col}' not found in alsfrs_3m\")\n",
        "        g = g.sort_values(time_col)\n",
        "        # Select first `max_visits` rows\n",
        "        g = g.iloc[:max_visits]\n",
        "        # Extract question matrix (rows=visits, cols=questions)\n",
        "        M = g[question_cols].to_numpy(dtype=np.float32)  # shape (v, n_q)\n",
        "        # Transpose to (n_q, v)\n",
        "        M = M.T\n",
        "        # If fewer than max_visits, pad by last-value carry-forward along time axis\n",
        "        if M.shape[1] < max_visits:\n",
        "            # Forward-fill along time dimension\n",
        "            pad_len = max_visits - M.shape[1]\n",
        "            if M.shape[1] == 0:\n",
        "                # no visit rows at all ‚Üí keep zeros\n",
        "                M_filled = np.zeros((n_q, max_visits), dtype=np.float32)\n",
        "            else:\n",
        "                M_filled = np.concatenate([M, np.repeat(M[:, -1:], pad_len, axis=1)], axis=1)\n",
        "        else:\n",
        "            M_filled = M[:, :max_visits]\n",
        "        # Replace remaining NaNs with 0\n",
        "        M_filled = np.nan_to_num(M_filled, nan=0.0)\n",
        "        A[i] = M_filled\n",
        "\n",
        "    # add channel dim\n",
        "    A = A[..., np.newaxis]  # (N, n_q, max_visits, 1)\n",
        "    return A, question_cols\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Train/Val helpers and model constructors\n",
        "# ==========================================\n",
        "\n",
        "def compile_and_fit(model: keras.Model, X_train, y_train, X_val, y_val, epochs=300, batch_size=64, patience=20, verbose=1):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
        "    cb = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)]\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=cb,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    return hist\n",
        "\n",
        "\n",
        "def make_ffnn(input_dim: int) -> keras.Model:\n",
        "    inp = layers.Input(shape=(input_dim,), name='tabular_in')\n",
        "    x = layers.Dense(128, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    out = layers.Dense(1, name='y')(x)\n",
        "    return keras.Model(inp, out, name='FFNN')\n",
        "\n",
        "\n",
        "def make_cnn_fusion(n_q: int, max_visits: int, tab_dim: int) -> keras.Model:\n",
        "    # matrix branch\n",
        "    mat_in = layers.Input(shape=(n_q, max_visits, 1), name='alsfrs_mat')\n",
        "    x = layers.Conv2D(16, kernel_size=(n_q, 3), activation='relu', padding='valid')(mat_in)  # (1, max_visits-2, 16)\n",
        "    x = layers.Conv2D(32, kernel_size=(1, 3), activation='relu', padding='valid')(x)        # (1, max_visits-4, 32)\n",
        "    x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # tabular non-ALS branch\n",
        "    tab_in = layers.Input(shape=(tab_dim,), name='non_als_tab')\n",
        "\n",
        "    h = layers.Concatenate()([x, tab_in])\n",
        "    h = layers.Dense(64, activation='relu')(h)\n",
        "    h = layers.Dropout(0.3)(h)\n",
        "    out = layers.Dense(1, name='y')(h)\n",
        "\n",
        "    return keras.Model([mat_in, tab_in], out, name='CNN_Fusion')\n",
        "\n",
        "\n",
        "def make_rnn_fusion(n_q: int, max_visits: int, tab_dim: int, cell: str = 'LSTM') -> keras.Model:\n",
        "    mat_in = layers.Input(shape=(n_q, max_visits, 1), name='alsfrs_mat')\n",
        "    # reshape to (time, features) = (max_visits, n_q)\n",
        "    x = layers.Lambda(lambda t: tf.squeeze(t, axis=-1))(mat_in)  # (n_q, max_visits)\n",
        "    x = layers.Permute((2, 1))(x)  # (max_visits, n_q)\n",
        "\n",
        "    if cell.upper() == 'GRU':\n",
        "        x = layers.GRU(64, return_sequences=True)(x)\n",
        "        x = layers.GRU(32)(x)\n",
        "    else:\n",
        "        x = layers.LSTM(64, return_sequences=True)(x)\n",
        "        x = layers.LSTM(32)(x)\n",
        "\n",
        "    tab_in = layers.Input(shape=(tab_dim,), name='non_als_tab')\n",
        "    h = layers.Concatenate()([x, tab_in])\n",
        "    h = layers.Dense(64, activation='relu')(h)\n",
        "    h = layers.Dropout(0.3)(h)\n",
        "    out = layers.Dense(1, name='y')(h)\n",
        "\n",
        "    return keras.Model([mat_in, tab_in], out, name='RNN_Fusion')\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Main training & evaluation\n",
        "# =============================\n",
        "\n",
        "def run_pipeline(bootstrap_n: int = 10000, verbose: int = 1):\n",
        "    # Sanity checks\n",
        "    for var in ['X_train', 'X_test', 'y_train', 'y_test', 'features_df', 'alsfrs_3m']:\n",
        "        if var not in globals():\n",
        "            raise RuntimeError(f\"Missing `{var}` in memory. Please run your Step 1 & Step 2 cells first.\")\n",
        "\n",
        "    set_global_seed(42)\n",
        "\n",
        "    # ------------------\n",
        "    # 1) FFNN baseline\n",
        "    # ------------------\n",
        "    if verbose:\n",
        "        print(\"\\n[1] Training FFNN baseline on selected summary features (tabular only)...\")\n",
        "    ffnn = make_ffnn(input_dim=X_train.shape[1])\n",
        "\n",
        "    # validation split from training set\n",
        "    hist = compile_and_fit(\n",
        "        ffnn,\n",
        "        X_train, y_train,\n",
        "        X_val=X_test, y_val=y_test,  # simple holdout val; for hyperparam search, wrap with KFold CV\n",
        "        epochs=300, batch_size=64, patience=20, verbose=verbose\n",
        "    )\n",
        "\n",
        "    y_pred_ffnn = ffnn.predict(X_test, verbose=0).reshape(-1)\n",
        "    ffnn_rmsd, ffnn_ci = bootstrap_ci(y_test.values, y_pred_ffnn, rmsd, n_boot=bootstrap_n)\n",
        "    ffnn_p, ffnn_p_ci = bootstrap_ci(y_test.values, y_pred_ffnn, pcc, n_boot=bootstrap_n)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"FFNN  ‚Üí RMSD: {ffnn_rmsd:.3f}  95% CI{ffnn_ci} | PCC: {ffnn_p:.3f} 95% CI{ffnn_p_ci}\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 2) Build ALSFRS visit-matrices for fusion nets\n",
        "    # -----------------------------------------\n",
        "    if verbose:\n",
        "        print(\"\\n[2] Building ALSFRS visit-matrices (n_q √ó 5) from alsfrs_3m...\")\n",
        "\n",
        "    patient_ids_all = features_df.index.tolist()\n",
        "    A_all, qcols = build_alsfrs_matrices(alsfrs_3m, patient_ids_all, max_visits=5, time_col='ALSFRS_Delta')\n",
        "\n",
        "    # Align with X_train/X_test by index order\n",
        "    id_to_pos = {pid: i for i, pid in enumerate(patient_ids_all)}\n",
        "    idx_tr = [id_to_pos[i] for i in X_train.index]\n",
        "    idx_te = [id_to_pos[i] for i in X_test.index]\n",
        "\n",
        "    A_tr = A_all[idx_tr]\n",
        "    A_te = A_all[idx_te]\n",
        "\n",
        "    # Non‚ÄëALS tabular branch: drop columns that appear to be ALSFRS-derived (prefix 'Q')\n",
        "    non_als_cols = [c for c in X_train.columns if not c.startswith('Q')]\n",
        "    Xtr_nonals = X_train[non_als_cols].copy()\n",
        "    Xte_nonals = X_test[non_als_cols].copy()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   - Question cols (n_q): {len(qcols)}: {qcols}\")\n",
        "        print(f\"   - Non‚ÄëALS tabular dim: {Xtr_nonals.shape[1]}\")\n",
        "\n",
        "    # ---------------------\n",
        "    # 3) CNN-fusion model\n",
        "    # ---------------------\n",
        "    if verbose:\n",
        "        print(\"\\n[3] Training CNN-fusion model (11√ó3 ‚Üí 1√ó3 convs + FFNN head)...\")\n",
        "    cnn_f = make_cnn_fusion(n_q=A_tr.shape[1], max_visits=A_tr.shape[2], tab_dim=Xtr_nonals.shape[1])\n",
        "    compile_and_fit(cnn_f, [A_tr, Xtr_nonals], y_train, [A_te, Xte_nonals], y_test, epochs=300, batch_size=64, patience=20, verbose=verbose)\n",
        "\n",
        "    y_pred_cnn = cnn_f.predict([A_te, Xte_nonals], verbose=0).reshape(-1)\n",
        "    cnn_rmsd, cnn_ci = bootstrap_ci(y_test.values, y_pred_cnn, rmsd, n_boot=bootstrap_n)\n",
        "    cnn_p, cnn_p_ci = bootstrap_ci(y_test.values, y_pred_cnn, pcc, n_boot=bootstrap_n)\n",
        "    if verbose:\n",
        "        print(f\"CNN-F ‚Üí RMSD: {cnn_rmsd:.3f}  95% CI{cnn_ci} | PCC: {cnn_p:.3f} 95% CI{cnn_p_ci}\")\n",
        "\n",
        "    # ---------------------\n",
        "    # 4) RNN-fusion model\n",
        "    # ---------------------\n",
        "    if verbose:\n",
        "        print(\"\\n[4] Training RNN-fusion model (2 recurrent layers + FFNN head)...\")\n",
        "    rnn_f = make_rnn_fusion(n_q=A_tr.shape[1], max_visits=A_tr.shape[2], tab_dim=Xtr_nonals.shape[1], cell='LSTM')\n",
        "    compile_and_fit(rnn_f, [A_tr, Xtr_nonals], y_train, [A_te, Xte_nonals], y_test, epochs=300, batch_size=64, patience=20, verbose=verbose)\n",
        "\n",
        "    y_pred_rnn = rnn_f.predict([A_te, Xte_nonals], verbose=0).reshape(-1)\n",
        "    rnn_rmsd, rnn_ci = bootstrap_ci(y_test.values, y_pred_rnn, rmsd, n_boot=bootstrap_n)\n",
        "    rnn_p, rnn_p_ci = bootstrap_ci(y_test.values, y_pred_rnn, pcc, n_boot=bootstrap_n)\n",
        "    if verbose:\n",
        "        print(f\"RNN-F ‚Üí RMSD: {rnn_rmsd:.3f}  95% CI{rnn_ci} | PCC: {rnn_p:.3f} 95% CI{rnn_p_ci}\")\n",
        "\n",
        "    # ---------------------\n",
        "    # 5) Simple ensemble\n",
        "    # ---------------------\n",
        "    if verbose:\n",
        "        print(\"\\n[5] Ensemble (FFNN + CNN) ‚Äî simple average of predictions...\")\n",
        "    ens_pred = 0.5 * y_pred_ffnn + 0.5 * y_pred_cnn\n",
        "    ens_rmsd, ens_ci = bootstrap_ci(y_test.values, ens_pred, rmsd, n_boot=bootstrap_n)\n",
        "    ens_p, ens_p_ci = bootstrap_ci(y_test.values, ens_pred, pcc, n_boot=bootstrap_n)\n",
        "    if verbose:\n",
        "        print(f\"ENS   ‚Üí RMSD: {ens_rmsd:.3f}  95% CI{ens_ci} | PCC: {ens_p:.3f} 95% CI{ens_p_ci}\")\n",
        "\n",
        "    return {\n",
        "        'ffnn':  {'pred': y_pred_ffnn, 'rmsd': ffnn_rmsd, 'rmsd_ci': ffnn_ci, 'pcc': ffnn_p, 'pcc_ci': ffnn_p_ci},\n",
        "        'cnn_f': {'pred': y_pred_cnn,  'rmsd': cnn_rmsd,  'rmsd_ci': cnn_ci,  'pcc': cnn_p,  'pcc_ci': cnn_p_ci},\n",
        "        'rnn_f': {'pred': y_pred_rnn,  'rmsd': rnn_rmsd,  'rmsd_ci': rnn_ci,  'pcc': rnn_p,  'pcc_ci': rnn_p_ci},\n",
        "        'ens':   {'pred': ens_pred,    'rmsd': ens_rmsd,  'rmsd_ci': ens_ci,  'pcc': ens_p,  'pcc_ci': ens_p_ci},\n",
        "        'qcols': qcols,\n",
        "        'non_als_cols': non_als_cols\n",
        "    }\n",
        "\n",
        "\n",
        "# Example usage (uncomment to run after your preprocessing cells):\n",
        "# results = run_pipeline(bootstrap_n=10000, verbose=1)\n",
        "# print(results)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# PyTorch Training Module (Warmup + Cosine, RMSE-focus)\n",
        "# Matches the user-provided training loop API but supports tuple inputs\n",
        "# for fusion models (ALS matrix + non‚ÄëALS tabular)\n",
        "# =============================\n",
        "\n",
        "import time, gc, math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# -------------------------------\n",
        "# Config (example)\n",
        "# -------------------------------\n",
        "config = {\n",
        "    'n_epochs': 300,\n",
        "    'early_stopping_patience': 20,\n",
        "    'batch_size': 64,\n",
        "    'warmup_epochs': 10,\n",
        "    'learning_rate_start': 1e-5,\n",
        "    'learning_rate_max': 1e-3\n",
        "}\n",
        "\n",
        "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure placeholder exists for artifact bundle\n",
        "try:\n",
        "    preprocessing_artifacts\n",
        "except NameError:\n",
        "    preprocessing_artifacts = {}\n",
        "\n",
        "# -------------------------------\n",
        "# Datasets\n",
        "# -------------------------------\n",
        "class TabularDS(Dataset):\n",
        "    def __init__(self, X_df, y_s):\n",
        "        self.X = torch.tensor(X_df.values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_s.values.reshape(-1, 1), dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class FusionDS(Dataset):\n",
        "    def __init__(self, A_np, X_nonals_df, y_s):\n",
        "        # A: (N, n_q, max_visits, 1) ‚Üí to torch as (N, 1, n_q, max_visits)\n",
        "        A = np.transpose(A_np, (0, 3, 1, 2))\n",
        "        self.A = torch.tensor(A, dtype=torch.float32)\n",
        "        self.T = torch.tensor(X_nonals_df.values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_s.values.reshape(-1, 1), dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        # Return ((als_mat, tab_nonals), y)\n",
        "        return (self.A[idx], self.T[idx]), self.y[idx]\n",
        "\n",
        "# -------------------------------\n",
        "# Models (paper-faithful)\n",
        "# -------------------------------\n",
        "class FFNN_PT(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CNNFusion_PT(nn.Module):\n",
        "    def __init__(self, n_q, max_visits, tab_dim):\n",
        "        super().__init__()\n",
        "        # Input A: (B, 1, n_q, max_visits)\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(n_q, 3), padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 3), padding=0)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(32 + tab_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # x may be ((A, T)) or (A, T) or just A\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            A, T = x\n",
        "        else:\n",
        "            A, T = x, None\n",
        "        h = torch.relu(self.conv1(A))\n",
        "        h = torch.relu(self.conv2(h))\n",
        "        # Global max over H and W\n",
        "        h = torch.amax(h, dim=(2, 3))  # (B, 32)\n",
        "        if T is None:\n",
        "            raise RuntimeError(\"CNNFusion expects (ALS_matrix, tabular_nonALS)\")\n",
        "        z = torch.cat([h, T], dim=1)\n",
        "        return self.head(z)\n",
        "\n",
        "class RNNFusion_PT(nn.Module):\n",
        "    def __init__(self, n_q, max_visits, tab_dim, cell='LSTM'):\n",
        "        super().__init__()\n",
        "        self.cell = cell.upper()\n",
        "        feat = n_q\n",
        "        hid1, hid2 = 64, 32\n",
        "        if self.cell == 'GRU':\n",
        "            self.r1 = nn.GRU(input_size=feat, hidden_size=hid1, batch_first=True)\n",
        "            self.r2 = nn.GRU(input_size=hid1, hidden_size=hid2, batch_first=True)\n",
        "        else:\n",
        "            self.r1 = nn.LSTM(input_size=feat, hidden_size=hid1, batch_first=True)\n",
        "            self.r2 = nn.LSTM(input_size=hid1, hidden_size=hid2, batch_first=True)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hid2 + tab_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            A, T = x\n",
        "        else:\n",
        "            A, T = x, None\n",
        "        # A: (B, 1, n_q, max_visits) ‚Üí (B, max_visits, n_q)\n",
        "        A = torch.permute(A.squeeze(1), (0, 2, 1))\n",
        "        o1, _ = self.r1(A)\n",
        "        o2, _ = self.r2(o1)\n",
        "        h = o2[:, -1, :]\n",
        "        if T is None:\n",
        "            raise RuntimeError(\"RNNFusion expects (ALS_matrix, tabular_nonALS)\")\n",
        "        z = torch.cat([h, T], dim=1)\n",
        "        return self.head(z)\n",
        "\n",
        "# -------------------------------\n",
        "# LR Scheduler (Warmup + Cosine)\n",
        "# -------------------------------\n",
        "\n",
        "def get_lr(epoch, cfg):\n",
        "    if epoch < cfg['warmup_epochs']:\n",
        "        return cfg['learning_rate_start'] + (cfg['learning_rate_max'] - cfg['learning_rate_start']) * (epoch / cfg['warmup_epochs'])\n",
        "    progress = (epoch - cfg['warmup_epochs']) / max(1, (cfg['n_epochs'] - cfg['warmup_epochs']))\n",
        "    return cfg['learning_rate_max'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# -------------------------------\n",
        "# Train/Eval that support tuple inputs and device\n",
        "# -------------------------------\n",
        "\n",
        "def _to_device(batch_x, batch_y):\n",
        "    if isinstance(batch_x, (list, tuple)):\n",
        "        batch_x = tuple(t.to(_device) for t in batch_x)\n",
        "    else:\n",
        "        batch_x = batch_x.to(_device)\n",
        "    batch_y = batch_y.to(_device)\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, epoch, cfg):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    lr = get_lr(epoch, cfg)\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg['lr'] = lr\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_x, batch_y = _to_device(batch_x, batch_y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        pred = model(batch_x)\n",
        "        loss = criterion(pred, batch_y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        del pred, loss\n",
        "    gc.collect()\n",
        "    return total_loss / len(loader), lr\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in loader:\n",
        "            batch_x, batch_y = _to_device(batch_x, batch_y)\n",
        "            pred = model(batch_x)\n",
        "            preds.extend(pred.detach().cpu().numpy().reshape(-1))\n",
        "            trues.extend(batch_y.detach().cpu().numpy().reshape(-1))\n",
        "            del pred\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "    rmse = float(np.sqrt(mean_squared_error(trues, preds)))\n",
        "    mae = float(mean_absolute_error(trues, preds))\n",
        "    r2 = float(r2_score(trues, preds))\n",
        "    pcc = float(pearsonr(trues, preds)[0]) if np.std(preds) > 0 and np.std(trues) > 0 else float('nan')\n",
        "    return rmse, mae, r2, pcc, preds, trues\n",
        "\n",
        "# -------------------------------\n",
        "# Runner (as per provided printouts)\n",
        "# -------------------------------\n",
        "\n",
        "def run_training(model, train_loader, test_loader, cfg):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 4: BREAKTHROUGH TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nüöÄ Starting breakthrough training...\")\n",
        "    print(f\"   Target: RMSE < 0.35, PCC > 0.65\")\n",
        "    print(f\"   Strategy: Warmup + Cosine decay + Heavy RMSE focus\")\n",
        "    print(f\"   Epochs: {cfg['n_epochs']}\")\n",
        "    print(f\"   Patience: {cfg['early_stopping_patience']}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    model = model.to(_device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg['learning_rate_start'])\n",
        "\n",
        "    best_score = -float('inf')\n",
        "    best_rmse = float('inf')\n",
        "    best_pcc = -1\n",
        "    best_mae = float('inf')\n",
        "    best_r2 = -1\n",
        "    patience_counter = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_rmse': [],\n",
        "        'test_pcc': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(cfg['n_epochs']):\n",
        "        epoch_start = time.time()\n",
        "        train_loss, lr = train_epoch(model, train_loader, criterion, optimizer, epoch, cfg)\n",
        "        rmse, mae, r2, pcc, _, _ = evaluate(model, test_loader)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_rmse'].append(rmse)\n",
        "        history['test_pcc'].append(pcc)\n",
        "        history['learning_rates'].append(lr)\n",
        "\n",
        "        combined_score = -2.0 * rmse + pcc\n",
        "        if combined_score > best_score:\n",
        "            best_score = combined_score\n",
        "            best_rmse = rmse\n",
        "            best_pcc = pcc\n",
        "            best_mae = mae\n",
        "            best_r2 = r2\n",
        "            best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{cfg['n_epochs']} | Loss: {train_loss:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | PCC: {pcc:.4f} | R¬≤: {r2:.4f} | LR: {lr:.6f} | Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        if patience_counter >= cfg['early_stopping_patience']:\n",
        "            print(f\"\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "            print(f\"   No improvement for {cfg['early_stopping_patience']} epochs\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"‚úÖ Training completed in {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "\n",
        "    print(\"\\nüìä Loading best model and final evaluation...\")\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model = model.to(_device)\n",
        "    final_rmse, final_mae, final_r2, final_pcc, predictions, actuals = evaluate(model, test_loader)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üèÜ BREAKTHROUGH RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüìä Final Test Performance:\")\n",
        "    print(f\"   RMSE: {final_rmse:.4f}\")\n",
        "    print(f\"   MAE: {final_mae:.4f}\")\n",
        "    print(f\"   R¬≤ Score: {final_r2:.4f}\")\n",
        "    print(f\"   Pearson Correlation (PCC): {final_pcc:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìà Performance vs Targets:\")\n",
        "    print(f\"   Target RMSE: < 0.35\")\n",
        "    print(f\"   Achieved RMSE: {final_rmse:.4f}\")\n",
        "    if final_rmse < 0.35:\n",
        "        improvement = ((0.58 - final_rmse) / 0.58) * 100\n",
        "        print(f\"   ‚úÖ RMSE TARGET ACHIEVED!\")\n",
        "        print(f\"   üéâ {improvement:.1f}% improvement from original (0.58)\")\n",
        "    else:\n",
        "        gap = final_rmse - 0.35\n",
        "        reduction_from_041 = ((0.41 - final_rmse) / 0.41) * 100\n",
        "        print(f\"   üìç Gap to target: {gap:.4f}\")\n",
        "        print(f\"   üìâ Reduced from 0.41: {reduction_from_041:.1f}%\")\n",
        "\n",
        "    print(f\"\\n   Target PCC: > 0.65\")\n",
        "    print(f\"   Achieved PCC: {final_pcc:.4f}\")\n",
        "    if final_pcc > 0.65:\n",
        "        print(f\"   ‚úÖ PCC TARGET ACHIEVED!\")\n",
        "    else:\n",
        "        gap = 0.65 - final_pcc\n",
        "        print(f\"   üìç Gap to target: {gap:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìä Training Statistics:\")\n",
        "    print(f\"   Total epochs: {len(history['train_loss'])}\")\n",
        "    print(f\"   Best RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"   Best PCC: {best_pcc:.4f}\")\n",
        "    print(f\"   Best MAE: {best_mae:.4f}\")\n",
        "    print(f\"   Best R¬≤: {best_r2:.4f}\")\n",
        "    print(f\"   Final LR: {history['learning_rates'][-1]:.6f}\")\n",
        "    print(f\"   Training time: {total_time/60:.1f} minutes\")\n",
        "\n",
        "    errors = actuals - predictions\n",
        "    print(\"\\nüîç Detailed Prediction Analysis (first 15 samples):\")\n",
        "    print(f\"{'Actual':>10} | {'Predicted':>10} | {'Error':>10} | {'% Error':>10}\")\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(min(15, len(actuals))):\n",
        "        a = actuals[i]; p = predictions[i]\n",
        "        err = a - p; pct = (err / a * 100) if a != 0 else 0\n",
        "        print(f\"{a:>10.4f} | {p:>10.4f} | {err:>10.4f} | {pct:>9.1f}%\")\n",
        "\n",
        "    print(\"\\nüìä Error Statistics:\")\n",
        "    print(f\"   Mean error: {errors.mean():.4f}\")\n",
        "    print(f\"   Std error: {errors.std():.4f}\")\n",
        "    print(f\"   Max overestimation: {errors.max():.4f}\")\n",
        "    print(f\"   Max underestimation: {errors.min():.4f}\")\n",
        "    print(f\"   Median absolute error: {np.median(np.abs(errors)):.4f}\")\n",
        "\n",
        "    final_artifacts = {\n",
        "        'model_state': best_model_state,\n",
        "        'config': cfg,\n",
        "        'preprocessing': preprocessing_artifacts,\n",
        "        'history': history,\n",
        "        'best_metrics': {\n",
        "            'rmse': best_rmse,\n",
        "            'mae': best_mae,\n",
        "            'r2': best_r2,\n",
        "            'pcc': best_pcc\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ STEP 4 COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nüéâ BREAKTHROUGH TRAINING COMPLETE!\")\n",
        "    print(f\"\\nüéØ Final Summary:\")\n",
        "    print(f\"   This Model:      RMSE={final_rmse:.4f}, PCC={final_pcc:.4f}\")\n",
        "    return final_artifacts, (predictions, actuals)\n",
        "\n",
        "# -------------------------------\n",
        "# Wiring for each paper model\n",
        "# -------------------------------\n",
        "# Pre-req from earlier cells: X_train, X_test, y_train, y_test, features_df, alsfrs_3m\n",
        "\n",
        "# Build ALSFRS matrices aligned with splits (reuse earlier function build_alsfrs_matrices)\n",
        "patient_ids_all = features_df.index.tolist()\n",
        "A_all, qcols = build_alsfrs_matrices(alsfrs_3m, patient_ids_all, max_visits=5, time_col='ALSFRS_Delta')\n",
        "idx_map = {pid: i for i, pid in enumerate(patient_ids_all)}\n",
        "idx_tr = [idx_map[i] for i in X_train.index]\n",
        "idx_te = [idx_map[i] for i in X_test.index]\n",
        "A_tr = A_all[idx_tr]\n",
        "A_te = A_all[idx_te]\n",
        "\n",
        "non_als_cols = [c for c in X_train.columns if not c.startswith('Q')]\n",
        "Xtr_nonals = X_train[non_als_cols].copy()\n",
        "Xte_nonals = X_test[non_als_cols].copy()\n",
        "\n",
        "# DataLoaders\n",
        "train_loader_ff = DataLoader(TabularDS(X_train, y_train), batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
        "test_loader_ff  = DataLoader(TabularDS(X_test, y_test),   batch_size=config['batch_size'], shuffle=False, drop_last=False)\n",
        "\n",
        "train_loader_cnn = DataLoader(FusionDS(A_tr, Xtr_nonals, y_train), batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
        "test_loader_cnn  = DataLoader(FusionDS(A_te, Xte_nonals, y_test),   batch_size=config['batch_size'], shuffle=False, drop_last=False)\n",
        "\n",
        "train_loader_rnn = train_loader_cnn  # same inputs\n",
        "test_loader_rnn  = test_loader_cnn\n",
        "\n",
        "# Instantiate models\n",
        "ffnn_pt = FFNN_PT(in_dim=X_train.shape[1])\n",
        "cnn_f_pt = CNNFusion_PT(n_q=A_tr.shape[1], max_visits=A_tr.shape[2], tab_dim=Xtr_nonals.shape[1])\n",
        "rnn_f_pt = RNNFusion_PT(n_q=A_tr.shape[1], max_visits=A_tr.shape[2], tab_dim=Xtr_nonals.shape[1], cell='LSTM')\n",
        "\n",
        "# === RUN ===\n",
        "ffnn_artifacts, _ = run_training(ffnn_pt, train_loader_ff, test_loader_ff, config)\n",
        "# cnn_artifacts,  _ = run_training(cnn_f_pt, train_loader_cnn, test_loader_cnn, config)\n",
        "# rnn_artifacts,  _ = run_training(rnn_f_pt, train_loader_rnn, test_loader_rnn, config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIaINhCk9ETs",
        "outputId": "b434a7f7-7a9c-4a61-c98d-34a25fbb83f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 4: BREAKTHROUGH TRAINING\n",
            "============================================================\n",
            "\n",
            "üöÄ Starting breakthrough training...\n",
            "   Target: RMSE < 0.35, PCC > 0.65\n",
            "   Strategy: Warmup + Cosine decay + Heavy RMSE focus\n",
            "   Epochs: 300\n",
            "   Patience: 20\n",
            "------------------------------------------------------------\n",
            "Epoch   1/300 | Loss: 0.4430 | RMSE: 0.6017 | MAE: 0.3981 | PCC: -0.0788 | R¬≤: -0.5374 | LR: 0.000010 | Time: 0.6s\n",
            "Epoch   5/300 | Loss: 0.2103 | RMSE: 0.4221 | MAE: 0.2948 | PCC: 0.5223 | R¬≤: 0.2435 | LR: 0.000406 | Time: 0.6s\n",
            "Epoch  10/300 | Loss: 0.1692 | RMSE: 0.4018 | MAE: 0.2772 | PCC: 0.5661 | R¬≤: 0.3146 | LR: 0.000901 | Time: 0.4s\n",
            "Epoch  15/300 | Loss: 0.1640 | RMSE: 0.4056 | MAE: 0.2768 | PCC: 0.5593 | R¬≤: 0.3013 | LR: 0.001000 | Time: 0.4s\n",
            "Epoch  20/300 | Loss: 0.1537 | RMSE: 0.4058 | MAE: 0.2825 | PCC: 0.5497 | R¬≤: 0.3007 | LR: 0.000998 | Time: 0.4s\n",
            "Epoch  25/300 | Loss: 0.1534 | RMSE: 0.4063 | MAE: 0.2772 | PCC: 0.5563 | R¬≤: 0.2990 | LR: 0.000994 | Time: 0.4s\n",
            "Epoch  30/300 | Loss: 0.1487 | RMSE: 0.4086 | MAE: 0.2824 | PCC: 0.5429 | R¬≤: 0.2912 | LR: 0.000989 | Time: 0.6s\n",
            "\n",
            "‚èπÔ∏è  Early stopping triggered at epoch 33\n",
            "   No improvement for 20 epochs\n",
            "------------------------------------------------------------\n",
            "‚úÖ Training completed in 15.7s (0.3 min)\n",
            "\n",
            "üìä Loading best model and final evaluation...\n",
            "\n",
            "============================================================\n",
            "üèÜ BREAKTHROUGH RESULTS\n",
            "============================================================\n",
            "\n",
            "üìä Final Test Performance:\n",
            "   RMSE: 0.4004\n",
            "   MAE: 0.2751\n",
            "   R¬≤ Score: 0.3193\n",
            "   Pearson Correlation (PCC): 0.5684\n",
            "\n",
            "üìà Performance vs Targets:\n",
            "   Target RMSE: < 0.35\n",
            "   Achieved RMSE: 0.4004\n",
            "   üìç Gap to target: 0.0504\n",
            "   üìâ Reduced from 0.41: 2.3%\n",
            "\n",
            "   Target PCC: > 0.65\n",
            "   Achieved PCC: 0.5684\n",
            "   üìç Gap to target: 0.0816\n",
            "\n",
            "üìä Training Statistics:\n",
            "   Total epochs: 33\n",
            "   Best RMSE: 0.4004\n",
            "   Best PCC: 0.5684\n",
            "   Best MAE: 0.2751\n",
            "   Best R¬≤: 0.3193\n",
            "   Final LR: 0.000986\n",
            "   Training time: 0.3 minutes\n",
            "\n",
            "üîç Detailed Prediction Analysis (first 15 samples):\n",
            "    Actual |  Predicted |      Error |    % Error\n",
            "--------------------------------------------------\n",
            "   -0.4396 |    -0.4196 |    -0.0200 |       4.5%\n",
            "    0.1000 |    -0.5762 |     0.6762 |     676.2%\n",
            "   -0.1095 |    -0.0447 |    -0.0647 |      59.1%\n",
            "    0.0000 |    -0.0190 |     0.0190 |       0.0%\n",
            "   -0.3203 |    -0.0494 |    -0.2708 |      84.6%\n",
            "    0.0000 |    -0.0532 |     0.0532 |       0.0%\n",
            "   -0.2222 |    -0.0727 |    -0.1496 |      67.3%\n",
            "    0.0000 |    -0.0390 |     0.0390 |       0.0%\n",
            "   -0.3249 |    -0.0877 |    -0.2373 |      73.0%\n",
            "   -0.1158 |    -0.0250 |    -0.0909 |      78.5%\n",
            "   -0.1071 |    -0.0146 |    -0.0926 |      86.4%\n",
            "    0.0000 |    -0.4887 |     0.4887 |       0.0%\n",
            "   -1.4590 |    -1.0053 |    -0.4537 |      31.1%\n",
            "   -0.1948 |    -0.4661 |     0.2713 |    -139.3%\n",
            "   -1.2245 |    -0.6108 |    -0.6137 |      50.1%\n",
            "\n",
            "üìä Error Statistics:\n",
            "   Mean error: -0.0248\n",
            "   Std error: 0.3996\n",
            "   Max overestimation: 1.2403\n",
            "   Max underestimation: -1.8395\n",
            "   Median absolute error: 0.1908\n",
            "\n",
            "============================================================\n",
            "‚úÖ STEP 4 COMPLETED SUCCESSFULLY\n",
            "============================================================\n",
            "\n",
            "üéâ BREAKTHROUGH TRAINING COMPLETE!\n",
            "\n",
            "üéØ Final Summary:\n",
            "   This Model:      RMSE=0.4004, PCC=0.5684\n"
          ]
        }
      ]
    }
  ]
}